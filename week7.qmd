---
title: "第 7 周：聚类基础与层次结构"
---

## 学习目标

*   回顾 K-Means 算法的原理、优缺点和局限性。
*   理解层次聚类的基本原理、不同连接标准和树状图 (Dendrogram) 的解读。
*   掌握使用 `scikit-learn` 实现 K-Means 和层次聚类的方法。
*   了解层次聚类在市场细分等场景的应用。
*   启动小组项目三，明确项目要求和可选算法。

## 第一次课：K-Means 回顾与层次聚类入门

### 1. K-Means 算法回顾与局限性

我们在之前的课程中已经学习了 K-Means 算法，它是一种非常常用且高效的划分式聚类算法。

*   **核心思想:** 将数据点划分为预先设定的 K 个簇，使得每个点都属于离其最近的簇中心（质心），并且簇内数据点的平方和最小。
*   **优点:**
    *   算法简单，易于理解和实现。
    *   对于球状簇且簇之间分离明显的数据集，效果较好。
    *   计算效率相对较高，适合处理大规模数据集。
*   **缺点与局限性:**
    *   **需要预先指定 K 值:** K 值的选择对结果影响很大，选择不当会导致次优的聚类效果。肘部法则和轮廓系数可以辅助选择，但并非总能找到最优 K。
    *   **对初始质心敏感:** 不同的初始质心可能导致不同的聚类结果。`scikit-learn` 中的 `n_init` 参数可以通过多次初始化来缓解这个问题。
    *   **对非球状簇效果不佳:** K-Means 假设簇是凸形的、各向同性的（类似圆形或球形），难以发现非规则形状的簇。
    *   **对异常值敏感:** 异常值会对质心的计算产生较大影响，可能导致簇的划分发生偏移。

正是因为 K-Means 的这些局限性，我们需要学习其他聚类算法来应对更复杂的数据和场景。

### 2. 层次聚类 (Hierarchical Clustering)

层次聚类是一种不需要预先指定簇数量的聚类方法，它会构建一个嵌套的簇的层次结构。

*   **两种主要方式:**
    *   **凝聚式 (Agglomerative):** 自底向上，开始时每个数据点自成一簇，然后逐步合并最相似的簇，直到所有点合并为一个簇。这是最常用的方式。
    *   **分裂式 (Divisive):** 自顶向下，开始时所有数据点在一个簇，然后逐步分裂最不相似的簇，直到每个点自成一簇。计算复杂度较高，不太常用。

*   **凝聚式层次聚类步骤:**
    1.  将每个数据点视为一个单独的簇。
    2.  计算所有簇之间的距离（或相似度）。
    3.  合并距离最近（最相似）的两个簇。
    4.  重新计算新合并簇与其他簇之间的距离。
    5.  重复步骤 3 和 4，直到所有数据点合并为一个簇。

*   **关键概念:**
    *   **距离度量 (Distance Metric):** 用于计算数据点或簇之间的距离，常用欧氏距离 (Euclidean distance)。
    *   **连接标准 (Linkage Criteria):** 定义如何计算簇之间的距离。常用标准包括：
        *   **Ward:** 最小化簇内方差的增量。通常效果较好，倾向于产生大小相似的簇。(`linkage='ward'`)
        *   **Average Linkage:** 计算两个簇中所有点对之间距离的平均值。(`linkage='average'`)
        *   **Complete Linkage (Maximum Linkage):** 计算两个簇中所有点对之间距离的最大值。(`linkage='complete'`)
        *   **Single Linkage (Minimum Linkage):** 计算两个簇中所有点对之间距离的最小值。(`linkage='single'`) 对异常值敏感，可能产生链状效果。

*   **树状图 (Dendrogram):**
    *   层次聚类的结果通常用树状图可视化。
    *   纵轴表示簇合并时的距离（或不相似度），横轴表示数据点（或样本索引）。
    *   通过在某个距离阈值水平切割树状图，可以得到指定数量的簇。切割线穿过的垂直线数量即为簇的数量。
    *   树状图的高度差可以反映簇之间的分离程度。

*   **优点:**
    *   **无需预先指定 K 值:** 可以根据树状图决定合适的簇数量。
    *   **可以揭示数据的层次结构:** 树状图本身提供了丰富的结构信息。
    *   对于某些连接标准（如 Single Linkage），可以发现非凸形状的簇。
*   **缺点:**
    *   **计算复杂度较高:** 通常为 O(n^3) 或 O(n^2 log n)，不适合非常大的数据集。
    *   **合并决策不可撤销:** 一旦两个簇被合并，后续步骤无法撤销。
    *   对距离度量和连接标准的选择比较敏感。

### 3. 实践：使用 `scikit-learn` 实现层次聚类

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage

# 1. 生成示例数据
X, y = make_blobs(n_samples=50, centers=3, cluster_std=1.0, random_state=42)

# 2. 计算连接矩阵 (用于绘制树状图)
# 使用 'ward' 连接标准
linked_ward = linkage(X, method='ward')

# 3. 绘制树状图 (Dendrogram)
plt.figure(figsize=(10, 7))
dendrogram(linked_ward,
            orientation='top',
            labels=np.arange(1, X.shape[0] + 1), # 可以替换为样本标签
            distance_sort='descending',
            show_leaf_counts=True)
plt.title('Hierarchical Clustering Dendrogram (Ward Linkage)')
plt.xlabel('Sample Index')
plt.ylabel('Distance (Ward)')
# 添加一条水平线来建议切割点 (例如，选择3个簇)
plt.axhline(y=15, color='r', linestyle='--')
plt.show()

# 4. 使用 AgglomerativeClustering 进行聚类
# 假设我们根据树状图决定分为 3 个簇
n_clusters = 3
agg_clustering = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')
agg_labels = agg_clustering.fit_predict(X)

# 5. 可视化聚类结果
plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], c=agg_labels, cmap='viridis', s=50)
plt.title(f'Agglomerative Clustering (k={n_clusters}, Ward)')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.grid(True)
plt.show()

print("层次聚类分配的标签:", agg_labels)
```

**AI 辅助编程提示:**

*   尝试让 AI 解释 `scipy.cluster.hierarchy.linkage` 和 `dendrogram` 函数的参数。
*   让 AI 生成不同连接标准 (`average`, `complete`, `single`) 的树状图并比较差异。
*   询问 AI 如何根据树状图自动选择最佳 K 值（虽然没有完美方法，但可以了解一些启发式策略）。

### 4. 小组项目三启动：用户分群模型构建与分析 (更新)

*   **目标:** 掌握不同聚类算法的应用，理解聚类结果评估，并能对聚类结果进行业务解读。
*   **核心要求:**
    *   **算法选择:** **至少尝试两种**聚类算法（从 K-Means, 层次聚类, GMM, DBSCAN 中选择）。
    *   **对比分析:** 对比不同算法在你的数据集上的表现（例如，使用轮廓系数、Davies-Bouldin 指数评估，并结合可视化结果），解释选择最终模型的原因。
    *   **结果解读:** 对最终的聚类结果进行业务层面的分析和解读（例如，不同用户群体的特征是什么？）。
*   **数据集:** 学生小组自主选择，建议选择包含用户画像或用户行为特征的数据集，鼓励寻找能体现不同算法优势的数据。
*   **可选挑战:** 鼓励有能力的小组尝试**基于文本 Embeddings 的聚类**（我们将在下周介绍概念）。例如，对电商评论数据进行聚类。
*   **提交时间:**
    *   初步模型与算法选择理由 (例如 K-Means vs 层次聚类): 第 9 周课前
    *   最终模型、对比分析与报告: 第 10 或 11 周课前 (待定)

## 第二次课：层次聚类应用与项目讨论

### 1. 层次聚类应用场景

层次聚类由于其能够展示数据的层级结构，在许多领域都有应用：

*   **市场细分 (Market Segmentation):**
    *   通过对用户的购买行为、人口统计学特征、兴趣偏好等数据进行层次聚类，可以发现不同层级的用户群体。
    *   树状图可以直观地展示细分市场之间的关系和距离，帮助营销人员理解市场的结构，制定更精准的营销策略。例如，先区分高价值和低价值客户，再在高价值客户中细分出不同偏好的群体。
*   **生物信息学 (Bioinformatics):**
    *   基因表达谱分析：对不同样本或不同条件下的基因表达数据进行聚类，可以发现功能相似或共同调控的基因群，或者对样本进行分类。
    *   物种分类：构建物种的系统发育树。
*   **社交网络分析 (Social Network Analysis):**
    *   社群发现：识别社交网络中的紧密连接的子群组或社区。
*   **图像分割 (Image Segmentation):**
    *   将图像中的像素根据颜色、纹理等特征进行层次聚类，实现图像区域的划分。

### 2. 小组项目三：数据探索与算法选择讨论

*   **课堂活动:**
    *   各小组展示初步选择的数据集。
    *   讨论数据集的特点（维度、样本量、特征类型、是否存在明显异常值、预期的簇形状等）。
    *   基于数据特点和项目目标，初步讨论选择哪两种或更多聚类算法进行尝试。
        *   如果数据量很大，层次聚类可能计算较慢。
        *   如果预期簇是球状的，K-Means 是个好的起点。
        *   如果想探索数据的内在层次结构，层次聚类更合适。
        *   如果数据中有明显异常值，K-Means 可能受影响较大。
    *   教师巡回指导，提供建议。

### 3. 思考与练习

1.  K-Means 的主要局限性有哪些？在什么情况下它可能不是最佳选择？
2.  解释凝聚式层次聚类和分裂式层次聚类的区别。为什么凝聚式更常用？
3.  什么是连接标准 (Linkage Criteria)？ 'Ward' 连接和 'Single' 连接的主要区别和适用场景是什么？
4.  如何解读树状图 (Dendrogram)？如何利用它来帮助选择簇的数量？
5.  假设你要对一批客户根据他们的消费金额和购买频率进行聚类，你会优先考虑 K-Means 还是层次聚类？为什么？如果数据量非常大（百万级别），你的选择会改变吗？