---
title: "第六周：回归算法进阶 - XGBoost 与模型优化"
subtitle: "掌握强大的 Boosting 算法，精进房价预测模型"
---

上周我们学习了线性回归、多项式回归以及正则化技术。本周，我们将学习一种在各种数据竞赛和实际应用中都表现极为出色的集成学习算法——**XGBoost (Extreme Gradient Boosting, 极限梯度提升算法)**。该算法由陈天奇等人于2014年提出，现已成为机器学习领域最受欢迎的算法之一。我们将理解 Boosting 的思想，掌握 XGBoost 的使用，并将其应用于我们的房价预测项目（项目二）的优化中。

::: {.callout-info title="项目二进展"}
请确保你已经完成了项目二的基础模型构建（线性回归）以及初步的改进尝试（多项式回归/正则化）。本周我们将使用 XGBoost 来进一步提升模型性能。
:::

## 1. Boosting 思想简介

我们在第四周学习了 Bagging（以随机森林为代表），它通过并行训练多个独立的基学习器（决策树）并聚合结果来提高性能。**Boosting** 是另一种强大的集成学习思想，它的工作方式与 Bagging 不同：

*   **串行训练:** Boosting 算法**依次**训练基学习器（通常也是决策树）。
*   **关注错误:** 每个新的基学习器都**重点关注**前面学习器**预测错误**的样本。
*   **加权组合:** 最终的模型是所有基学习器的**加权组合**，表现好的学习器权重更大。

简单来说，Boosting 就像一个学习小组，第一个同学先学一遍，然后第二个同学重点学习第一个同学没掌握好的知识点，第三个同学再重点学习前两个同学都没掌握好的... 最后综合所有同学的知识。

## 2. 从 GBDT 到 XGBoost

**梯度提升决策树 (Gradient Boosting Decision Tree, GBDT)** 是 Boosting 思想的一个经典实现。它使用梯度下降的思想来优化损失函数，每一棵新树都拟合前面所有树预测结果的**残差（Residuals）** 的负梯度。

**XGBoost (Extreme Gradient Boosting)** 是 GBDT 的一种**高效、灵活且可扩展**的实现。它在 GBDT 的基础上做了许多工程和算法上的优化，使其成为目前最流行、性能最好的机器学习算法之一。

**XGBoost 的主要优势:**

*   **正则化:** 内置了 L1 和 L2 正则化项，有助于防止过拟合。
*   **缺失值处理:** 能够自动处理数据中的缺失值。
*   **并行处理:** 在特征层面支持并行计算，训练速度更快。
*   **内置交叉验证:** 可以在训练过程中进行交叉验证。
*   **树剪枝:** 包含更优化的剪枝策略。
*   **灵活性:** 支持自定义优化目标和评估指标。

## 3. 使用 XGBoost 进行回归

XGBoost 有自己独立的 Python 库 `xgboost`。我们需要先安装它（如果你的 Anaconda 环境没有预装的话）：
`pip install xgboost` 或 `conda install py-xgboost`

然后，我们可以使用 `XGBRegressor` 类来进行回归任务。

```{python}
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import pandas as pd
import numpy as np
# 假设线性回归数据已准备好
np.random.seed(0)
X_lin = 2 * np.random.rand(100, 1)
y_lin = 4 + 3 * X_lin + np.random.randn(100, 1)
X_train_lin, X_test_lin, y_train_lin, y_test_lin = train_test_split(
    X_lin, y_lin, test_size=0.2, random_state=42
)


# --- 假设已有 X_train, X_test, y_train, y_test (来自项目二预处理后的数据) ---
# 如果没有，需要重新加载数据并划分
# 示例：使用上周的线性数据 X_train_lin, X_test_lin, y_train_lin, y_test_lin
# 注意：XGBoost 对特征缩放不敏感，但如果之前做了缩放也没关系

# --- 训练 XGBoost 回归模型 ---
# 1. 创建模型实例 (常用参数解释见下文)
xgb_reg = xgb.XGBRegressor(objective='reg:squarederror', # 回归任务，目标是最小化平方误差
                           n_estimators=100,         # 树的数量 (基学习器数量)
                           learning_rate=0.1,        # 学习率 (步长)，控制每棵树的贡献，防止过拟合
                           max_depth=3,              # 每棵树的最大深度
                           subsample=0.8,            # 训练每棵树时随机抽取的样本比例
                           colsample_bytree=0.8,     # 训练每棵树时随机抽取的特征比例
                           gamma=0,                  # 控制是否后剪枝的参数 (越大越保守)
                           reg_alpha=0,              # L1 正则化项系数
                           reg_lambda=1,             # L2 正则化项系数 (默认)
                           random_state=42,
                           n_jobs=-1)

# 2. 拟合模型
# XGBoost 可以使用验证集进行早停 (Early Stopping) 来防止过拟合，提高效率
# eval_set: 提供一个或多个验证集用于评估
# early_stopping_rounds: 如果验证集上的评估指标连续 N 轮没有改善，则停止训练
eval_set = [(X_train_lin, y_train_lin), (X_test_lin, y_test_lin)]
xgb_reg.fit(X_train_lin, y_train_lin,
            eval_set=eval_set,
            eval_metric='rmse', # 指定在验证集上监控的指标
            early_stopping_rounds=10,
            verbose=False) # verbose=True 会打印每一轮的评估结果

# --- 进行预测 ---
y_pred_xgb = xgb_reg.predict(X_test_lin)

# --- 评估模型 ---
print("\n--- XGBoost 回归评估 ---")
mse_xgb = mean_squared_error(y_test_lin, y_pred_xgb)
rmse_xgb = np.sqrt(mse_xgb)
r2_xgb = r2_score(y_test_lin, y_pred_xgb)

print(f"XGBoost 均方根误差 (RMSE): {rmse_xgb:.4f}")
print(f"XGBoost R 方 (R-squared): {r2_xgb:.4f}")

# --- (可选) 对比线性回归结果 ---
# from sklearn.linear_model import LinearRegression # 需要导入
# lin_reg = LinearRegression().fit(X_train_lin, y_train_lin)
# y_pred_lin = lin_reg.predict(X_test_lin)
# rmse_lin = np.sqrt(mean_squared_error(y_test_lin, y_pred_lin))
# r2_lin = r2_score(y_test_lin, y_pred_lin)
# print(f"\n线性回归 RMSE: {rmse_lin:.4f}")
# print(f"线性回归 R 方: {r2_lin:.4f}")
```

### 3.1 常用参数解释 (直观理解)

*   `objective`: 指定学习任务和对应的损失函数。回归常用 `'reg:squarederror'` (均方误差)。分类常用 `'binary:logistic'` (二分类对数损失) 或 `'multi:softmax'` / `'multi:softprob'` (多分类)。
*   `n_estimators`: 树的数量。太少可能欠拟合，太多可能过拟合（但有早停可以缓解）。通常从 100 开始尝试。
*   `learning_rate` (或 `eta`): 学习率。较小的值（如 0.01-0.3）通常需要更多的树 (`n_estimators`)，但模型更稳健。
*   `max_depth`: 每棵树的最大深度。控制树的复杂度，防止过拟合。常用 3-10。
*   `subsample`: 训练每棵树时使用的样本比例。引入随机性，防止过拟合。常用 0.5-1.0。
*   `colsample_bytree`: 构建每棵树时使用的特征比例。引入随机性，防止过拟合。常用 0.5-1.0。
*   `gamma`: 节点分裂所需的最小损失降低。值越大，算法越保守，越不容易分裂。
*   `reg_alpha` (L1) / `reg_lambda` (L2): 正则化系数。控制模型的复杂度。

::: {.callout-tip title="AI 辅助 XGBoost 学习"}
*   "解释 XGBoost 中的 `learning_rate` 和 `n_estimators` 参数如何相互影响？"
*   "XGBoost 如何处理缺失值？"
*   "对比 XGBoost 和 RandomForest 的主要区别。"
*   "查找 XGBoost `XGBRegressor` 的官方文档中关于 `subsample` 和 `colsample_bytree` 参数的详细说明。"
*   "什么是 XGBoost 的早停 (Early Stopping) 机制？它有什么好处？"
:::

### 3.2 实践：示例数据集训练 XGBoost

请使用一个简单回归数据集（例如上周的多项式数据 `X_poly`, `y_poly`，或者老师提供的数据），完成以下步骤：

1.  划分训练集和测试集。
2.  训练一个 `XGBRegressor` 模型（可以先用默认或推荐参数）。
3.  训练一个 `LinearRegression` 模型作为对比。
4.  在测试集上进行预测。
5.  计算并比较两个模型的 RMSE 和 R²。观察 XGBoost 是否比线性回归表现更好。

## 4. XGBoost 参数调优

XGBoost 有很多超参数，找到最优组合对模型性能至关重要。手动调优效率低下，我们可以使用系统性的方法：

*   **调优策略:** 通常建议按重要性顺序调整参数：
    1.  先确定一个较高的 `learning_rate` (如 0.1) 和合适的 `n_estimators` (通过早停机制或初步估计)。
    2.  调优树相关的参数 `max_depth`, `min_child_weight` (控制叶子节点样本权重和的阈值), `gamma`。
    3.  调优采样参数 `subsample`, `colsample_bytree`。
    4.  调优正则化参数 `reg_alpha`, `reg_lambda`。
    5.  最后，降低 `learning_rate`，同时相应增加 `n_estimators` (通常成反比增加)，看是否能进一步提升。
*   **工具:**
    *   **GridSearchCV:** 尝试所有参数组合，计算量大，但能找到全局最优（在给定网格内）。
    *   **RandomizedSearchCV:** 从参数分布中随机采样组合进行尝试，计算量较小，能在合理时间内找到较优解，适合参数空间很大时。
    *   **贝叶斯优化库 (如 Hyperopt, Optuna):** 更智能的调优方法，根据历史评估结果来选择下一个尝试的参数组合，效率更高。

```python
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from scipy.stats import uniform, randint # 用于 RandomizedSearchCV 的参数分布

# --- 使用 GridSearchCV 调优 XGBoost (示例) ---
# (注意：实际调优可能需要更细致的参数范围和更多折数，会非常耗时)

param_grid_xgb = {
    'max_depth': [3, 5, 7],
    'learning_rate': [0.05, 0.1, 0.2],
    'n_estimators': [100, 200],
    'subsample': [0.7, 0.8],
    'colsample_bytree': [0.7, 0.8]
    # 可以加入 gamma, reg_alpha, reg_lambda 等
}

xgb_reg_tune = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, n_jobs=-1)

grid_search_xgb = GridSearchCV(estimator=xgb_reg_tune,
                               param_grid=param_grid_xgb,
                               scoring='neg_root_mean_squared_error', # 使用负 RMSE，因为 GridSearchCV 默认找最大值
                               cv=3,
                               verbose=1)

# grid_search_xgb.fit(X_train, y_train) # 在项目数据上执行

# print("GridSearchCV 最优参数:", grid_search_xgb.best_params_)
# print("GridSearchCV 最优负 RMSE:", grid_search_xgb.best_score_)
# best_xgb_reg = grid_search_xgb.best_estimator_


# --- 使用 RandomizedSearchCV 调优 XGBoost (示例) ---
param_dist_xgb = {
    'max_depth': randint(3, 10), # 3 到 9 的随机整数
    'learning_rate': uniform(0.01, 0.3), # 0.01 到 0.31 之间的均匀分布
    'n_estimators': randint(100, 500),
    'subsample': uniform(0.6, 0.4), # 0.6 到 1.0 之间
    'colsample_bytree': uniform(0.6, 0.4),
    'gamma': uniform(0, 0.5),
    'reg_alpha': uniform(0, 1),
    'reg_lambda': uniform(0.5, 1.5) # 0.5 到 2.0 之间
}

random_search_xgb = RandomizedSearchCV(estimator=xgb_reg_tune,
                                       param_distributions=param_dist_xgb,
                                       n_iter=50, # 尝试 50 组随机参数组合
                                       scoring='neg_root_mean_squared_error',
                                       cv=3,
                                       verbose=1,
                                       random_state=42,
                                       n_jobs=-1)

# random_search_xgb.fit(X_train, y_train) # 在项目数据上执行

# print("RandomizedSearchCV 最优参数:", random_search_xgb.best_params_)
# print("RandomizedSearchCV 最优负 RMSE:", random_search_xgb.best_score_)
# best_xgb_reg_random = random_search_xgb.best_estimator_
```

## 5. 小组项目二：XGBoost 模型优化与报告

本周需要使用 XGBoost 来优化你的房价预测模型，并完成最终报告。

*   **任务:**
    1.  **训练 XGBoost 模型:** 在你的预处理数据上训练一个 `XGBRegressor` 模型。记得使用验证集和早停机制 (`fit` 方法中的 `eval_set`, `eval_metric`, `early_stopping_rounds`)。
    2.  **评估初始 XGBoost:** 评估该模型的性能 (RMSE, R²)，并与之前的线性回归、多项式回归、正则化回归模型进行比较。
    3.  **参数调优:**
        *   选择一种调优方法（GridSearchCV 或 RandomizedSearchCV）。
        *   定义合适的参数搜索空间 (`param_grid` 或 `param_distributions`)，至少包含 `n_estimators`, `learning_rate`, `max_depth`, `subsample`, `colsample_bytree` 等关键参数。
        *   执行参数搜索，找到最优的超参数组合。**（注意：这可能非常耗时，可以先用较小的搜索空间或较少的迭代次数进行尝试）。**
    4.  **评估最优 XGBoost:** 使用找到的最佳超参数训练最终的 XGBoost 模型，并在**测试集**上评估其性能。
    5.  **结果对比与分析:**
        *   全面比较所有尝试过的模型（线性、多项式、正则化、初始 XGBoost、调优后 XGBoost）在测试集上的最终性能。哪个模型效果最好？
        *   (可选) 分析最优 XGBoost 模型的特征重要性 (`feature_importances_`)。哪些因素对房价影响最大？
        *   总结项目二的整个流程、模型选择、调优过程、最终结果以及对房价预测任务的理解和洞察。
*   **提交:**
    *   最终的代码，包含所有模型训练、评估、调优的代码、结果和分析。
    *   一份简洁的项目报告，清晰呈现项目流程、方法、结果对比、最终模型性能、结论和思考。
*   **DDL:** 第七周第一次课前。

## 6. 本周总结

本周我们学习了强大的 Boosting 算法，特别是 XGBoost。我们了解了它的核心思想、优势以及如何在 Scikit-learn 中使用 `XGBRegressor` 进行回归任务。我们还探讨了 XGBoost 的关键参数和调优策略，包括使用 GridSearchCV 和 RandomizedSearchCV。最后，我们将 XGBoost 应用于项目二的优化，力求构建更精准的房价预测模型。

**下周我们将进入无监督学习领域，学习第一个聚类算法——K-Means！**