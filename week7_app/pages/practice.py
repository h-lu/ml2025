import streamlit as st
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score

from utils.app_utils import create_custom_header, create_info_box, create_expander
from utils.data_generator import generate_blob_data, generate_moons_data, generate_circles_data, generate_anisotropic_data
from utils.visualization import plot_clusters

def show_practice():
    """æ˜¾ç¤ºå®è·µç¯èŠ‚é¡µé¢"""
    create_custom_header("èšç±»åˆ†æå®è·µç¯èŠ‚", "åº”ç”¨èšç±»ç®—æ³•è§£å†³å®é™…é—®é¢˜", "ğŸ’»")
    
    # åˆ›å»ºæ ‡ç­¾é¡µ
    tab1, tab2, tab3 = st.tabs([
        "ç®—æ³•æ•ˆæœå¯¹æ¯”", 
        "æ•°æ®æ¢ç´¢å®è·µ", 
        "æŒ‘æˆ˜ä¸æ€è€ƒ"
    ])
    
    with tab1:
        show_algorithm_comparison()
    
    with tab2:
        show_data_exploration()
    
    with tab3:
        show_challenges()

def show_algorithm_comparison():
    """ä¸åŒèšç±»ç®—æ³•æ•ˆæœå¯¹æ¯”"""
    st.subheader("èšç±»ç®—æ³•æ•ˆæœå¯¹æ¯”")
    
    st.markdown("""
    åœ¨ä¸åŒå½¢çŠ¶çš„æ•°æ®é›†ä¸Šï¼Œä¸åŒèšç±»ç®—æ³•çš„è¡¨ç°å¯èƒ½å·®å¼‚å¾ˆå¤§ã€‚æœ¬èŠ‚æˆ‘ä»¬å°†æ¯”è¾ƒK-meansã€å±‚æ¬¡èšç±»å’ŒDBSCANåœ¨å„ç§å½¢çŠ¶æ•°æ®ä¸Šçš„è¡¨ç°ã€‚
    """)
    
    # é€‰æ‹©æ•°æ®é›†ç±»å‹
    dataset_type = st.selectbox(
        "é€‰æ‹©æ•°æ®é›†ç±»å‹:",
        ["å‡åŒ€åˆ†å¸ƒçš„çƒçŠ¶ç°‡", "æœˆç‰™å½¢æ•°æ®", "ç¯å½¢æ•°æ®", "ä¸å‡åŒ€åˆ†å¸ƒç°‡"],
        index=0
    )
    
    # ç”Ÿæˆé€‰å®šç±»å‹çš„æ•°æ®
    if dataset_type == "å‡åŒ€åˆ†å¸ƒçš„çƒçŠ¶ç°‡":
        X, y_true = generate_blob_data(n_samples=300, n_centers=3, random_state=42)
        expected_clusters = 3
    elif dataset_type == "æœˆç‰™å½¢æ•°æ®":
        X, y_true = generate_moons_data(n_samples=300, noise=0.1, random_state=42)
        expected_clusters = 2
    elif dataset_type == "ç¯å½¢æ•°æ®":
        X, y_true = generate_circles_data(n_samples=300, noise=0.05, factor=0.5, random_state=42)
        expected_clusters = 2
    else:  # ä¸å‡åŒ€åˆ†å¸ƒ
        X, y_true = generate_anisotropic_data(n_samples=300, n_centers=3, random_state=42)
        expected_clusters = 3
    
    # ç»˜åˆ¶åŸå§‹æ•°æ®
    fig, ax = plt.subplots(figsize=(10, 6))
    scatter = ax.scatter(X[:, 0], X[:, 1], c=y_true, cmap='viridis', s=50, alpha=0.8)
    ax.set_title("åŸå§‹æ•°æ®ï¼ˆçœŸå®æ ‡ç­¾ï¼‰")
    ax.set_aspect('equal')
    ax.grid(True, linestyle='--', alpha=0.7)
    st.pyplot(fig)
    
    # è®¾ç½®èšç±»å‚æ•°
    n_clusters = st.slider("é€‰æ‹©ç°‡æ•°é‡ (K):", 2, 6, expected_clusters)
    
    # æ‰§è¡Œèšç±»ç®—æ³•
    # 1. K-means
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    kmeans_labels = kmeans.fit_predict(X)
    
    # 2. å±‚æ¬¡èšç±»
    hierarchical = AgglomerativeClustering(n_clusters=n_clusters)
    hierarchical_labels = hierarchical.fit_predict(X)
    
    # 3. DBSCAN
    # å¯¹äºDBSCANï¼Œæˆ‘ä»¬éœ€è¦è®¾ç½®epså’Œmin_samples
    eps = st.slider("DBSCAN - é‚»åŸŸåŠå¾„ (eps):", 0.1, 2.0, 0.5, 0.1)
    min_samples = st.slider("DBSCAN - æœ€å°æ ·æœ¬æ•° (min_samples):", 2, 20, 5)
    
    dbscan = DBSCAN(eps=eps, min_samples=min_samples)
    dbscan_labels = dbscan.fit_predict(X)
    
    # è®¡ç®—è½®å»“ç³»æ•° (å¯¹äºDBSCANï¼Œå¦‚æœåªæœ‰ä¸€ä¸ªç°‡æˆ–å…¨æ˜¯å™ªå£°ç‚¹ï¼Œåˆ™æ— æ³•è®¡ç®—)
    kmeans_silhouette = silhouette_score(X, kmeans_labels) if len(np.unique(kmeans_labels)) > 1 else np.nan
    hierarchical_silhouette = silhouette_score(X, hierarchical_labels) if len(np.unique(hierarchical_labels)) > 1 else np.nan
    
    unique_dbscan_labels = np.unique(dbscan_labels)
    if len(unique_dbscan_labels) > 1 and -1 not in unique_dbscan_labels:
        dbscan_silhouette = silhouette_score(X, dbscan_labels)
    elif len(unique_dbscan_labels) > 2 and -1 in unique_dbscan_labels:
        # å¦‚æœæœ‰å™ªå£°ç‚¹ä½†ä¹Ÿæœ‰å¤šä¸ªç°‡ï¼Œæˆ‘ä»¬å¯ä»¥åªé’ˆå¯¹éå™ªå£°ç‚¹è®¡ç®—
        mask = dbscan_labels != -1
        if np.sum(mask) > 0 and len(np.unique(dbscan_labels[mask])) > 1:
            dbscan_silhouette = silhouette_score(X[mask], dbscan_labels[mask])
        else:
            dbscan_silhouette = np.nan
    else:
        dbscan_silhouette = np.nan
    
    # ç»˜åˆ¶èšç±»ç»“æœ
    fig, axes = plt.subplots(1, 3, figsize=(18, 6))
    
    # K-meansç»“æœ
    axes[0].scatter(X[:, 0], X[:, 1], c=kmeans_labels, cmap='viridis', s=50, alpha=0.8)
    kmeans_score = f"{kmeans_silhouette:.3f}" if not np.isnan(kmeans_silhouette) else "N/A"
    axes[0].set_title(f"K-means (K={n_clusters})\nè½®å»“ç³»æ•°: {kmeans_score}")
    axes[0].set_aspect('equal')
    axes[0].grid(True, linestyle='--', alpha=0.7)
    
    # å±‚æ¬¡èšç±»ç»“æœ
    axes[1].scatter(X[:, 0], X[:, 1], c=hierarchical_labels, cmap='viridis', s=50, alpha=0.8)
    hierarchical_score = f"{hierarchical_silhouette:.3f}" if not np.isnan(hierarchical_silhouette) else "N/A"
    axes[1].set_title(f"å±‚æ¬¡èšç±» (K={n_clusters})\nè½®å»“ç³»æ•°: {hierarchical_score}")
    axes[1].set_aspect('equal')
    axes[1].grid(True, linestyle='--', alpha=0.7)
    
    # DBSCANç»“æœ
    scatter = axes[2].scatter(X[:, 0], X[:, 1], c=dbscan_labels, cmap='viridis', s=50, alpha=0.8)
    dbscan_score = f"{dbscan_silhouette:.3f}" if not np.isnan(dbscan_silhouette) else "N/A"
    axes[2].set_title(f"DBSCAN (eps={eps}, min_samples={min_samples})\nè½®å»“ç³»æ•°: {dbscan_score}")
    axes[2].set_aspect('equal')
    axes[2].grid(True, linestyle='--', alpha=0.7)
    
    # æ·»åŠ å›¾ä¾‹ï¼ˆåªä¸ºDBSCANæ·»åŠ ï¼Œå› ä¸ºå¯èƒ½æœ‰å™ªå£°ç‚¹ï¼‰
    if -1 in dbscan_labels:
        legend_elements = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='gray', 
                                    markersize=10, label='å™ªå£°ç‚¹')]
        axes[2].legend(handles=legend_elements, loc='upper right')
    
    plt.tight_layout()
    st.pyplot(fig)
    
    # åˆ†æç»“æœ
    st.markdown("### è§‚å¯Ÿä¸åˆ†æ")
    
    st.markdown(f"""
    æ ¹æ®ä¸Šé¢çš„èšç±»ç»“æœï¼Œæˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°ï¼š
    
    **K-means:**
    - ä¼˜ç‚¹: ç®€å•é«˜æ•ˆï¼Œå¯¹çƒå½¢æ•°æ®æ•ˆæœå¥½
    - é™åˆ¶: å¯¹éçƒå½¢æ•°æ®è¡¨ç°è¾ƒå·®ï¼Œæ— æ³•è¯†åˆ«å™ªå£°ç‚¹
    - åœ¨å½“å‰æ•°æ®ä¸Šè¡¨ç°: {"è‰¯å¥½" if kmeans_silhouette > 0.5 else "ä¸€èˆ¬" if kmeans_silhouette > 0.3 else "è¾ƒå·®"}
    
    **å±‚æ¬¡èšç±»:**
    - ä¼˜ç‚¹: å¯ä»¥å¤„ç†å„ç§å½¢çŠ¶çš„ç°‡ï¼Œæä¾›èšç±»çš„å±‚æ¬¡ç»“æ„
    - é™åˆ¶: è®¡ç®—å¤æ‚åº¦é«˜ï¼Œä¸é€‚åˆå¤§å‹æ•°æ®é›†
    - åœ¨å½“å‰æ•°æ®ä¸Šè¡¨ç°: {"è‰¯å¥½" if hierarchical_silhouette > 0.5 else "ä¸€èˆ¬" if hierarchical_silhouette > 0.3 else "è¾ƒå·®"}
    
    **DBSCAN:**
    - ä¼˜ç‚¹: èƒ½å¤Ÿå‘ç°ä»»æ„å½¢çŠ¶çš„ç°‡ï¼Œè‡ªåŠ¨è¯†åˆ«å™ªå£°ç‚¹
    - é™åˆ¶: å‚æ•°æ•æ„Ÿï¼Œå¯¹é«˜ç»´æ•°æ®æ•ˆæœå¯èƒ½ä¸ä½³
    - åœ¨å½“å‰æ•°æ®ä¸Šè¡¨ç°: {"è‰¯å¥½" if not np.isnan(dbscan_silhouette) and dbscan_silhouette > 0.5 else "ä¸€èˆ¬" if not np.isnan(dbscan_silhouette) and dbscan_silhouette > 0.3 else "è¾ƒå·®"}
    
    **æ€»ä½“åˆ†æ:**
    - å¯¹äº{dataset_type}ï¼Œ{
        "K-meansè¡¨ç°æœ€ä½³" if kmeans_silhouette == max([kmeans_silhouette, hierarchical_silhouette, dbscan_silhouette if not np.isnan(dbscan_silhouette) else -float('inf')]) else 
        "å±‚æ¬¡èšç±»è¡¨ç°æœ€ä½³" if hierarchical_silhouette == max([kmeans_silhouette, hierarchical_silhouette, dbscan_silhouette if not np.isnan(dbscan_silhouette) else -float('inf')]) else
        "DBSCANè¡¨ç°æœ€ä½³" if not np.isnan(dbscan_silhouette) and dbscan_silhouette == max([kmeans_silhouette, hierarchical_silhouette, dbscan_silhouette]) else
        "å„ç®—æ³•è¡¨ç°ç›¸ä¼¼"
    }
    """)
    
    # æç¤ºç”¨æˆ·å°è¯•ä¸åŒæ•°æ®é›†å’Œå‚æ•°
    create_info_box("å°è¯•ä¸åŒæ•°æ®é›†ç±»å‹å’Œå‚æ•°è®¾ç½®ï¼Œè§‚å¯Ÿèšç±»ç®—æ³•åœ¨ä¸åŒæƒ…å†µä¸‹çš„è¡¨ç°ã€‚ç‰¹åˆ«å…³æ³¨ç®—æ³•å¯¹éçƒå½¢æ•°æ®çš„å¤„ç†èƒ½åŠ›ã€‚", "info")

def show_data_exploration():
    """æ•°æ®æ¢ç´¢å®è·µ"""
    st.subheader("èšç±»åˆ†æå®è·µï¼šæ¢ç´¢çœŸå®æ•°æ®")
    
    st.markdown("""
    åœ¨è¿™ä¸ªç¯èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨çœŸå®ä¸–ç•Œçš„æ•°æ®é›†è¿›è¡Œèšç±»åˆ†æï¼Œå¹¶æ¢ç´¢å¦‚ä½•è§£é‡Šèšç±»ç»“æœã€‚
    """)
    
    # åŠ è½½æ¼”ç¤ºæ•°æ®
    st.markdown("### æ•°æ®ï¼šé¸¢å°¾èŠ±æ•°æ®é›†ï¼ˆIris Datasetï¼‰")
    
    st.markdown("""
    é¸¢å°¾èŠ±æ•°æ®é›†åŒ…å«ä¸‰ç§ä¸åŒç§ç±»é¸¢å°¾èŠ±çš„æµ‹é‡æ•°æ®ï¼š
    - èŠ±è¼é•¿åº¦ (Sepal Length)
    - èŠ±è¼å®½åº¦ (Sepal Width)
    - èŠ±ç“£é•¿åº¦ (Petal Length)
    - èŠ±ç“£å®½åº¦ (Petal Width)
    
    æˆ‘ä»¬å°†å¿½ç•¥çœŸå®æ ‡ç­¾ï¼Œä½¿ç”¨èšç±»ç®—æ³•æ¥å°è¯•å‘ç°æ•°æ®ä¸­çš„è‡ªç„¶åˆ†ç»„ã€‚
    """)
    
    # åŠ è½½é¸¢å°¾èŠ±æ•°æ®é›†
    from sklearn.datasets import load_iris
    
    iris = load_iris()
    X = iris.data
    y_true = iris.target
    feature_names = iris.feature_names
    
    # æ˜¾ç¤ºæ•°æ®é›†ä¿¡æ¯
    df = pd.DataFrame(X, columns=feature_names)
    df['species'] = [iris.target_names[i] for i in y_true]
    
    st.markdown("**æ•°æ®é›†é¢„è§ˆ:**")
    st.dataframe(df.head())
    
    st.markdown(f"**æ•°æ®å½¢çŠ¶:** {X.shape[0]} è¡Œ Ã— {X.shape[1]} åˆ—")
    
    # ç‰¹å¾é€‰æ‹©
    st.markdown("### ç‰¹å¾é€‰æ‹©")
    
    st.markdown("""
    ä¸ºäº†ç®€åŒ–å¯è§†åŒ–ï¼Œæˆ‘ä»¬å°†é€‰æ‹©ä¸¤ä¸ªç‰¹å¾è¿›è¡Œèšç±»ã€‚è¯·é€‰æ‹©ä½ æƒ³è¦ä½¿ç”¨çš„ç‰¹å¾ï¼š
    """)
    
    feature1 = st.selectbox("ç‰¹å¾ 1:", feature_names, index=0)
    feature2 = st.selectbox("ç‰¹å¾ 2:", feature_names, index=2)
    
    # æå–æ‰€é€‰ç‰¹å¾
    X_selected = df[[feature1, feature2]].values
    
    # æ•°æ®é¢„å¤„ç†é€‰é¡¹
    scale_data = st.checkbox("æ ‡å‡†åŒ–æ•°æ® (å‡å€¼=0, æ ‡å‡†å·®=1)", value=True)
    
    if scale_data:
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X_selected)
    else:
        X_scaled = X_selected
    
    # ç»˜åˆ¶åŸå§‹æ•°æ®ï¼ˆå¸¦çœŸå®æ ‡ç­¾ï¼‰
    fig, ax = plt.subplots(figsize=(10, 6))
    scatter = ax.scatter(X_selected[:, 0], X_selected[:, 1], c=y_true, cmap='viridis', s=50, alpha=0.8)
    
    # æ·»åŠ å›¾ä¾‹
    handles, labels = scatter.legend_elements()
    # ç¡®ä¿handlesæ˜¯æ™®é€šPythonåˆ—è¡¨è€Œä¸æ˜¯NumPyæ•°ç»„
    handles = list(handles)
    
    # ç›´æ¥ä½¿ç”¨å…·åå‚æ•°å¹¶ä¼ å…¥PythonåŸç”Ÿç±»å‹
    ax.legend(handles=handles, 
             labels=list(iris.target_names),
             loc="upper right", 
             title="ç‰©ç§")
    
    ax.set_xlabel(feature1)
    ax.set_ylabel(feature2)
    ax.set_title("åŸå§‹æ•°æ®ï¼ˆçœŸå®æ ‡ç­¾ï¼‰")
    ax.grid(True, linestyle='--', alpha=0.7)
    st.pyplot(fig)
    
    # é€‰æ‹©èšç±»ç®—æ³•
    algorithm = st.selectbox(
        "é€‰æ‹©èšç±»ç®—æ³•:",
        ["K-means", "å±‚æ¬¡èšç±» (Agglomerative)", "DBSCAN"],
        index=0
    )
    
    # è®¾ç½®ç®—æ³•å‚æ•°
    if algorithm == "K-means" or algorithm == "å±‚æ¬¡èšç±» (Agglomerative)":
        n_clusters = st.slider("é€‰æ‹©ç°‡æ•°é‡ (K):", 2, 8, 3)
    
    if algorithm == "DBSCAN":
        eps = st.slider("é‚»åŸŸåŠå¾„ (eps):", 0.1, 2.0, 0.5, 0.1)
        min_samples = st.slider("æœ€å°æ ·æœ¬æ•° (min_samples):", 2, 20, 5)
    
    # æ‰§è¡Œèšç±»
    if algorithm == "K-means":
        model = KMeans(n_clusters=n_clusters, random_state=42)
    elif algorithm == "å±‚æ¬¡èšç±» (Agglomerative)":
        linkage = st.selectbox("é€‰æ‹©linkageæ–¹æ³•:", ["ward", "complete", "average", "single"])
        model = AgglomerativeClustering(n_clusters=n_clusters, linkage=linkage)
    else:  # DBSCAN
        model = DBSCAN(eps=eps, min_samples=min_samples)
    
    y_pred = model.fit_predict(X_scaled)
    
    # å¯¹äºK-meansï¼Œæˆ‘ä»¬å¯ä»¥è·å–ç°‡ä¸­å¿ƒ
    if algorithm == "K-means":
        # å¦‚æœæ•°æ®è¢«ç¼©æ”¾ï¼Œæˆ‘ä»¬éœ€è¦å°†ç°‡ä¸­å¿ƒè½¬å›åŸå§‹å°ºåº¦
        if scale_data:
            centers = scaler.inverse_transform(model.cluster_centers_)
        else:
            centers = model.cluster_centers_
    else:
        centers = None
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡ï¼ˆå¦‚æœå¯èƒ½ï¼‰
    try:
        if len(np.unique(y_pred)) > 1 and -1 not in y_pred:
            silhouette = silhouette_score(X_scaled, y_pred)
            show_silhouette = True
        elif len(np.unique(y_pred)) > 2 and -1 in y_pred:
            # å¦‚æœæœ‰å™ªå£°ç‚¹ä½†ä¹Ÿæœ‰å¤šä¸ªç°‡ï¼Œæˆ‘ä»¬å¯ä»¥åªé’ˆå¯¹éå™ªå£°ç‚¹è®¡ç®—
            mask = y_pred != -1
            if np.sum(mask) > 1 and len(np.unique(y_pred[mask])) > 1:
                silhouette = silhouette_score(X_scaled[mask], y_pred[mask])
                show_silhouette = True
            else:
                show_silhouette = False
        else:
            show_silhouette = False
    except:
        show_silhouette = False
    
    # ç»˜åˆ¶èšç±»ç»“æœ
    fig, ax = plt.subplots(figsize=(10, 6))
    
    # ç»˜åˆ¶æ•°æ®ç‚¹
    scatter = ax.scatter(X_selected[:, 0], X_selected[:, 1], c=y_pred, cmap='viridis', s=50, alpha=0.8)
    
    # ç»˜åˆ¶ç°‡ä¸­å¿ƒï¼ˆä»…å¯¹K-meansï¼‰
    if centers is not None:
        ax.scatter(centers[:, 0], centers[:, 1], s=200, marker='X', c='red', edgecolors='k', label='ç°‡ä¸­å¿ƒ')
        ax.legend()
    
    # ä¸ºDBSCANæ·»åŠ å™ªå£°ç‚¹å›¾ä¾‹
    if algorithm == "DBSCAN" and -1 in y_pred:
        handles, labels = scatter.legend_elements()
        # å°†NumPyæ•°ç»„è½¬æ¢ä¸ºPythonåˆ—è¡¨ï¼Œé¿å…å¸ƒå°”åˆ¤æ–­é—®é¢˜
        handles = list(handles)
        labels = list(labels)
        labels = [label if i != 0 else "å™ªå£°ç‚¹" for i, label in enumerate(labels)]
        ax.legend(handles=handles, labels=labels, loc="upper right", title="ç°‡")
    
    ax.set_xlabel(feature1)
    ax.set_ylabel(feature2)
    alg_name = algorithm
    if algorithm == "K-means":
        alg_name += f" (K={n_clusters})"
    elif algorithm == "å±‚æ¬¡èšç±» (Agglomerative)":
        alg_name += f" (K={n_clusters}, linkage={linkage})"
    else:  # DBSCAN
        alg_name += f" (eps={eps}, min_samples={min_samples})"
    
    if show_silhouette:
        ax.set_title(f"{alg_name}\nè½®å»“ç³»æ•°: {silhouette:.3f}")
    else:
        ax.set_title(alg_name)
    
    ax.grid(True, linestyle='--', alpha=0.7)
    st.pyplot(fig)
    
    # ä¸çœŸå®æ ‡ç­¾æ¯”è¾ƒ
    st.markdown("### èšç±»ç»“æœä¸çœŸå®æ ‡ç­¾æ¯”è¾ƒ")
    
    # åˆ›å»ºäº¤å‰è¡¨
    cross_tab = pd.crosstab(y_true, y_pred, 
                           rownames=['çœŸå®ç‰©ç§'], 
                           colnames=['é¢„æµ‹ç°‡'])
    
    # æ·»åŠ è¡Œæ ‡ç­¾
    cross_tab.index = [iris.target_names[i] for i in range(len(iris.target_names))]
    
    st.table(cross_tab)
    
    # è®¡ç®—å¤–éƒ¨è¯„ä¼°æŒ‡æ ‡
    from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score
    
    # å¯¹äºåŒ…å«å™ªå£°ç‚¹çš„DBSCANç»“æœï¼Œæˆ‘ä»¬åªè€ƒè™‘è¢«åˆ†é…åˆ°ç°‡çš„ç‚¹
    if algorithm == "DBSCAN" and -1 in y_pred:
        mask = y_pred != -1
        if np.sum(mask) > 0:
            ari = adjusted_rand_score(y_true[mask], y_pred[mask])
            ami = adjusted_mutual_info_score(y_true[mask], y_pred[mask])
            noise_percentage = 100 * (1 - np.sum(mask) / len(mask))
        else:
            ari = ami = np.nan
            noise_percentage = 100
    else:
        ari = adjusted_rand_score(y_true, y_pred)
        ami = adjusted_mutual_info_score(y_true, y_pred)
        noise_percentage = 0
    
    cols = st.columns(3)
    
    with cols[0]:
        st.metric("è°ƒæ•´å…°å¾·æŒ‡æ•° (ARI)", f"{ari:.3f}" if not np.isnan(ari) else "N/A")
    
    with cols[1]:
        st.metric("è°ƒæ•´äº’ä¿¡æ¯ (AMI)", f"{ami:.3f}" if not np.isnan(ami) else "N/A")
    
    if algorithm == "DBSCAN" and -1 in y_pred:
        with cols[2]:
            st.metric("å™ªå£°ç‚¹æ¯”ä¾‹", f"{noise_percentage:.1f}%")
    
    # åˆ†æä¸è§£é‡Š
    st.markdown("### èšç±»ç»“æœåˆ†æ")
    
    # æå‰è®¡ç®—æ‰€æœ‰å¤æ‚è¡¨è¾¾å¼ï¼Œé¿å…åœ¨f-stringä¸­å‡ºç°å¤æ‚åµŒå¥—
    clusters_info = ""
    if algorithm != "K-means" and algorithm != "å±‚æ¬¡èšç±» (Agglomerative)":
        cluster_count = len(np.unique(y_pred)) if -1 not in y_pred else len(np.unique(y_pred))-1
        clusters_info = f"å¯¹æ¯”çœŸå®æ ‡ç­¾ï¼ˆ3ç§ç‰©ç§ï¼‰ï¼Œç®—æ³•è¯†åˆ«å‡ºäº†{cluster_count}ä¸ªç°‡"
    else:
        clusters_info = f"ç®—æ³•è¢«è®¾ç½®ä¸ºå¯»æ‰¾{n_clusters}ä¸ªç°‡"
    
    ari_value = f"{ari:.3f}" if not np.isnan(ari) else "æ— æ³•è®¡ç®—"
    
    if not np.isnan(ari):
        if ari > 0.7:
            ari_quality = "(è‰¯å¥½)"
        elif ari > 0.3:
            ari_quality = "(ä¸€èˆ¬)"
        else:
            ari_quality = "(è¾ƒå·®)"
    else:
        ari_quality = ""
    
    cluster_observation = ""
    if not np.isnan(ari) and ari > 0.3:
        cluster_observation = "èšç±»ç®—æ³•èƒ½å¤Ÿå¾ˆå¥½åœ°åŒºåˆ†Setosaç‰©ç§ï¼Œä½†å¯¹Versicolorå’ŒVirginicaçš„åŒºåˆ†è¾ƒä¸ºå›°éš¾"
    else:
        cluster_observation = "èšç±»æ•ˆæœä¸çœŸå®ç‰©ç§åˆ†ç±»æœ‰è¾ƒå¤§å·®å¼‚"
    
    cluster_count = len(np.unique(y_pred)) if -1 not in y_pred else len(np.unique(y_pred))-1
    feature_observation = f"åœ¨æ‰€é€‰çš„ä¸¤ä¸ªç‰¹å¾({feature1}å’Œ{feature2})ç©ºé—´ä¸­ï¼Œæ•°æ®å‘ˆç°{cluster_count}ä¸ªè‡ªç„¶åˆ†ç»„"
    
    feature_effect = ""
    if ari < 0.7 and not np.isnan(ari):
        feature_effect = f"ç‰¹å¾é€‰æ‹©å½±å“äº†èšç±»æ•ˆæœ - {feature1}å’Œ{feature2}å¯èƒ½æ— æ³•å®Œå…¨åˆ†ç¦»ä¸‰ä¸ªç‰©ç§"
    else:
        feature_effect = "æ‰€é€‰ç‰¹å¾èƒ½å¤Ÿå¾ˆå¥½åœ°è¡¨ç°æ•°æ®çš„è‡ªç„¶åˆ†ç»„ç»“æ„"
    
    algorithm_effect = ""
    if algorithm == "DBSCAN" or algorithm == "å±‚æ¬¡èšç±» (Agglomerative)":
        algorithm_effect = "ç®—æ³•å‚æ•°è®¾ç½®å¯¹ç»“æœæœ‰æ˜¾è‘—å½±å“"
    else:
        algorithm_effect = "Kçš„é€‰æ‹©å¯¹K-meansç»“æœå½±å“æ˜¾è‘—"
    
    # ä½¿ç”¨æ›´ç®€å•çš„f-stringæ ¼å¼
    st.markdown(f"""
    **ç®—æ³•è¡¨ç°åˆ†æ:**
    
    - æˆ‘ä»¬ä½¿ç”¨äº†{algorithm}ç®—æ³•å¯¹é¸¢å°¾èŠ±æ•°æ®è¿›è¡Œèšç±»
    - {clusters_info}
    - ä¸çœŸå®ç‰©ç§çš„åŒ¹é…ç¨‹åº¦ (ARI): {ari_value} {ari_quality}
    
    **ä»èšç±»ç»“æœä¸­èƒ½å­¦åˆ°ä»€ä¹ˆ?**
    
    é€šè¿‡è§‚å¯Ÿèšç±»ç»“æœå’Œäº¤å‰è¡¨ï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°:
    
    - {cluster_observation}
    - {feature_observation}
    
    **ä¸ºä»€ä¹ˆä¼šæœ‰è¿™æ ·çš„ç»“æœ?**
    
    - {feature_effect}
    - {algorithm_effect}
    """)
    
    # æ‰©å±•:å°è¯•å…¶ä»–ç‰¹å¾ç»„åˆ
    st.markdown("### è¿›ä¸€æ­¥æ¢ç´¢")
    
    st.markdown("""
    ä¸ºäº†æ›´å…¨é¢åœ°ç†è§£æ•°æ®ï¼Œå°è¯•ä»¥ä¸‹æ“ä½œ:
    
    1. é€‰æ‹©ä¸åŒçš„ç‰¹å¾ç»„åˆï¼Œè§‚å¯Ÿèšç±»æ•ˆæœå˜åŒ–
    2. è°ƒæ•´ç®—æ³•å‚æ•°ï¼Œå¯»æ‰¾æœ€ä½³èšç±»ç»“æœ
    3. æ¯”è¾ƒä¸åŒèšç±»ç®—æ³•åœ¨ç›¸åŒç‰¹å¾ä¸Šçš„è¡¨ç°å·®å¼‚
    """)

def show_challenges():
    """æŒ‘æˆ˜ä¸æ€è€ƒé—®é¢˜"""
    st.subheader("æŒ‘æˆ˜ä¸æ€è€ƒ")
    
    st.markdown("""
    ä»¥ä¸‹æ˜¯ä¸€äº›å…³äºèšç±»åˆ†æçš„è¿›é˜¶é—®é¢˜å’ŒæŒ‘æˆ˜ï¼Œä¾›ä½ æ€è€ƒï¼š
    """)
    
    # æ€è€ƒé—®é¢˜
    questions = [
        {
            "title": "é«˜ç»´ç©ºé—´çš„è¯…å’’",
            "content": """
            éšç€æ•°æ®ç»´åº¦çš„å¢åŠ ï¼Œèšç±»ç®—æ³•é¢ä¸´"é«˜ç»´ç©ºé—´çš„è¯…å’’"é—®é¢˜ï¼š
            
            1. è·ç¦»åº¦é‡å˜å¾—ä¸é‚£ä¹ˆæœ‰æ„ä¹‰ï¼Œå› ä¸ºé«˜ç»´ç©ºé—´ä¸­ç‚¹ä¸ç‚¹ä¹‹é—´çš„è·ç¦»è¶‹äºç›¸ç­‰
            2. æ•°æ®å˜å¾—æ›´ç¨€ç–ï¼Œéš¾ä»¥å½¢æˆå¯†é›†ç°‡
            
            **æ€è€ƒï¼š** å¦‚ä½•åœ¨é«˜ç»´æ•°æ®ï¼ˆå¦‚æ–‡æœ¬ã€åŸºå› è¡¨è¾¾æ•°æ®ï¼‰ä¸Šæœ‰æ•ˆåº”ç”¨èšç±»åˆ†æï¼Ÿ
            
            **å¯èƒ½çš„è§£å†³æ–¹æ¡ˆåŒ…æ‹¬ï¼š**
            - é™ç»´æŠ€æœ¯ï¼ˆå¦‚PCAã€t-SNEï¼‰
            - ç‰¹å¾é€‰æ‹©
            - å­ç©ºé—´èšç±»æ–¹æ³•
            """
        },
        {
            "title": "æ··åˆåˆ†å¸ƒçš„èšç±»",
            "content": """
            åœ¨å®é™…åº”ç”¨ä¸­ï¼Œæ•°æ®å¯èƒ½æ¥è‡ªå¤šä¸ªä¸åŒç±»å‹çš„åˆ†å¸ƒï¼š
            
            **æ€è€ƒï¼š** å¦‚ä½•å¤„ç†åŒ…å«ä¸åŒå½¢çŠ¶ã€å¤§å°å’Œå¯†åº¦ç°‡çš„æ•°æ®ï¼Ÿ
            
            **å°è¯•ï¼š** 
            - ç»„åˆå¤šç§èšç±»ç®—æ³•ï¼ˆå¦‚ï¼Œå…ˆç”¨DBSCANè¯†åˆ«å¯†é›†åŒºåŸŸï¼Œå†å¯¹å‰©ä½™ç‚¹ç”¨K-meansï¼‰
            - è€ƒè™‘å¯†åº¦æ•æ„Ÿçš„ç®—æ³•ï¼ˆå¦‚HDBSCANã€OPTICSï¼‰
            - ä½¿ç”¨æ›´çµæ´»çš„æ¨¡å‹ï¼ˆå¦‚æ··åˆæ¨¡å‹ï¼‰
            """
        },
        {
            "title": "å¤§è§„æ¨¡æ•°æ®èšç±»",
            "content": """
            å¯¹äºéå¸¸å¤§å‹çš„æ•°æ®é›†ï¼ˆç™¾ä¸‡åˆ°äº¿çº§æ•°æ®ç‚¹ï¼‰ï¼Œä¼ ç»Ÿèšç±»ç®—æ³•å¯èƒ½åœ¨è®¡ç®—ä¸Šä¸å¯è¡Œã€‚
            
            **æ€è€ƒï¼š** å¦‚ä½•æ‰©å±•èšç±»ç®—æ³•ä»¥å¤„ç†å¤§è§„æ¨¡æ•°æ®ï¼Ÿ
            
            **å¯èƒ½çš„æ–¹æ³•ï¼š**
            - é‡‡æ ·æŠ€æœ¯ï¼ˆå…ˆåœ¨æ ·æœ¬ä¸Šèšç±»ï¼Œå†å°†ç»“æœæ¨å¹¿ï¼‰
            - åœ¨çº¿/æµå¼èšç±»æ–¹æ³•ï¼ˆå•æ¬¡æ‰«ææ•°æ®ï¼‰
            - åˆ†å¸ƒå¼/å¹¶è¡Œèšç±»ç®—æ³•
            - è¿‘ä¼¼æœ€è¿‘é‚»æ–¹æ³•
            """
        },
        {
            "title": "è‡ªåŠ¨å‚æ•°é€‰æ‹©",
            "content": """
            èšç±»ç®—æ³•é€šå¸¸éœ€è¦æ‰‹åŠ¨æŒ‡å®šå…³é”®å‚æ•°ï¼ˆå¦‚Kå€¼ã€epså€¼ç­‰ï¼‰ã€‚
            
            **æŒ‘æˆ˜ï¼š** å¦‚ä½•è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜å‚æ•°è€Œä¸ä¾èµ–çœŸå®æ ‡ç­¾ï¼Ÿ
            
            **æ¢ç´¢ï¼š**
            - å†…éƒ¨è¯„ä¼°æŒ‡æ ‡çš„ç¨³å®šæ€§åˆ†æ
            - æ¨¡å‹ç¨³å®šæ€§æ–¹æ³•ï¼ˆå¦‚bootstrapé‡é‡‡æ ·ï¼‰
            - ä¿¡æ¯è®ºæ–¹æ³•ï¼ˆMDLåŸåˆ™ç­‰ï¼‰
            - Gapç»Ÿè®¡é‡å’Œç±»ä¼¼æ–¹æ³•
            """
        }
    ]
    
    # æ˜¾ç¤ºæ€è€ƒé—®é¢˜
    for i, q in enumerate(questions):
        with st.expander(f"{i+1}. {q['title']}", expanded=i==0):
            st.markdown(q["content"])
    
    # å¼€æ”¾æ€§æŒ‘æˆ˜
    st.markdown("### å¼€æ”¾æ€§æŒ‘æˆ˜")
    
    st.markdown("""
    ä»¥ä¸‹æ˜¯ä¸€äº›å¯ä»¥å°è¯•çš„å¼€æ”¾æ€§æŒ‘æˆ˜ï¼š
    
    1. **æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹ï¼š** ä½¿ç”¨èšç±»æ–¹æ³•æ£€æµ‹æ•°æ®ä¸­çš„å¼‚å¸¸ç‚¹ï¼Œæ¯”è¾ƒä¸åŒæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚
    
    2. **æ··åˆæ•°æ®èšç±»ï¼š** è®¾è®¡ä¸€ç§æ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°èšç±»åŒ…å«æ•°å€¼å’Œåˆ†ç±»ç‰¹å¾çš„æ··åˆæ•°æ®ã€‚
    
    3. **åŠ¨æ€æ•°æ®èšç±»ï¼š** æ¢ç´¢å¦‚ä½•å¯¹éšæ—¶é—´å˜åŒ–çš„æ•°æ®è¿›è¡Œèšç±»ï¼Œæ—¢è¦åæ˜ å½“å‰çŠ¶æ€ï¼Œåˆè¦è€ƒè™‘å†å²è¶‹åŠ¿ã€‚
    
    4. **è§†è§‰åŒ–åˆ†æï¼š** å¼€å‘æ›´ç›´è§‚çš„èšç±»ç»“æœå¯è§†åŒ–æ–¹æ³•ï¼Œå°¤å…¶æ˜¯å¯¹äºé«˜ç»´æ•°æ®ã€‚
    
    5. **é¢†åŸŸåº”ç”¨ï¼š** é€‰æ‹©ä¸€ä¸ªç‰¹å®šé¢†åŸŸï¼ˆå¦‚åŒ»ç–—å¥åº·ã€é‡‘èã€ç¤¾äº¤ç½‘ç»œç­‰ï¼‰ï¼Œåº”ç”¨èšç±»åˆ†æè§£å†³è¯¥é¢†åŸŸçš„å®é™…é—®é¢˜ã€‚
    """)
    
    # èµ„æºæ¨è
    st.markdown("### æ‰©å±•å­¦ä¹ èµ„æº")
    
    st.markdown("""
    å¦‚æœä½ å¸Œæœ›æ·±å…¥äº†è§£èšç±»åˆ†æï¼Œä»¥ä¸‹æ˜¯ä¸€äº›æ¨èçš„å­¦ä¹ èµ„æºï¼š
    
    **ä¹¦ç±ï¼š**
    - *"æ•°æ®æŒ–æ˜ï¼šæ¦‚å¿µä¸æŠ€æœ¯"* (Jiawei Han, Micheline Kamber, Jian Pei)
    - *"Pattern Recognition and Machine Learning"* (Christopher Bishop)
    - *"Elements of Statistical Learning"* (Trevor Hastie, Robert Tibshirani, Jerome Friedman)
    
    **åœ¨çº¿è¯¾ç¨‹ä¸æ•™ç¨‹ï¼š**
    - Scikit-learnèšç±»æ–‡æ¡£ï¼š[scikit-learn.org/stable/modules/clustering.html](https://scikit-learn.org/stable/modules/clustering.html)
    - Stanford CS246: æŒ–æ˜å¤§è§„æ¨¡æ•°æ®é›†
    - Courseraä¸Šçš„æ•°æ®æŒ–æ˜ä¸“é¡¹è¯¾ç¨‹
    
    **å­¦æœ¯è®ºæ–‡ï¼š**
    - *"A Survey of Clustering Data Mining Techniques"* (Pavel Berkhin)
    - *"Clustering"* (Jain et al., 1999)
    - *"A Density-Based Algorithm for Discovering Clusters"* (DBSCANè®ºæ–‡)
    
    **Pythonå·¥å…·åŒ…ï¼š**
    - scikit-learn: æœ€å¸¸ç”¨çš„æœºå™¨å­¦ä¹ åº“
    - HDBSCAN: DBSCANçš„å±‚æ¬¡åŒ–æ‰©å±•
    - yellowbrick: æœºå™¨å­¦ä¹ å¯è§†åŒ–
    """)
    
    create_info_box("è®°ä½ï¼šèšç±»åˆ†ææ˜¯ä¸€é—¨è‰ºæœ¯ï¼Œä¹Ÿæ˜¯ä¸€é—¨ç§‘å­¦ã€‚æŠ€æœ¯å·¥å…·å¾ˆé‡è¦ï¼Œä½†å¯¹ä¸šåŠ¡é¢†åŸŸçš„ç†è§£å’Œæ‰¹åˆ¤æ€§æ€ç»´åŒæ ·ä¸å¯æˆ–ç¼ºã€‚", "info") 