---
title: "深入探索聚类分析：K-means 与层次聚类"
---

## 课程目标

本节课结束时，学生将能够：

*   **深入理解**聚类分析的核心概念、目标和多样化的应用场景。
*   **熟练掌握** K-means 聚类算法的原理、数学表达、步骤，并能分析其对初始化的敏感性及 K 值选择策略。
*   **清晰理解**凝聚型层次聚类的原理、不同 Linkage 方法的计算方式及其对结果的影响，并能解读树状图。
*   **掌握**常用的聚类评估指标（内部与外部）及其应用场景。
*   **能够**根据数据特点和分析目标，** обоснованно (justifiably)** 选择合适的聚类算法。
*   **初步了解**在 Python 中实现基本聚类算法的方法。
*   **培养**对算法局限性的批判性思维和解决实际问题的能力。

## 1. 聚类分析：发现数据中的自然分组 (20 分钟)

### 1.1 什么是聚类？—— 无监督的探索

*   **核心思想：** 聚类是一种无监督学习技术，其目标是在没有预先定义标签的情况下，发现数据中隐藏的、自然的群组结构。我们希望组内（簇内）的数据点彼此相似，而组间（簇间）的数据点差异较大。
*   **可视化示例：** (展示一张散点图，其中数据点明显分成几个群组，但没有颜色或标签标示)
    *   *提问：* 从这张图中，你能看出数据大概可以分成几组吗？你是如何判断的？
*   **生活中的聚类：**
    *   **超市购物篮分析：** 发现哪些商品经常被一起购买（例如，啤酒和尿布？），用于优化货架摆放或捆绑销售。
    *   **社交网络分析：** 识别具有相似兴趣或联系紧密的用户群体，用于社区发现或信息推荐。
    *   **图像分割：** 将图像中具有相似颜色或纹理的像素分组，用于物体识别或背景分离。
    *   **基因表达数据分析：** 发现具有相似表达模式的基因，可能意味着它们在功能上相关。
*   **互动思考：** 请再列举 1-2 个你认为可以用聚类解决的实际问题。

### 1.2 聚类 vs. 分类：目标与方法的差异

*   **关键区别：**
    *   **分类 (Supervised Learning):** **有**预定义的类别标签，目标是学习一个模型，将新数据点分配到这些已知类别中。（例如，判断邮件是垃圾邮件还是非垃圾邮件）
    *   **聚类 (Unsupervised Learning):** **没有**预定义的类别标签，目标是发现数据本身的内在结构，自动将数据分成簇。（例如，将客户分成不同的消费习惯群体）
*   **表格对比：**

| 特征         | 分类 (Classification)             | 聚类 (Clustering)                |
| :----------- | :---------------------------------- | :------------------------------- |
| **学习类型** | 有监督学习 (Supervised)           | 无监督学习 (Unsupervised)        |
| **输入数据** | 带标签的数据 (X, y)               | 不带标签的数据 (X)             |
| **目标**     | 学习从 X到 y 的映射函数             | 发现数据 X 中的分组结构          |
| **输出**     | 新数据点的类别预测                | 数据点的簇分配                  |
| **评估**     | 准确率、精确率、召回率、F1 分数等 | 轮廓系数、DB 指数、纯度 (需标签) 等 |

*   **讨论：** 想象一个分析银行客户流失的场景。我们应该使用分类还是聚类？为什么？（引导学生思考：如果目标是预测哪些客户 *将要* 流失，需要历史流失标签，用分类；如果目标是了解 *现有* 客户可以分成哪些群体，以便针对性营销，可以用聚类）

## 2. K-means 聚类：简单高效的划分方法 (40 分钟)

### 2.1 算法核心思想与步骤

*   **目标：** 将数据划分为 \(K\) 个簇，使得每个数据点都属于离其最近的簇的质心（均值中心），同时最小化簇内平方和 (Within-Cluster Sum of Squares, WCSS)。
*   **数学表达 (可选)：** 最小化目标函数 \(J = \sum_{j=1}^{K} \sum_{i \in C_j} ||x_i - \mu_j||^2\)，其中 \(C_j\) 是第 \(j\) 个簇，\(\mu_j\) 是簇 \(C_j\) 的质心，\(x_i\) 是簇 \(C_j\) 中的数据点。
*   **详细步骤：**
    1.  **初始化 (Initialization):** **随机**选择 \(K\) 个数据点作为初始质心 (\(\mu_1, \mu_2, ..., \mu_K\))。（*讨论：还有其他初始化方法吗？例如 K-means++*）
    2.  **分配 (Assignment):** 对于数据集中的**每一个**数据点 \(x_i\)，计算它到**所有** \(K\) 个质心 \(\mu_j\) 的距离 (通常使用欧氏距离)，并将其分配给距离**最近**的质心所代表的簇 \(C_j\)。
        *   **欧氏距离公式:** \(d(x, \mu) = \sqrt{\sum_{d=1}^{D}(x_d - \mu_d)^2}\) （\(D\) 是数据维度）
        *   *思考：为什么常用欧氏距离？它有什么几何意义？（直线距离）还有其他距离度量吗？（曼哈顿距离、余弦相似度等）*
    3.  **更新 (Update):** 对于**每一个**簇 \(C_j\)，重新计算其质心 \(\mu_j\)，即该簇中所有数据点的**均值**。
        *   \(\mu_j = \frac{1}{|C_j|} \sum_{x_i \in C_j} x_i\) （\(|C_j|\) 是簇 \(C_j\) 中的数据点数量）
    4.  **迭代 (Iteration):** 重复步骤 2 (分配) 和步骤 3 (更新)，直到满足停止条件。
        *   **停止条件：**
            *   质心位置不再发生显著变化（例如，移动距离小于某个阈值）。
            *   数据点的簇分配不再改变。
            *   达到预设的最大迭代次数。
*   **可视化演示：** (播放 K-means 迭代过程的动画，展示数据点归属和质心移动的变化)

### 2.2 互动环节：手动 K-means 模拟 (K=2)

*   **准备：** 在白板或幻灯片上绘制以下 6 个二维数据点： A(1,1), B(1,2), C(2,1), D(5,4), E(5,5), F(6,5)。
*   **任务：** 设定 \(K=2\)。
    1.  **分组 1：** 选择 A(1,1) 和 D(5,4) 作为初始质心。
    2.  **分组 2：** 选择 B(1,2) 和 F(6,5) 作为初始质心。
*   **步骤：**
    *   **迭代 1 (分配)：** 计算每个点到两个初始质心的欧氏距离，并将点分到最近的簇。
    *   **迭代 1 (更新)：** 分别计算两个新簇的质心（均值）。
    *   **迭代 2 (分配)：** 使用新的质心，重新计算距离并分配。
    *   **迭代 2 (更新)：** 再次更新质心。
*   **讨论：**
    *   两个分组得到的最终聚类结果相同吗？
    *   这说明了 K-means 的什么特点？（对初始质心敏感）
    *   如何缓解这个问题？（多次随机初始化取最优结果，或使用 K-means++ 初始化）

### 2.3 关键参数 K 的选择：肘部法则 (Elbow Method)

*   **问题：** K-means 需要预先指定 K 值，但我们通常不知道最佳 K 是多少。
*   **肘部法则思想：**
    1.  尝试不同的 K 值（例如，从 1 到 10）。
    2.  对于每个 K 值，运行 K-means 算法，并计算簇内平方和 (WCSS)。
    3.  绘制 K 值与 WCSS 的关系图。
    4.  观察图像，寻找曲线下降速率趋于平缓的“肘部”对应的 K 值。这个点通常被认为是 WCSS 下降带来的收益（解释了更多方差）与增加簇数量带来的复杂性之间的较好平衡点。
*   **可视化：** (展示一个典型的肘部法则曲线图)
*   **局限性：** “肘部”有时不明显，该方法仅提供参考。

### 2.4 K-means 的优缺点总结

*   **优点：**
    *   **简单直观：** 算法逻辑清晰，易于理解和实现。
    *   **高效：** 对于大规模数据集，计算速度相对较快，时间复杂度接近线性 \(O(N \times K \times D \times I)\)，其中 N 是样本数，K 是簇数，D 是维度，I 是迭代次数。
*   **缺点：**
    *   **对初始质心敏感：** 不同的初始点可能导致不同的聚类结果，甚至可能陷入局部最优。
    *   **需预先指定 K 值：** K 值的选择对结果影响很大，且没有绝对最优的方法确定 K。
    *   **对噪声和异常值敏感：** 异常值会对均值计算产生较大影响，可能导致质心偏移。
    *   **假设簇为凸状/球状：** 对于非凸形状（如环状、月牙状）或大小/密度差异很大的簇，效果不佳。 (可以画图示意)
    *   **仅适用于数值型数据：** 标准 K-means 基于均值和欧氏距离，难以直接处理类别型数据 (需要转换)。

## 3. 层次聚类：构建簇的层级结构 (40 分钟)

### 3.1 核心思想：合并或分裂

*   **两种主要策略：**
    *   **凝聚型 (Agglomerative) - 自底向上 (Bottom-up):**
        1.  开始时，每个数据点自成一簇。
        2.  在每一步，合并**最相似**（距离最近）的两个簇。
        3.  重复此过程，直到所有点合并成一个大簇。
        *   *本节课重点讲解凝聚型。*
    *   **分裂型 (Divisive) - 自顶向下 (Top-down):**
        1.  开始时，所有数据点属于同一个簇。
        2.  在每一步，将一个簇分裂成两个**最不相似**（距离最远）的子簇。
        3.  重复此过程，直到每个点自成一簇或达到某个停止条件（如指定的簇数量）。
        *   *分裂型计算复杂度通常更高。*

### 3.2 凝聚型层次聚类的步骤与树状图 (Dendrogram)

*   **详细步骤 (凝聚型):**
    1.  **初始化：** 将 \(N\) 个数据点各自视为一个簇，共 \(N\) 个簇。计算所有点对之间的距离，形成距离矩阵。
    2.  **查找最近簇：** 在距离矩阵中找到距离最小的两个簇 \(C_i\) 和 \(C_j\)。
    3.  **合并：** 将簇 \(C_i\) 和 \(C_j\) 合并成一个新的簇 \(C_{new}\)。
    4.  **更新距离矩阵：** 从矩阵中移除 \(C_i\) 和 \(C_j\) 的行和列，添加新簇 \(C_{new}\) 的行和列。计算 \(C_{new}\) 与其他现有簇 \(C_k\) 之间的距离（**关键在于如何定义簇间距离，即 Linkage 方法**）。
    5.  **迭代：** 重复步骤 2-4，直到只剩下一个簇。
*   **树状图 (Dendrogram)：**
    *   **作用：** 可视化层次聚类的整个过程。
    *   **解读：**
        *   **叶节点：** 代表原始数据点。
        *   **纵轴：** 通常表示簇合并时的距离或不相似度。
        *   **横轴：** 代表数据点或簇。
        *   **合并点：** 水平线连接的两个或多个分支表示这些簇在该纵轴高度（距离）被合并。
    *   **如何确定簇数量：** 在树状图上选择一个“切割高度”（水平线），与该水平线相交的竖线数量即为最终得到的簇数量。 (展示一个树状图示例，并演示如何在不同高度切割得到不同数量的簇)

### 3.3 Linkage 方法：定义簇间距离

*   **核心问题：** 如何衡量两个簇（而不是两个点）之间的距离？不同的定义方式会导致不同的聚类结果。
*   **常用 Linkage 方法：**
    *   **单 Linkage (Single Linkage / Minimum Linkage):**
        *   **定义：** 两个簇之间的距离 = 两个簇中**最近**的两个点之间的距离。 \(D(C_i, C_j) = \min_{x \in C_i, y \in C_j} d(x, y)\)
        *   **特点：** 倾向于产生链状的、细长的簇，对噪声敏感。容易受到 "链式效应" 影响。
    *   **全 Linkage (Complete Linkage / Maximum Linkage):**
        *   **定义：** 两个簇之间的距离 = 两个簇中**最远**的两个点之间的距离。 \(D(C_i, C_j) = \max_{x \in C_i, y \in C_j} d(x, y)\)
        *   **特点：** 倾向于产生紧凑的、球状的簇，对异常值没有单 Linkage 那么敏感。
    *   **平均 Linkage (Average Linkage):**
        *   **定义：** 两个簇之间的距离 = 两个簇中所有点对距离的**平均值**。 \(D(C_i, C_j) = \frac{1}{|C_i||C_j|} \sum_{x \in C_i} \sum_{y \in C_j} d(x, y)\)
        *   **特点：** 效果介于 Single 和 Complete Linkage 之间，较为常用。
    *   **Ward's Linkage:**
        *   **定义：** 合并两个簇，使得合并后所有簇的**总簇内平方和增量最小**。旨在最小化方差。
        *   **特点：** 倾向于产生大小相似、方差较小的球状簇，对噪声敏感。常与欧氏距离配合使用。
*   **互动讨论/案例：** (展示同一数据集使用不同 Linkage 方法得到的树状图和聚类结果)
    *   观察不同 Linkage 方法产生的簇形状和结构有何不同？
    *   在什么情况下你可能会选择 Single Linkage？（例如，寻找可能连接的模式）什么时候选择 Complete 或 Ward？（例如，寻找紧凑的群组）

### 3.4 层次聚类的优缺点总结

*   **优点：**
    *   **无需预先指定簇数量：** 树状图提供了对不同簇数量划分的可视化，可以根据需求选择切割点。
    *   **提供层次结构：** 树状图本身揭示了数据点之间的层次关系，有助于理解数据的嵌套结构。
    *   **对簇形状假设较少：** 相较于 K-means，对簇的形状（特别是 Single/Average Linkage）没那么严格的假设。
    *   **结果唯一确定 (给定 Linkage 和距离):** 不像 K-means 那样受初始化影响。
*   **缺点：**
    *   **计算复杂度高：** 凝聚型算法的时间复杂度通常为 \(O(N^3)\) 或 \(O(N^2 \log N)\)（使用优化的数据结构），空间复杂度为 \(O(N^2)\)（存储距离矩阵），难以处理非常大规模的数据集。
    *   **合并/分裂不可撤销：** 一旦一个合并（或分裂）发生，后续步骤无法撤销，早期错误的合并可能影响最终结果。
    *   **对距离度量和 Linkage 方法敏感：** 选择不同的度量和 Linkage 会显著影响结果。
    *   **难以解释大型树状图：** 当数据点非常多时，树状图可能变得非常复杂，难以解读。

## 4. 聚类评估与算法选择 (20 分钟)

### 4.1 如何评价聚类结果的好坏？

*   聚类是无监督的，没有绝对的“正确”答案，评估通常是相对的。
*   **内部评估指标 (Internal Indices):** 仅利用聚类结果本身和原始数据进行评估。
    *   **目标：** 衡量簇的紧密度 (Cohesion) 和分离度 (Separation)。
    *   **常用指标：**
        *   **轮廓系数 (Silhouette Coefficient):**
            *   衡量每个样本点与其自身簇的紧密度以及与其他最近簇的分离度。
            *   取值范围 [-1, 1]。值越接近 1，表示聚类效果越好；接近 0 表示簇重叠；接近 -1 表示样本点可能被分到了错误的簇。
            *   计算每个点的轮廓系数，然后求平均值。
        *   **戴维斯-布尔丁指数 (Davies-Bouldin Index, DBI):**
            *   计算任意两个簇的簇内散度之和与簇心距离的比值，然后取所有簇对中该比值的最大值的平均。
            *   值越小，表示簇内距离小，簇间距离大，聚类效果越好。
    *   **优点：** 不需要外部信息（标签）。
    *   **缺点：** 高分不一定代表发现了有意义的模式，可能偏好某些类型的簇结构（如球状）。
*   **外部评估指标 (External Indices):** 需要已知“真实”类别标签 (Ground Truth)。
    *   **目标：** 将聚类结果与真实类别进行比较。
    *   **常用指标：**
        *   **调整兰德指数 (Adjusted Rand Index, ARI):**
            *   衡量聚类结果与真实标签之间的一致性程度，并对随机分配进行了调整。
            *   取值范围 [-1, 1]。值越接近 1，表示聚类结果与真实标签越吻合；接近 0 表示随机分配水平。
        *   **纯度 (Purity):**
            *   计算每个簇中占比最大的真实类别的样本数之和，再除以总样本数。
            *   取值范围 [0, 1]。值越接近 1 越好。
    *   **优点：** 直接衡量与真实结构的符合程度。
    *   **缺点：** 现实中往往没有 Ground Truth。
*   **视觉评估：**
    *   **散点图 (二维或降维后):** 将不同簇的点用不同颜色标出，直观观察簇的分离情况。
    *   **树状图 (层次聚类):** 观察簇的合并过程和结构。

### 4.2 如何选择合适的聚类算法？

*   没有万能的算法，需要根据具体情况权衡：

| 考虑因素             | K-means 倾向                                   | 层次聚类倾向                                        |
| :------------------- | :--------------------------------------------- | :-------------------------------------------------- |
| **数据集大小**       | 大规模 (相对高效)                              | 中小规模 (计算复杂度高)                             |
| **簇数量 K**         | 需要预先指定 (或通过肘部法则等估计)             | 无需预先指定 (可通过树状图选择)                     |
| **簇的形状**         | 倾向于球状、凸状                               | 对形状假设较少 (取决于 Linkage)                    |
| **对噪声/异常值**    | 敏感                                           | 敏感 (但 Linkage 方法有差异)                        |
| **需要层次结构**     | 否                                             | 是 (树状图)                                         |
| **结果可复现性**     | 受初始化影响，可能不同                         | 结果唯一确定 (给定 Linkage 和距离)                 |
| **实现复杂度**       | 相对简单                                       | 相对复杂 (特别是距离更新)                           |
| **数据类型**         | 主要数值型 (K-prototypes 可处理混合类型)         | 可处理不同距离度量 (适用于不同类型，但需定义距离) |

*   **决策流程思考：**
    1.  **数据探索 (EDA):** 了解数据规模、维度、类型、分布特点。
    2.  **分析目标：** 我想发现什么？需要多少个簇？是否需要层次关系？
    3.  **算法初选：** 根据上述因素，选择 1-2 个候选算法。
    4.  **参数选择/调优：** 如 K-means 的 K 值，层次聚类的 Linkage 方法。
    5.  **执行聚类：** 应用算法。
    6.  **结果评估：** 使用内部/外部指标和可视化进行评估。
    7.  **迭代优化：** 根据评估结果，调整参数或尝试其他算法。

## 5. 实践环节：Python 实现初探 (10 分钟)

*   **介绍常用库：** `scikit-learn` 是 Python 中进行机器学习（包括聚类）的强大工具。
*   **核心类：**
    *   `sklearn.cluster.KMeans`: 实现 K-means 算法。
        ```python
        from sklearn.cluster import KMeans
        # 假设 X 是你的数据
        kmeans = KMeans(n_clusters=3, random_state=42, n_init=10) # n_init='auto' in newer versions
        kmeans.fit(X)
        labels = kmeans.labels_ # 获取每个点的簇标签
        centroids = kmeans.cluster_centers_ # 获取质心
        wcss = kmeans.inertia_ # 获取 WCSS
        ```
    *   `sklearn.cluster.AgglomerativeClustering`: 实现凝聚型层次聚类。
        ```python
        from sklearn.cluster import AgglomerativeClustering
        # 假设 X 是你的数据
        # linkage='ward', 'complete', 'average', 'single'
        hierarchical = AgglomerativeClustering(n_clusters=3, linkage='ward')
        labels = hierarchical.fit_predict(X) # 直接拟合和预测标签
        ```
    *   `scipy.cluster.hierarchy`: 提供更灵活的层次聚类功能，包括生成和绘制树状图。
        ```python
        from scipy.cluster.hierarchy import linkage, dendrogram
        import matplotlib.pyplot as plt
        # 假设 X 是你的数据
        linked = linkage(X, method='ward') # 计算 linkage 矩阵
        plt.figure(figsize=(10, 7))
        dendrogram(linked,
                   orientation='top',
                   labels=None, # 可以设置标签
                   distance_sort='descending',
                   show_leaf_counts=True)
        plt.show()
        ```
*   **提醒：** 实际应用中通常需要数据预处理（如特征缩放/标准化），这对于基于距离的算法（如 K-means 和许多层次聚类配置）尤为重要。

## 6. 总结、反思与展望 (5 分钟)

*   **核心回顾：**
    *   聚类的本质是发现数据分组。
    *   K-means：简单高效，但有局限性（K 值、初始化、形状假设）。
    *   层次聚类：提供结构，无需预设 K，但计算成本高。
    *   评估是关键：内部、外部、可视化结合。
    *   算法选择需权衡。
*   **批判性思考：**
    *   聚类结果总是“有意义”的吗？（可能发现的是算法偏好而非真实结构）
    *   不同算法在同一数据上给出不同结果，哪个是对的？（没有绝对对错，看哪个更符合分析目标和数据特性）
*   **课后探索：**
    *   尝试使用 `scikit-learn` 对一个简单数据集（如 Iris 数据集，去掉标签）进行 K-means 和层次聚类，比较结果。
    *   研究 K-means++ 初始化方法。
    *   了解 DBSCAN 等基于密度的聚类算法，它们能处理任意形状的簇。
*   **学习目标自评：** 对照开头的学习目标，你觉得自己掌握了多少？
