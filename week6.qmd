# 第六周：集成学习与回归模型优化

## 第一次课：回归算法(二) - 集成学习(XGBoost)

::: {.callout-tip}
## 本周学习目标
* 理解集成学习的基本原理和分类
* 掌握梯度提升决策树(GBDT)的工作原理
* 深入学习XGBoost算法的特点和优势
* 掌握XGBoost模型的参数调优方法
* 能够使用xgboost库实现和评估回归模型
* 学习回归模型评估指标的选择和应用场景
* 将XGBoost应用于实际的房价预测问题
:::

### 内容概要

1. **集成学习概述**

::: {.callout-note}
## 什么是集成学习？
* **定义**: 集成学习通过组合多个基学习器来提高学习效果的一种机器学习方法
* **主要策略**:
  * **Bagging**: 并行训练多个基学习器(如随机森林)
  * **Boosting**: 串行训练基学习器，每个学习器关注前一个学习器的错误(如AdaBoost, GBDT, XGBoost)
  * **Stacking**: 将多个不同类型的模型组合起来
* **优势**: 
  * 降低方差，减少过拟合
  * 提高模型稳定性和预测准确性
  * 处理复杂非线性关系的能力强
:::

2. **梯度提升决策树(GBDT)基础**

::: {.callout-important}
## GBDT原理
* **基本思想**: 通过不断拟合前一个模型的残差来提高整体模型性能
* **算法步骤**:
  1. 初始化模型为一个常数值(如平均值)
  2. 计算当前模型的负梯度(残差)
  3. 拟合一个新的决策树来预测这些残差
  4. 将新树添加到模型中(带学习率)
  5. 更新残差并重复步骤2-4直到满足停止条件
* **特点**:
  * 具有天然的特征重要性评估能力
  * 可以处理缺失值和混合类型特征
  * 对数据尺度不敏感
:::

3. **XGBoost算法详解**

::: {.callout-note}
## XGBoost的优势
* **XGBoost = e**X**treme **G**radient **Boost**ing
* **相比传统GBDT的改进**:
  * 使用了更为正则化的目标函数，减少过拟合
  * 支持列抽样(类似随机森林)，进一步降低过拟合风险
  * 优化算法，支持并行计算
  * 自动处理缺失值
  * 内置交叉验证和提前停止功能
* **损失函数**:
  * 目标函数 = 训练损失 + 正则化项
  * $L = \sum_{i=1}^n l(y_i, \hat{y}_i) + \sum_{k=1}^K \Omega(f_k)$
:::

4. **XGBoost参数调优**

::: {.callout-important}
## 重要参数解析
* **训练参数**:
  * `learning_rate`: 学习率，控制每棵树的贡献权重
  * `n_estimators`: 树的数量
  * `max_depth`: 树的最大深度
  * `min_child_weight`: 叶子节点最小样本权重和
  
* **正则化参数**:
  * `gamma`: 节点分裂所需的最小损失减少值
  * `reg_alpha`: L1正则化参数
  * `reg_lambda`: L2正则化参数
  
* **随机化参数**:
  * `subsample`: 行抽样比例
  * `colsample_bytree`: 列抽样比例
  
* **调参策略**:
  * 先调整树的复杂度参数(max_depth, min_child_weight)
  * 再调整随机化参数(subsample, colsample_bytree)
  * 最后调整正则化参数(gamma, reg_alpha, reg_lambda)
  * 降低learning_rate并增加n_estimators
:::

5. **回归模型评估指标选择**

::: {.callout-note}
## 评估指标与应用场景
* **均方误差(MSE)**: 对离群值敏感，适用于预测值不能有大偏差的场景
* **均方根误差(RMSE)**: 与MSE类似，但单位与预测值相同，更直观
* **平均绝对误差(MAE)**: 对离群值不敏感，适用于存在异常值的场景
* **平均绝对百分比误差(MAPE)**: 衡量相对误差，适用于不同量级预测的比较
* **决定系数(R²)**: 衡量模型解释方差的比例，适用于模型解释能力的评估
* **选择原则**:
  * 根据业务需求选择最合适的指标
  * 通常使用多个指标结合评估
  * 考虑预测偏差的严重程度与业务影响
:::

### 实践: XGBoost实现与调优

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import xgboost as xgb
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression

# 加载数据示例 (可替换为实际的房价数据集)
# 这里使用生成的示例数据
np.random.seed(42)
n_samples = 1000
X = np.random.rand(n_samples, 5) 
# 创建非线性关系
y = 5 + 3*X[:, 0]**2 + 2*X[:, 1] + np.sin(3*X[:, 2]) + np.exp(X[:, 3]) - 2*X[:, 4] + np.random.normal(0, 1, n_samples)

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 标准化特征
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 线性回归作为基线模型
lr = LinearRegression()
lr.fit(X_train_scaled, y_train)
lr_pred = lr.predict(X_test_scaled)
lr_mse = mean_squared_error(y_test, lr_pred)
lr_rmse = np.sqrt(lr_mse)
lr_mae = mean_absolute_error(y_test, lr_pred)
lr_r2 = r2_score(y_test, lr_pred)

print("线性回归基线模型:")
print(f"MSE: {lr_mse:.4f}")
print(f"RMSE: {lr_rmse:.4f}")
print(f"MAE: {lr_mae:.4f}")
print(f"R²: {lr_r2:.4f}")

# 基本XGBoost模型
params = {
    'objective': 'reg:squarederror',
    'learning_rate': 0.1,
    'max_depth': 5,
    'min_child_weight': 1,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'gamma': 0,
    'reg_alpha': 0,
    'reg_lambda': 1,
    'random_state': 42
}

xgb_model = xgb.XGBRegressor(**params)
xgb_model.fit(X_train_scaled, y_train)
xgb_pred = xgb_model.predict(X_test_scaled)
xgb_mse = mean_squared_error(y_test, xgb_pred)
xgb_rmse = np.sqrt(xgb_mse)
xgb_mae = mean_absolute_error(y_test, xgb_pred)
xgb_r2 = r2_score(y_test, xgb_pred)

print("\n基本XGBoost模型:")
print(f"MSE: {xgb_mse:.4f}")
print(f"RMSE: {xgb_rmse:.4f}")
print(f"MAE: {xgb_mae:.4f}")
print(f"R²: {xgb_r2:.4f}")

# 简单的参数调优示例
param_grid = {
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2],
    'n_estimators': [50, 100, 200],
    'min_child_weight': [1, 3, 5]
}

grid_search = GridSearchCV(
    estimator=xgb.XGBRegressor(objective='reg:squarederror', 
                               subsample=0.8, 
                               colsample_bytree=0.8, 
                               random_state=42),
    param_grid=param_grid,
    cv=5,
    scoring='neg_mean_squared_error',
    verbose=1,
    n_jobs=-1
)

# 注意：实际运行时这一步可能需要较长时间
# 可以根据实际情况减少参数搜索空间
grid_search.fit(X_train_scaled, y_train)

print("\n最优参数:")
print(grid_search.best_params_)

# 使用最优参数的模型
best_xgb_model = grid_search.best_estimator_
best_xgb_pred = best_xgb_model.predict(X_test_scaled)
best_xgb_mse = mean_squared_error(y_test, best_xgb_pred)
best_xgb_rmse = np.sqrt(best_xgb_mse)
best_xgb_mae = mean_absolute_error(y_test, best_xgb_pred)
best_xgb_r2 = r2_score(y_test, best_xgb_pred)

print("\n调优后的XGBoost模型:")
print(f"MSE: {best_xgb_mse:.4f}")
print(f"RMSE: {best_xgb_rmse:.4f}")
print(f"MAE: {best_xgb_mae:.4f}")
print(f"R²: {best_xgb_r2:.4f}")

# 特征重要性可视化
plt.figure(figsize=(10, 6))
xgb.plot_importance(best_xgb_model, importance_type='weight')
plt.title('XGBoost 特征重要性')
plt.tight_layout()
plt.show()

# 预测vs实际值对比
plt.figure(figsize=(10, 6))
plt.scatter(y_test, best_xgb_pred, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.xlabel('实际值')
plt.ylabel('预测值')
plt.title('XGBoost模型预测vs实际值')
plt.tight_layout()
plt.show()

# 对比不同模型性能
models = ['线性回归', '基本XGBoost', '调优XGBoost']
mse_values = [lr_mse, xgb_mse, best_xgb_mse]
r2_values = [lr_r2, xgb_r2, best_xgb_r2]

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.bar(models, mse_values)
plt.title('均方误差比较')
plt.ylabel('MSE')

plt.subplot(1, 2, 2)
plt.bar(models, r2_values)
plt.title('R²比较')
plt.ylabel('R²')

plt.tight_layout()
plt.show()
```

### 小组项目二：房价预测模型优化 (XGBoost)

::: {.callout-note}
## 项目要求
* 在上周线性/多项式回归模型的基础上，使用XGBoost算法优化房价预测模型
* 进行模型参数调优，提高预测性能
* 比较XGBoost与线性回归、多项式回归的性能差异
* 分析特征重要性，解释模型预测结果
:::

::: {.callout-important}
## 提交内容
1. 代码实现
   * XGBoost模型的实现代码
   * 参数调优策略与实现
   * 性能对比代码

2. 实验报告
   * 数据集描述
   * 预处理步骤
   * 不同模型的性能指标比较
   * 参数调优过程与结果
   * 特征重要性分析
   * 结论与讨论

3. 提交时间
   * 第7周课前提交
:::

### 下次课预告

下节课我们将进行小组项目二的代码实践，指导大家使用XGBoost优化房价预测模型，解决实际问题中遇到的困难，并帮助大家撰写高质量的项目报告。

## 第二次课：小组项目二：房价预测模型优化 (XGBoost)

### 课堂实践安排

::: {.callout-note}
## 课堂活动
* 小组开发环境准备与代码规划
* 在老师指导下，各小组使用XGBoost算法优化房价预测模型
* 实施参数调优，提高模型性能
* 比较不同模型性能，分析优劣
* 撰写项目报告
:::

### 指导重点

1. **参数调优实践**
   * 使用网格搜索、随机搜索或贝叶斯优化进行参数调优
   * 使用交叉验证评估模型性能
   * 解读参数调优结果，理解参数对模型性能的影响

2. **特征工程探索**
   * 尝试不同的特征变换方法
   * 利用XGBoost的特征重要性进行特征选择
   * 处理类别特征的多种方法

3. **模型评估与解释**
   * 使用多种评估指标综合评估模型性能
   * 解释XGBoost模型的预测结果
   * 通过特征重要性分析理解房价影响因素

4. **报告撰写指导**
   * 实验报告结构与内容组织
   * 数据可视化的有效方法
   * 结果分析与讨论的深度与广度

### 开发流程建议

1. **数据准备与特征工程**
   ```python
   # 数据加载与预处理
   import pandas as pd
   from sklearn.preprocessing import StandardScaler, OneHotEncoder
   from sklearn.compose import ColumnTransformer
   from sklearn.pipeline import Pipeline
   
   # 加载数据
   df = pd.read_csv('房价数据集.csv')
   
   # 分离特征和目标
   X = df.drop('price', axis=1)
   y = df['price']
   
   # 识别数值和类别特征
   numerical_features = X.select_dtypes(include=['int64', 'float64']).columns
   categorical_features = X.select_dtypes(include=['object']).columns
   
   # 创建预处理管道
   preprocessor = ColumnTransformer(
       transformers=[
           ('num', StandardScaler(), numerical_features),
           ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
       ])
   
   # 划分训练集和测试集
   from sklearn.model_selection import train_test_split
   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
   ```

2. **XGBoost模型训练与调优**
   ```python
   import xgboost as xgb
   from sklearn.model_selection import GridSearchCV
   
   # 创建模型管道
   xgb_pipeline = Pipeline([
       ('preprocessor', preprocessor),
       ('xgb', xgb.XGBRegressor(objective='reg:squarederror', random_state=42))
   ])
   
   # 参数网格
   param_grid = {
       'xgb__max_depth': [3, 5, 7],
       'xgb__learning_rate': [0.01, 0.1, 0.2],
       'xgb__n_estimators': [100, 200],
       'xgb__min_child_weight': [1, 3, 5],
       'xgb__subsample': [0.7, 0.8, 0.9],
       'xgb__colsample_bytree': [0.7, 0.8, 0.9]
   }
   
   # 网格搜索
   grid_search = GridSearchCV(
       xgb_pipeline,
       param_grid,
       cv=5,
       scoring='neg_mean_squared_error',
       verbose=1,
       n_jobs=-1
   )
   
   grid_search.fit(X_train, y_train)
   
   # 最优模型
   best_model = grid_search.best_estimator_
   ```

3. **模型评估与比较**
   ```python
   from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
   import numpy as np
   import matplotlib.pyplot as plt
   
   # 获取预测结果
   y_pred = best_model.predict(X_test)
   
   # 计算各种评估指标
   mse = mean_squared_error(y_test, y_pred)
   rmse = np.sqrt(mse)
   mae = mean_absolute_error(y_test, y_pred)
   r2 = r2_score(y_test, y_pred)
   
   print(f"MSE: {mse:.2f}")
   print(f"RMSE: {rmse:.2f}")
   print(f"MAE: {mae:.2f}")
   print(f"R²: {r2:.2f}")
   
   # 与线性回归和多项式回归比较
   # (假设你已有这些模型的结果)
   models = ['线性回归', '多项式回归', 'XGBoost']
   rmse_scores = [linear_rmse, poly_rmse, rmse]
   r2_scores = [linear_r2, poly_r2, r2]
   
   plt.figure(figsize=(12, 5))
   plt.subplot(1, 2, 1)
   plt.bar(models, rmse_scores)
   plt.title('RMSE比较')
   plt.ylabel('RMSE (越低越好)')
   
   plt.subplot(1, 2, 2)
   plt.bar(models, r2_scores)
   plt.title('R²比较')
   plt.ylabel('R² (越高越好)')
   
   plt.tight_layout()
   plt.show()
   ```

4. **特征重要性分析**
   ```python
   # 获取特征重要性
   xgb_model = best_model.named_steps['xgb']
   preprocessor = best_model.named_steps['preprocessor']
   
   # 获取处理后的特征名称
   feature_names = (
       numerical_features.tolist() +
       preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features).tolist()
   )
   
   # 可视化特征重要性
   importance = xgb_model.feature_importances_
   indices = np.argsort(importance)[::-1]
   
   plt.figure(figsize=(12, 8))
   plt.title('XGBoost特征重要性')
   plt.bar(range(len(importance)), importance[indices])
   plt.xticks(range(len(importance)), [feature_names[i] for i in indices], rotation=90)
   plt.tight_layout()
   plt.show()
   ```

### 提醒事项

::: {.callout-important}
## 重要提示
* 注意模型过拟合问题，使用交叉验证评估模型性能
* 参数调优时注意计算资源消耗，合理设置参数搜索空间
* 结合业务背景解释特征重要性和模型预测结果
* 同时关注模型的预测准确性和可解释性
* 报告中需对比不同模型，分析XGBoost的优势与局限性
:::

### 下周预告

下周我们将开始学习聚类算法，首先介绍K-Means聚类算法，并将其应用于用户分群问题，开启我们在无监督学习领域的探索。同时，请各小组在下周课前提交房价预测模型优化(XGBoost)及报告。 