---
title: "第八周：深入聚类 - DBSCAN 与业务解读"
subtitle: "学习基于密度的聚类，掌握聚类评估与可视化，深化用户分群项目"
---

上周我们学习了由 MacQueen (1967) 提出的 K-Means 聚类算法，并了解了如何通过 Thorndike (1953) 提出的肘部法则和 Rousseeuw (1987) 提出的轮廓系数来选择 K 值。本周，我们将学习由 Ester 等人 (1996) 提出的另一种重要的聚类算法——**DBSCAN (Density-Based Spatial Clustering of Applications with Noise, 基于密度的噪点空间聚类法）**，它特别擅长发现**任意形状**的簇并能识别**噪声点**。我们还将深入探讨如何评估聚类结果，如何进行可视化，以及最重要的——如何从聚类结果中提炼出有价值的**业务洞察**，并完成我们的用户分群项目（项目三）。

::: {.callout-info title="项目三进展"}
请确保你已经完成了项目三（用户分群）的第一阶段任务，包括数据预处理、使用 K-Means 进行聚类以及 K 值的选择过程。
:::

## 1. 聚类评估指标深入

除了上周提到的轮廓系数，还有其他一些指标可以帮助我们评估聚类效果，尤其是在没有真实标签（Ground Truth）的情况下。

*   **轮廓系数 (Silhouette Score) 回顾:**
    *   衡量单个样本与其自身簇的紧密度以及与其他簇的分离度。
    *   平均轮廓系数越接近 1 越好。
*   **戴维斯-布尔丁指数 (Davies-Bouldin Index, DBI):**
    *   衡量簇内的紧密度与簇间的分离度之比。它计算每个簇与其最相似簇的相似度，目标是最小化这个值。
    *   **DBI 越小越好**，表示簇内距离小，簇间距离大。
    *   `sklearn.metrics.davies_bouldin_score`

```{python}
from sklearn.metrics import davies_bouldin_score
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_blobs
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt 
import seaborn as sns


# --- 使用上周的示例数据 ---
X_blobs, y_blobs_true = make_blobs(n_samples=300, centers=4, cluster_std=0.8, random_state=42)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_blobs)

# --- 假设已确定 K=4 是较好的选择 ---
kmeans_final = KMeans(n_clusters=4, init='k-means++', n_init='auto', random_state=42)
kmeans_final.fit(X_scaled)
final_labels = kmeans_final.labels_

# --- 计算 DBI ---
dbi_score = davies_bouldin_score(X_scaled, final_labels)
print(f"Davies-Bouldin Index (K=4): {dbi_score:.4f}") # 值越小越好
```

**注意:** 这些内部评估指标（不依赖真实标签）可以帮助选择 K 值或比较不同算法/参数的效果，但最终聚类结果的好坏还需要结合业务场景来判断。

## 2. 聚类结果可视化

将高维数据的聚类结果可视化出来，有助于我们直观地理解簇的分布和特征。对于二维数据，可以直接绘制散点图。但对于更高维的数据，我们需要先进行**降维 (Dimensionality Reduction)**。

*   **主成分分析 (PCA - Principal Component Analysis):** 由 Karl Pearson (1901) 提出的一种经典线性降维方法，通过找到数据方差最大的方向（主成分），将高维数据投影到这些主成分方向上，从而实现降维。我们将在后续章节详细探讨其数学原理和应用。
*   **t-分布随机邻域嵌入 (t-SNE - t-distributed Stochastic Neighbor Embedding):** 由 Laurens van der Maaten 和 Geoffrey Hinton (2008) 提出的一种非线性降维方法，特别擅长将高维数据映射到二维或三维空间进行可视化，能较好地保持数据的局部结构（相似的点在低维空间中也靠近）。计算成本较高。

```{python}
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE as t_SNE
import matplotlib.pyplot as plt
import seaborn as sns # 用于更美观的绘图

# --- 假设 X_scaled 是你的高维特征数据 (已缩放) ---
# --- final_labels 是你的聚类结果标签 ---

# --- 使用 PCA 降维到 2 维 ---
pca = PCA(n_components=2, random_state=42)
X_pca = pca.fit_transform(X_scaled)

plt.figure(figsize=(10, 7))
sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=final_labels, palette='viridis', s=50, alpha=0.7)
plt.title('K-Means Clustering Visualization (PCA)')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend(title='Cluster')
# plt.show()

# --- 使用 t-SNE 降维到 2 维 ---
# t-SNE 参数:
# n_components: 降维后的维度 (通常是 2 或 3)
# perplexity: 与近邻数量相关，影响局部和全局的平衡，常用 5-50
# n_iter: 迭代次数
# random_state: 保证结果可复现
tsne = t_SNE(n_components=2, perplexity=30, n_iter=300, random_state=42)
X_tsne = tsne.fit_transform(X_scaled) # t-SNE 通常只用 fit_transform

plt.figure(figsize=(10, 7))
sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], hue=final_labels, palette='viridis', s=50, alpha=0.7)
plt.title('K-Means Clustering Visualization (t-SNE)')
plt.xlabel('t-SNE Dimension 1')
plt.ylabel('t-SNE Dimension 2')
plt.legend(title='Cluster')
# plt.show()
```

::: {.callout-tip title="AI 辅助可视化"}
*   "如何使用 Matplotlib 或 Seaborn 绘制带有不同颜色标记的聚类散点图？"
*   "解释 PCA 和 t-SNE 的主要区别以及它们各自的适用场景。"
*   "帮我生成代码，使用 PCA 将 'feature_df' 降到 3 维，并绘制 3D 散点图。"
*   "t-SNE 中的 `perplexity` 参数是什么意思？如何选择合适的值？"
:::

## 3. DBSCAN 聚类算法

K-Means 的主要缺点之一是它难以处理非球状的簇，并且对 K 值的选择和异常值敏感。**DBSCAN** 是一种基于**密度**的聚类算法，可以克服这些缺点。

### 3.1 原理简介 (核心概念)

DBSCAN 不需要预先指定簇的数量 K，而是根据样本点的**密度**来划分簇。它定义了以下核心概念：

*   **ε (Epsilon / eps):** 一个距离阈值，定义了样本点的“邻域”半径。
*   **MinPts (min_samples):** 一个整数阈值，表示一个点要成为“核心点”，其 ε-邻域内至少需要包含多少个样本点（包括自身）。
*   **核心点 (Core Point):** 如果一个样本点的 ε-邻域内至少有 MinPts 个样本点，则该点为核心点。
*   **边界点 (Border Point):** 一个样本点不是核心点，但它落在某个核心点的 ε-邻域内。
*   **噪声点 (Noise Point / Outlier):** 既不是核心点也不是边界点的样本点。

**DBSCAN 的聚类过程:**

1.  随机选择一个未访问过的样本点 P。
2.  检查 P 的 ε-邻域：
    *   如果邻域内样本数**大于等于 MinPts**，则 P 是一个**核心点**。以 P 为起点创建一个**新簇**，并将邻域内的所有点（包括核心点和边界点）加入该簇和待处理队列。
    *   如果邻域内样本数**小于 MinPts**，则 P 暂时标记为**噪声点**（它后续可能被发现是某个簇的边界点）。
3.  处理队列中的每一个点：
    *   如果该点是核心点，将其 ε-邻域内所有**未分配簇**且**未标记为噪声**的点加入当前簇和队列。
    *   如果该点是边界点，不做任何操作（因为它不扩展簇）。
4.  当队列为空时，当前簇形成完毕。返回步骤 1，选择下一个未访问过的点。
5.  所有点都被访问后，聚类完成。被标记为噪声的点不属于任何簇。

**核心思想:** 一个簇由密度相连的核心点组成，边界点则附属于这些核心点。密度稀疏区域的点被视为噪声。

![DBSCAN Concepts](https://upload.wikimedia.org/wikipedia/commons/0/05/DBSCAN-density-data.svg){fig-alt="DBSCAN Concepts" width=60%}

### 3.2 使用 Scikit-learn 实现

Scikit-learn 提供了 `DBSCAN` 类。

::: {.callout-warning title="特征缩放"}
DBSCAN 同样基于距离计算，因此**必须进行特征缩放**。
:::

```{python}
from sklearn.cluster import DBSCAN
from sklearn.datasets import make_moons # 用于生成非球状测试数据

# --- 准备非球状数据 (示例) ---
X_moons, y_moons = make_moons(n_samples=200, noise=0.05, random_state=42)

# 特征缩放
scaler_moons = StandardScaler()
X_moons_scaled = scaler_moons.fit_transform(X_moons)

# --- 训练 DBSCAN 模型 ---
# eps: 邻域半径，需要根据数据调整
# min_samples: 成为核心点的最小邻域样本数，通常建议 >= D+1 (D是数据维度)，或根据经验调整
dbscan = DBSCAN(eps=0.3, min_samples=5) # 这两个参数需要仔细选择

# 拟合模型并获取标签 (-1 表示噪声点)
dbscan_labels = dbscan.fit_predict(X_moons_scaled) # fit_predict 更方便

print("DBSCAN 聚类标签 (前 20 个):", dbscan_labels[:20])
n_clusters_ = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)
n_noise_ = list(dbscan_labels).count(-1)
print(f"\nDBSCAN 发现的簇数量: {n_clusters_}")
print(f"DBSCAN 发现的噪声点数量: {n_noise_}")

# --- 可视化 DBSCAN 结果 ---
plt.figure(figsize=(8, 6))
sns.scatterplot(x=X_moons_scaled[:, 0], y=X_moons_scaled[:, 1], hue=dbscan_labels,
                palette='viridis', s=50, alpha=0.7, legend='full')
plt.title(f'DBSCAN Clustering (eps=0.3, min_samples=5)\nEstimated clusters: {n_clusters_}')
plt.xlabel("Scaled Feature 1")
plt.ylabel("Scaled Feature 2")
# plt.show()

# --- 对比 K-Means 在该数据上的效果 ---
kmeans_moons = KMeans(n_clusters=2, init='k-means++', n_init='auto', random_state=42)
kmeans_labels = kmeans_moons.fit_predict(X_moons_scaled)

plt.figure(figsize=(8, 6))
sns.scatterplot(x=X_moons_scaled[:, 0], y=X_moons_scaled[:, 1], hue=kmeans_labels,
                palette='viridis', s=50, alpha=0.7, legend='full')
plt.title('K-Means Clustering (K=2) on Moons Data')
plt.xlabel("Scaled Feature 1")
plt.ylabel("Scaled Feature 2")
# plt.show()
```

**观察:** K-Means 无法很好地分离月亮形状的数据，而 DBSCAN 可以。

### 3.3 如何选择 `eps` 和 `min_samples`？

这两个参数的选择对 DBSCAN 的结果至关重要，通常需要一些经验和尝试：

*   **`min_samples`:**
    *   通常建议从 `D+1` 开始尝试，其中 D 是数据的维度。
    *   对于高维数据或有噪声的数据，可能需要设置更大的值。
    *   `min_samples` 越大，需要的密度越高，形成的簇越少，噪声点越多。
*   **`eps`:**
    *   确定 `min_samples` 后，可以通过绘制 **K-距离图 (K-distance graph)** 来辅助选择 `eps`。
    *   计算每个点到其第 `min_samples-1` 个最近邻的距离（称为 K-距离）。
    *   将所有点的 K-距离按升序排序并绘制出来。
    *   寻找图中曲线**斜率急剧变化**的“拐点 (knee/elbow)”。该拐点对应的距离值可以作为 `eps` 的一个候选值。

```{python}
from sklearn.neighbors import NearestNeighbors

# --- K-距离图辅助选择 eps ---
# 假设 min_samples 已选定，例如 min_samples = 5
k = 5 # k = min_samples
# 找到每个点到其第 k-1 个邻居的距离
nbrs = NearestNeighbors(n_neighbors=k).fit(X_moons_scaled)
distances, indices = nbrs.kneighbors(X_moons_scaled)

# 获取每个点的 k-距离 (第 k-1 个邻居的距离，即 distances 数组的最后一列)
k_distances = np.sort(distances[:, k-1], axis=0)

# 绘制 K-距离图
plt.figure(figsize=(8, 5))
plt.plot(k_distances)
plt.title(f'{k}-Distance Graph')
plt.xlabel("Points sorted by distance")
plt.ylabel(f'{k}-th nearest neighbor distance')
plt.grid(True)
# plt.show()
# 在图中寻找“拐点”来估计 eps
```

**解读:** 在 Moons 数据的 K-距离图中，拐点大约在 0.3 附近，这与我们之前示例中使用的 `eps=0.3` 吻合。

### 3.4 DBSCAN 优缺点

*   **优点:**
    *   **不需要指定簇数量 K。**
    *   **能发现任意形状的簇。**
    *   **能识别噪声点/异常值。**
    *   对簇的形状和大小不敏感（相比 K-Means）。
    *   只需要两个参数 (`eps`, `min_samples`)。
*   **缺点:**
    *   **对参数 `eps` 和 `min_samples` 非常敏感:** 参数选择不当会导致结果差异很大。
    *   **对于密度差异很大的簇效果不佳:** 难以用一组全局参数来识别不同密度的簇。
    *   **对于高维数据效果可能下降:** “维度灾难”使得距离度量在高维空间意义减弱，密度定义变得困难。
    *   **计算复杂度相对较高:** 特别是在密集区域。

## 4. 聚类算法选择对比 (K-Means vs. DBSCAN)

| 特性             | K-Means                                  | DBSCAN                                     |
| :--------------- | :--------------------------------------- | :----------------------------------------- |
| **簇形状**       | 倾向于球状、凸形                         | 可以是任意形状                             |
| **簇数量 K**     | 需要预先指定                             | 自动确定                                   |
| **噪声/异常值**  | 对异常值敏感，所有点都会被分配到某个簇   | 能识别噪声点，不将其分配到任何簇           |
| **簇密度**       | 对密度不敏感                             | 对密度敏感，难以处理密度差异大的簇         |
| **参数**         | K                                        | `eps`, `min_samples`                       |
| **参数敏感度**   | 对 K 和初始质心敏感                      | 对 `eps` 和 `min_samples` 非常敏感         |
| **计算复杂度**   | 相对较低                                 | 可能较高（取决于数据和参数）               |
| **特征缩放**     | **需要**                                 | **需要**                                   |
| **适用场景**     | 簇大致呈球状、数量已知、无明显噪声         | 簇形状不规则、数量未知、可能包含噪声       |

## 5. 聚类结果的业务解读 (重点！)

得到聚类结果（每个样本的簇标签）只是第一步，更重要的是理解这些簇代表什么，并将这些理解转化为**商业洞察和行动**。

### 5.1 分析各簇特征

1.  **添加簇标签:** 将聚类得到的标签添加到原始（或预处理前）的 DataFrame 中。
2.  **分组统计:** 使用 `groupby()` 方法按簇标签分组，然后计算每个簇中各个特征的**描述性统计量**（如 `mean()`, `median()`, `std()`, `count()`, `value_counts()`）。
3.  **对比差异:** 比较不同簇之间在关键特征上的**平均值、中位数或分布**差异。例如：
    *   簇 0 的用户平均年龄比簇 1 低吗？
    *   簇 2 的用户平均消费金额远高于其他簇吗？
    *   簇 3 的用户更偏爱购买哪个类别的商品？
4.  **可视化特征分布:** 绘制每个簇中关键特征的分布图（如箱线图、直方图、密度图），更直观地比较差异。

```python
# --- 假设 final_labels 是最终的聚类标签 (来自 K-Means 或 DBSCAN, 注意 DBSCAN 有 -1 标签) ---
# --- 假设 df_original 是包含原始特征的 DataFrame ---
# --- 假设 X_scaled_df 是包含缩放后特征和索引的 DataFrame ---

# 创建一个包含原始特征和聚类标签的 DataFrame 用于分析
# (确保索引对齐，如果需要先 merge)
# 示例：假设 df_original 和 X_scaled 的行索引一致
df_analysis = df_original.copy() # 创建副本以防修改原始数据
df_analysis['Cluster'] = final_labels

# 过滤掉噪声点 (如果使用 DBSCAN 且存在噪声)
df_analysis_no_noise = df_analysis[df_analysis['Cluster'] != -1].copy()

# 计算每个簇的特征均值 (在去噪数据上)
# 选择你关心的数值特征列
numeric_features = ['Age', 'Income', 'SpendingScore'] # 替换为你的实际特征名
cluster_means = df_analysis_no_noise.groupby('Cluster')[numeric_features].mean()
print("各簇特征均值:\n", cluster_means)

# 计算每个簇的大小 (包括噪声点)
cluster_counts = df_analysis['Cluster'].value_counts().sort_index()
print("\n各簇样本数量 (包括噪声点 -1):\n", cluster_counts)

# (可选) 计算其他统计量，如中位数、标准差等
# cluster_medians = df_analysis_no_noise.groupby('Cluster')[numeric_features].median()
# cluster_stds = df_analysis_no_noise.groupby('Cluster')[numeric_features].std()

# 可视化特征分布 (示例：箱线图，在去噪数据上)
# plt.figure(figsize=(15, 5))
# for i, col in enumerate(numeric_features):
#     plt.subplot(1, len(numeric_features), i+1)
#     sns.boxplot(x='Cluster', y=col, data=df_analysis_no_noise)
#     plt.title(f'{col} Distribution by Cluster')
# plt.tight_layout()
# plt.show()

# 对于类别特征，可以使用 value_counts()
# categorical_feature = 'Gender' # 替换为你的类别特征名
# cluster_cat_dist = df_analysis_no_noise.groupby('Cluster')[categorical_feature].value_counts(normalize=True).unstack()
# print(f"\n各簇 {categorical_feature} 分布:\n", cluster_cat_dist)
# cluster_cat_dist.plot(kind='bar', stacked=True, figsize=(10, 6))
# plt.title(f'{categorical_feature} Distribution by Cluster')
# plt.ylabel('Proportion')
# plt.show()

```

### 5.2 给簇贴标签 (Profiling)

根据上一步分析出的各簇显著特征，尝试给每个簇起一个**有意义的标签**，概括这个群体的核心特点。例如：

*   **簇 0:** 年轻、低收入、高消费意愿 -> "潜力年轻人" / "月光族"
*   **簇 1:** 中年、高收入、高消费 -> "高价值核心用户"
*   **簇 2:** 老年、中等收入、低消费 -> "保守储蓄型用户"
*   **簇 3:** 年龄收入不限、消费极低 -> "低活跃/流失风险用户"
*   **簇 -1 (噪声):** 行为模式异常或数据不足的用户 -> "待观察/异常用户"

标签应简洁、易懂，并能反映业务含义。

### 5.3 转化为商业洞察与行动

这是最终目的。根据不同用户群体的特征和标签，制定差异化的商业策略：

*   **对于 "潜力年轻人":**
    *   **洞察:** 对价格敏感，追求新潮，易受社交影响。
    *   **行动:** 推送优惠券、限时折扣；推广潮流新品；利用社交媒体营销。
*   **对于 "高价值核心用户":**
    *   **洞察:** 购买力强，注重品质和服务，忠诚度可能较高。
    *   **行动:** 提供 VIP 服务、专属优惠；推荐高端产品；进行客户关系维护。
*   **对于 "保守储蓄型用户":**
    *   **洞察:** 对价格敏感，需求明确，不易冲动消费。
    *   **行动:** 推送实用性强、性价比高的产品；强调产品耐用性。
*   **对于 "低活跃/流失风险用户":**
    *   **洞察:** 可能即将流失或对平台不感兴趣。
    *   **行动:** 进行召回活动（如发送关怀邮件、提供回归奖励）；分析流失原因；或暂时减少营销投入。
*   **对于 "待观察/异常用户" (噪声点):**
    *   **洞察:** 行为模式特殊，可能是新用户、异常账户或数据错误。
    *   **行动:** 进一步单独分析这些用户；检查数据质量；监控其后续行为。

**关键在于将数据分析结果与业务目标相结合，提出可行的、能带来价值的行动方案。**

## 6. 小组项目三：模型优化与业务解读

本周需要完成用户分群项目的最后部分。

*   **任务:**
    1.  **(可选) 尝试 DBSCAN:**
        *   在你的预处理数据上尝试使用 `DBSCAN` 进行聚类。
        *   仔细选择 `eps` 和 `min_samples` 参数（可以借助 K-距离图）。
        *   评估 DBSCAN 的结果（发现的簇数量、噪声点比例）。
        *   与 K-Means 的结果进行比较。DBSCAN 是否发现了更有意义的结构或识别出了噪声用户？
    2.  **选择最终聚类模型:** 根据 K-Means 和 (可选的) DBSCAN 的结果，以及聚类评估指标和可视化效果，选择一个最终的聚类方案（确定算法和参数/K值）。
    3.  **(重点) 深入业务解读:**
        *   将最终的聚类标签添加到你的数据中。
        *   详细分析每个簇在关键用户特征（人口统计学、消费行为、浏览行为等）上的表现，计算统计量并进行可视化对比（箱线图、条形图等）。
        *   为每个簇（包括可能的噪声簇）贴上具有业务含义的标签 (Profiling)。
        *   根据每个簇的特点，提出**具体、可行的商业建议或营销策略**。解释为什么这些策略适用于特定的用户群体。
*   **提交:**
    *   最终的代码，包含 K-Means、DBSCAN、聚类评估、可视化、**详细的业务解读过程和分析**。
    *   一份简洁的项目报告，总结项目目标、数据、方法（包括 K 值/参数选择）、最终聚类结果、**深入的各簇特征分析与业务洞察**、商业建议。**报告中必须包含详细的业务解读部分。**
*   **DDL:** 第九周第一次课前。

## 7. 本周总结

本周我们深入探讨了聚类分析。学习了基于密度的 DBSCAN 算法，了解了其原理、实现、参数选择和优缺点，并与 K-Means 进行了对比。我们还学习了更多的聚类评估指标（DBI）和可视化技术（PCA/t-SNE 降维）。最重要的是，我们强调了聚类结果的业务解读，学习了如何分析簇特征、给簇贴标签，并将其转化为商业价值。最后，我们完成了用户分群项目。

**下周我们将学习模型评估的深化技巧以及如何处理不平衡数据！**