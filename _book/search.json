[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "机器学习 (Python)",
    "section": "",
    "text": "欢迎来到 机器学习 (Python) 课程网站! 👋\n面向经管学生的实战型机器学习课程\n本课程 弱化复杂的数学理论，侧重机器学习和生成式AI的实战应用，并深度结合 AI 辅助编程工具，旨在帮助你快速掌握核心技能，并培养 数据驱动的商业创新思维。\n课程面向：经管类本科生、研究生，以及对机器学习和AI商业应用感兴趣的各专业学生\n核心技能：Python编程, 机器学习算法, 生成式AI, AI辅助编程\n课程目标：掌握机器学习和生成式AI核心概念与应用，能够运用经典算法和AI工具解决实际商业问题\n点击查看 课程大纲\n开始 第一周学习",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>机器学习 (Python)</span>"
    ]
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "课程大纲",
    "section": "",
    "text": "课程概述\n本课程为经管学生设计，旨在弱化复杂的数学理论，侧重机器学习和生成式AI的实战应用，并深度结合 AI 辅助编程工具。课程共计 16 周，每周两次课，每次 1.5 小时。课程以项目为中心，贯穿始终，帮助学生掌握核心技能并培养创新思维。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>课程大纲</span>"
    ]
  },
  {
    "objectID": "syllabus.html#课程目标",
    "href": "syllabus.html#课程目标",
    "title": "课程大纲",
    "section": "课程目标",
    "text": "课程目标\n完成本课程后，学生将能够：\n\n熟练运用 AI 辅助编程工具: 例如 GitHub Copilot, Cursor, 通义灵码等，提升代码编写效率和质量。\n理解核心概念: 解释机器学习和生成式AI的核心概念，并区分不同算法的适用场景。\n掌握编程工具: 熟练使用 VS Code, Cursor 等代码编辑器，以及 GitHub Copilot, 通义灵码, Cline 等 AI 辅助编程工具，以及 scikit-learn, TensorFlow, PyTorch, Langchain 等 Python 库进行机器学习和生成式AI编程。\n应用经典算法: 运用分类、回归、聚类等经典机器学习算法，以及 Prompt 工程、Stable Diffusion 等生成式AI技术，解决实际商业问题 (例如：客户分类、销量预测、图像生成、Agent 开发)。\n团队协作: 有效地进行团队合作，完成四个具有实际商业应用价值的小组项目，并撰写项目报告和进行项目展示。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>课程大纲</span>"
    ]
  },
  {
    "objectID": "syllabus.html#课程结构",
    "href": "syllabus.html#课程结构",
    "title": "课程大纲",
    "section": "课程结构",
    "text": "课程结构\n课程分为四个阶段，循序渐进地引导学生从理论到实践，最终完成小组项目，并拓展到更广泛的商业应用场景和伦理考量。\n\n每个阶段都将深度结合 AI 辅助编程工具，提升学习效率和编程体验。\n阶段一 (1-2周): 机器学习与Python入门\n\n目标：建立机器学习基础概念，熟悉Python编程环境和AI辅助工具。\n\n阶段二 (3-8周): 机器学习核心算法实践\n\n目标：掌握分类、回归、聚类等核心机器学习算法，并能应用于实际问题。\n\n阶段三 (9-13周): 生成式AI探索与应用\n\n目标：了解大语言模型、多模态AI等生成式AI技术，掌握Prompt工程和Agent开发等技能。\n\n阶段四 (14-16周): 小组综合项目实战与商业应用拓展\n\n目标：团队合作，综合应用机器学习和生成式AI技术解决实际商业问题，并拓展到更广泛的商业应用场景和伦理考量。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>课程大纲</span>"
    ]
  },
  {
    "objectID": "syllabus.html#每周课程安排",
    "href": "syllabus.html#每周课程安排",
    "title": "课程大纲",
    "section": "每周课程安排",
    "text": "每周课程安排\n以下是详细的每周课程内容和项目安排，每次课程 1.5 小时。\n\n第一阶段 (1-2周): 机器学习与Python入门\n\n第1周 (共2次课)\n\n\n第一次课: 机器学习导论与Python环境搭建\n\n内容：\n\n讲解机器学习的核心概念：监督学习、无监督学习、强化学习 (简要介绍)。\n对比机器学习与传统统计分析、数据挖掘。\n展示机器学习和生成式AI在商业领域的应用案例 (例如：精准营销、风险管理、智能客服、内容创作)。\nPython 机器学习环境搭建：VS Code, Python, Jupyter 插件 (Extension), scikit-learn, pandas, numpy 等常用库安装。\n介绍并体验 AI 辅助编程工具 (例如：GitHub Copilot, 通义灵码, Cline)。\n\n实践：\n\n熟悉 VS Code, Jupyter Notebook 插件和基本的 Python 数据操作。\n使用 AI 工具辅助编写简单的 Python 代码，体验代码自动补全和生成功能。\n\n\n第二次课: Python 基础语法与数据操作\n\n内容：\n\nPython 基础语法回顾：变量、数据类型、运算符、控制流 (循环、条件语句)、函数、模块。\nNumpy 基础：数组创建、数组运算、常用函数。\nPandas 基础：数据结构 (Series, DataFrame)、数据读取、数据选择、数据清洗 (重复值处理)。\n\n实践：\n\n完成 Python 基础语法和数据操作的编程练习。\n使用 AI 工具辅助完成 Python 代码。\n\n\n\n\n\n\n第2周 (共2次课)\n\n\n第一次课: 数据预处理与特征工程基础\n\n内容：\n\n数据加载与探索：使用 pandas 读取和查看数据，进行数据基本统计分析和可视化。\n数据清洗：缺失值处理策略 (删除、填充)、异常值检测与处理 (箱线图、Z-score)。\n特征工程基础：\n\n数值特征处理：特征缩放 (标准化、归一化)、特征转换 (多项式特征)。\n类别特征处理：独热编码、标签编码。\n\n使用 scikit-learn 进行数据预处理和特征工程的常用方法。\n\n实践：\n\n使用 pandas 和 numpy 进行实际的数据预处理操作练习。\n使用 AI 工具辅助完成数据预处理代码。\n\n小组项目一：电商用户行为数据探索与预处理 (案例介绍与数据准备)\n\n介绍电商用户行为数据集，明确项目目标：对用户行为数据进行探索性分析和预处理，为后续的机器学习建模做准备。\n布置小组项目一任务：数据探索、清洗和特征工程，下周课前提交预处理后的数据集和代码。\n\n\n第二次课: 小组项目一：电商用户行为数据探索与预处理 (数据分析与代码实践)\n\n内容：\n\n学生分组进行小组项目一：电商用户行为数据探索与预处理\n教师巡回指导，解答学生在数据分析和代码实现中遇到的问题。\n\n实践：\n\n学生以小组为单位，使用 Python 和 AI 工具，进行电商用户行为数据探索、数据清洗和特征工程。\n完成小组项目一代码初稿。\n\n\n\n\n\n\n\n第二阶段 (3-8周): 机器学习核心算法实践\n\n第3周 (共2次课)\n\n\n第一次课: 分类算法 (一) - 逻辑回归与支持向量机 (SVM)\n\n内容：\n\n分类问题概述：应用场景 (客户分类、风险预测、垃圾邮件识别等)。\n逻辑回归：原理、公式推导 (简要介绍)、scikit-learn 实现、模型评估指标 (准确率、精确率、召回率、F1-score、AUC-ROC)。\n支持向量机 (SVM)：原理 (最大间隔、核函数 - 线性核、RBF核)、scikit-learn 实现、SVM 的优缺点。\n模型选择与调优：交叉验证、网格搜索 (GridSearchCV) (简要介绍)。\n\n实践：\n\n使用 scikit-learn 构建和评估逻辑回归和 SVM 分类模型，应用于电商用户行为数据集 (小组项目一)。\n使用 AI 工具辅助完成模型训练和评估代码。\n\n小组项目一：电商用户行为数据分类模型构建 (逻辑回归或SVM)\n\n指导学生使用逻辑回归或 SVM 算法，基于预处理后的电商用户行为数据，构建用户分类模型 (例如：用户购买意愿预测、用户价值分层)。\n明确小组项目一提交内容：模型代码、实验结果报告 (包括模型评估指标)。\n\n\n第二次课: 小组项目一：电商用户行为数据分类模型构建 (模型训练与代码实践)\n\n内容：\n\n学生分组进行小组项目一：电商用户行为数据分类模型构建 (逻辑回归或SVM)\n教师巡回指导，解答学生在模型训练和代码实现中遇到的问题。\n\n实践：\n\n学生以小组为单位，使用 Python 和 AI 工具，构建和训练逻辑回归或 SVM 分类模型。\n完成小组项目一代码和实验报告初稿。\n\n\n\n\n\n\n第4周 (共2次课)\n\n\n第一次课: 分类算法 (二) - 决策树与集成学习 (随机森林)\n\n内容：\n\n决策树：原理、信息增益/基尼系数、scikit-learn 实现、决策树的优缺点、可视化 (graphviz)。\n集成学习：Bagging (随机森林)：原理、scikit-learn 实现、随机森林的优缺点、特征重要性。\n模型评估与选择：模型评估指标回顾、交叉验证、网格搜索 (GridSearchCV) 详细讲解与实践。\n分类模型评估指标的选择和应用场景。\n\n实践：\n\n使用 scikit-learn 构建和评估决策树和随机森林分类模型，并进行模型调优。\n比较不同分类模型在电商用户行为数据集上的性能。\n使用 AI 工具辅助完成模型训练、评估和调优代码。\n\n小组项目一：电商用户行为数据分类模型优化 (随机森林)\n\n指导学生使用随机森林算法，优化电商用户行为数据分类模型，并进行模型性能评估和比较。\n小组项目一：电商用户行为数据探索与预处理 提交 (第5周课前)\n\n\n第二次课: 小组项目一：电商用户行为数据分类优化 (模型优化与报告撰写)\n\n内容：\n\n学生分组进行小组项目一：电商用户行为数据分类优化 (随机森林)\n教师巡回指导，解答学生在模型优化和代码实现中遇到的问题。\n\n实践：\n\n学生以小组为单位，使用 Python 和 AI 工具，优化随机森林分类模型。\n撰写小组项目一实验报告，包括数据预处理、模型构建、模型评估、结果分析和结论。\n\n\n\n\n\n\n第5周 (共2次课)\n\n\n第一次课: 回归算法 (一) - 线性回归与多项式回归\n\n内容：\n\n回归问题概述：应用场景 (房价预测、销量预测、股票价格预测等)。\n线性回归：原理、公式推导 (简要介绍)、scikit-learn 实现、模型评估指标 (均方误差 MSE、均方根误差 RMSE、平均绝对误差 MAE、R-squared)。\n多项式回归：原理、scikit-learn 实现、多项式回归与线性回归的关系。\n正则化：L1 正则化 (LASSO)、L2 正则化 (Ridge) (简要介绍)。\n\n实践：\n\n使用 scikit-learn 构建和评估线性回归和多项式回归模型，应用于房价预测数据集 (小组项目二)。\n使用 AI 工具辅助完成模型训练和评估代码。\n\n小组项目二：房价预测模型构建 (线性回归或多项式回归)\n\n指导学生使用线性回归或多项式回归算法，基于房价数据集，构建房价预测模型。\n明确小组项目二提交内容：模型代码、实验结果报告 (包括模型评估指标)。\n\n\n第二次课: 小组项目二：房价预测模型构建 (模型训练与代码实践)\n\n内容：\n\n学生分组进行小组项目二：房价预测模型构建 (线性回归或多项式回归)\n教师巡回指导，解答学生在模型训练和代码实现中遇到的问题。\n\n实践：\n\n学生以小组为单位，使用 Python 和 AI 工具，构建和训练线性回归或多项式回归模型。\n完成小组项目二代码和实验报告初稿。\n\n\n\n\n\n\n第6周 (共2次课)\n\n\n第一次课: 回归算法 (二) - 集成学习 (XGBoost)\n\n内容：\n\n集成学习：Boosting (XGBoost)：原理 (梯度提升、树模型)、xgboost 库实现、XGBoost 的优缺点、模型参数调优。\n回归模型评估指标的选择和应用场景。\n\n实践：\n\n使用 xgboost 库构建和评估 XGBoost 回归模型，并进行模型调优。\n比较 XGBoost 与线性回归、多项式回归在房价预测数据集上的性能。\n使用 AI 工具辅助完成模型训练、评估和调优代码。\n\n小组项目二：房价预测模型优化 (XGBoost)\n\n指导学生使用 XGBoost 算法，优化房价预测模型，并进行模型性能评估和比较。\n小组项目二：房价预测模型构建 (线性/多项式回归) 提交 (第7周课前)\n\n\n第二次课: 小组项目二：房价预测模型优化 (模型优化与报告撰写)\n\n内容：\n\n学生分组进行小组项目二：房价预测模型优化 (XGBoost)\n教师巡回指导，解答学生在模型优化和代码实现中遇到的问题。\n\n实践：\n\n学生以小组为单位，使用 Python 和 AI 工具，优化 XGBoost 回归模型。\n撰写小组项目二实验报告，包括模型构建、模型评估、模型调优、结果分析和结论。\n\n\n\n\n\n\n第7周 (共2次课)\n\n\n第一次课: 聚类算法 (一) - K-Means\n\n内容：\n\n聚类问题概述：应用场景 (用户分群、商品推荐、异常检测等)。\nK-Means 聚类：原理、算法步骤、scikit-learn 实现、K 值选择 (轮廓系数、肘部法则)、K-Means 的优缺点。\n聚类模型评估指标：轮廓系数、 Davies-Bouldin 指数 (简要介绍)。\n聚类结果可视化：散点图、聚类中心可视化。\n\n实践：\n\n使用 scikit-learn 构建和评估 K-Means 聚类模型，应用于用户分群数据集 (小组项目三)。\n使用 AI 工具辅助完成模型训练、评估和可视化代码。\n\n小组项目三：用户分群模型构建 (K-Means)\n\n指导学生使用 K-Means 聚类算法，基于用户分群数据集，构建用户分群模型。\n明确小组项目三提交内容：模型代码、实验结果报告 (包括聚类结果可视化和分析)。\n\n\n第二次课: 小组项目三：用户分群模型构建 (模型训练与代码实践)\n\n内容：\n\n学生分组进行小组项目三：用户分群模型构建 (K-Means)\n教师巡回指导，解答学生在模型训练和代码实现中遇到的问题。\n\n实践：\n\n学生以小组为单位，使用 Python 和 AI 工具，构建和训练 K-Means 聚类模型。\n完成小组项目三代码和实验报告初稿。\n\n\n\n\n\n\n第8周 (共2次课)\n\n\n第一次课: 聚类算法 (二) - DBSCAN\n\n内容：\n\nDBSCAN 聚类：原理 (密度聚类)、算法步骤、scikit-learn 实现、DBSCAN 的优缺点、参数选择。\n聚类算法选择：K-Means vs. DBSCAN，不同聚类算法的应用场景。\n聚类结果评估：轮廓系数、 Davies-Bouldin 指数 详细讲解与实践，聚类结果的业务意义解读。\n聚类结果可视化：高级可视化技巧，例如使用 seaborn 或 plotly 绘制更美观、更具信息量的聚类结果图。\n\n实践：\n\n使用 scikit-learn 构建和评估 DBSCAN 聚类模型，并进行模型调优和参数选择。\n比较 K-Means 和 DBSCAN 在用户分群数据集上的性能和聚类效果。\n使用 AI 工具辅助完成模型训练、评估、可视化和调优代码。\n\n小组项目三：用户分群模型优化 (DBSCAN 或 K-Means 优化)\n\n指导学生使用 DBSCAN 算法或优化 K-Means 算法 (例如：尝试不同的 K 值或初始化方法)，优化用户分群模型，并进行模型性能评估和聚类结果分析。\n小组项目三：用户分群模型构建 (K-Means) 提交 (第9周课前)\n\n\n第二次课: 小组项目三：用户分群模型优化 (模型优化与报告撰写)\n\n内容：\n\n学生分组进行小组项目三：用户分群模型优化 (DBSCAN 或 K-Means 优化)\n教师巡回指导，解答学生在模型优化和代码实现中遇到的问题。\n\n实践：\n\n学生以小组为单位，使用 Python 和 AI 工具，优化 DBSCAN 或 K-Means 聚类模型。\n撰写小组项目三实验报告，包括模型构建、模型评估、聚类结果可视化、结果分析和结论。\n\n\n\n\n\n\n\n第三阶段 (9-13周): 生成式AI探索与应用\n\n第9周 (共2次课)\n\n\n第一次课: 生成式AI 导论 - 大语言模型 (LLM) 与 Prompt 工程\n\n内容：\n\n生成式AI 概述：发展历程、应用场景 (内容创作、对话系统、代码生成等)、与判别式 AI 的区别。\n大语言模型 (LLM)：Transformer 架构 (简要介绍)、预训练、微调、代表性 LLM (GPT, BERT, Llama 等)。\nPrompt 工程：Prompt 的概念、Prompt 设计原则 (清晰、具体、简洁)、常用 Prompt 技巧 (Few-shot, Chain-of-Thought, Instruction Tuning)。\nLangchain 框架：Langchain 简介、核心组件 (Models, Prompts, Chains, Agents)、Langchain 的应用场景。\n\n实践：\n\n体验在线 LLM 产品 (例如：ChatGPT, 文心一言, Bard)，尝试不同的 Prompt，体验 LLM 的文本生成能力。\n使用 AI 工具辅助进行 Prompt 设计和优化。\n\n小组项目四：智能文案生成 Agent 开发 (项目启动与 Prompt 设计)\n\n介绍小组项目四：智能文案生成 Agent 开发，明确项目目标：利用 LLM 和 Langchain 框架，开发一个能够根据用户需求自动生成高质量文案的 Agent。\n布置小组项目四任务：Prompt 设计、Agent 框架搭建、核心功能开发和扩展功能探索。\n小组项目三：用户分群模型优化 (DBSCAN/K-Means) 及报告 提交 (第10周课前)\n\n\n第二次课: 小组项目四：智能文案生成 Agent Prompt 优化与 Langchain 框架初探\n\n内容：\n\n学生分组进行小组项目四：智能文案生成 Agent Prompt 优化\n教师巡回指导，解答学生在 Prompt 设计和优化中遇到的问题。\nLangchain 框架初探：安装 Langchain, Langchain 核心组件 (Models, Prompts) 实践，构建简单的 LLM 应用。\n\n实践：\n\n学生以小组为单位，使用 Python 和 AI 工具，进行智能文案生成 Agent 的 Prompt 设计和优化。\n完成小组项目四 Prompt 设计方案初稿。\n初步体验 Langchain 框架，完成简单的 LLM 应用代码。\n\n\n\n\n\n\n第10周 (共2次课)\n\n\n第一次课: Langchain 核心组件与 Agent 框架搭建\n\n内容：\n\nLangchain 核心组件：Chains (SequentialChain, SimpleSequentialChain, TransformChain), Memory (ConversationBufferMemory, ConversationChain)。\nAgent 框架搭建：Agent 的概念、Agent 类型 (Zero-shot Agent, Conversational Agent)、Langchain Agent 实现、Agent 工具 (Tools) 的使用。\n智能文案生成 Agent 框架设计：确定 Agent 类型、选择 Tools, 设计 Agent 的工作流程。\n\n实践：\n\n使用 Langchain 框架，构建智能文案生成 Agent 的基本框架，包括 Agent, Tools 和 Memory 组件。\n使用 AI 工具辅助完成 Agent 框架代码。\n\n小组项目四：智能文案生成 Agent Agent 框架搭建\n\n指导学生使用 Langchain 框架，搭建智能文案生成 Agent 的基本框架，为后续功能开发奠定基础。\n小组项目四：智能文案生成 Agent Prompt 设计方案 提交 (第11周课前)\n\n\n第二次课: 小组项目四：智能文案生成 Agent Agent 框架代码实践\n\n内容：\n\n学生分组进行小组项目四：智能文案生成 Agent Agent 框架代码实现\n教师巡回指导，解答学生在 Agent 框架搭建和代码实现中遇到的问题。\n\n实践：\n\n学生以小组为单位，使用 Python 和 AI 工具，完成智能文案生成 Agent 的框架代码。\n完成小组项目四 Agent 框架代码初稿。\n\n\n\n\n\n\n第11周 (共2次课)\n\n\n第一次课: 智能文案生成 Agent 核心功能开发 (文案生成)\n\n内容：\n\n智能文案生成 Agent 核心功能开发：\n\n文案生成流程设计： 用户需求分析、Prompt 构建、LLM 调用、文案生成结果处理。\nLangchain Chains 应用： 使用 Chains 组合 Prompt 和 LLM, 实现文案生成流程。\n文案生成质量评估： 常用评估指标 (流畅度、相关性、信息量)、人工评估方法。\n\nLLM 调用与 API Key 管理：如何调用 LLM API (例如：OpenAI API)、API Key 的申请和安全管理。\n\n实践：\n\n使用 Langchain Chains 和 LLM API，实现智能文案生成 Agent 的核心功能：根据用户输入的关键词或主题，自动生成文案。\n使用 AI 工具辅助完成文案生成功能代码。\n\n小组项目四：智能文案生成 Agent 核心功能 (文案生成) 开发\n\n指导学生实现智能文案生成 Agent 的核心功能，使其能够根据用户需求生成基本的文案内容。\n小组项目四：智能文案生成 Agent Agent 框架代码 提交 (第12周课前)\n\n\n第二次课: 小组项目四：智能文案生成 Agent 核心功能代码实践\n\n内容：\n\n学生分组进行小组项目四：智能文案生成 Agent 核心功能 (文案生成) 代码实现\n教师巡回指导，解答学生在文案生成功能开发和代码实现中遇到的问题。\n\n实践：\n\n学生以小组为单位，使用 Python 和 AI 工具，完成智能文案生成 Agent 的核心功能代码。\n完成小组项目四 Agent 核心功能代码初稿。\n\n\n\n\n\n\n第12周 (共2次课)\n\n\n第一次课: 智能文案生成 Agent 功能扩展 (一) - 文案风格控制与优化\n\n内容：\n\n智能文案生成 Agent 功能扩展：\n\n文案风格控制： Prompt 技巧 (风格关键词、指令)、参数调整 (temperature, top_p)、实现文案风格多样化 (例如：正式、幽默、创意)。\n文案优化： Prompt 优化技巧、迭代优化流程、提升文案质量 (例如：信息准确性、逻辑性、吸引力)。\nLangchain Memory 应用： 在 Agent 中引入 Memory 组件，实现对话记忆功能，提升文案生成的连贯性和上下文相关性。\n\nLLM 参数调优：temperature, top_p 等常用参数的含义和作用，参数调优策略。\n\n实践：\n\n扩展智能文案生成 Agent 的功能，实现文案风格控制和优化。\n尝试不同的 Prompt 技巧和参数调整，提升文案生成质量和多样性。\n使用 AI 工具辅助完成功能扩展代码。\n\n小组项目四：智能文案生成 Agent 功能扩展 (文案风格控制与优化)\n\n指导学生扩展智能文案生成 Agent 的功能，使其能够控制文案风格和进行文案优化，提升 Agent 的实用性和用户体验。\n小组项目四：智能文案生成 Agent 核心功能代码 提交 (第13周课前)\n\n\n第二次课: 小组项目四：智能文案生成 Agent 功能扩展代码实践\n\n内容：\n\n学生分组进行小组项目四：智能文案生成 Agent 功能扩展 (文案风格控制与优化) 代码实现\n教师巡回指导，解答学生在功能扩展代码实现中遇到的问题。\n\n实践：\n\n学生以小组为单位，使用 Python 和 AI 工具，完成智能文案生成 Agent 的功能扩展代码。\n完成小组项目四 Agent 功能扩展代码初稿。\n\n\n\n\n\n\n第13周 (共2次课)\n\n\n第一次课: 智能文案生成 Agent 功能扩展 (二) - 文生图 (可选) 与多模态 AI 初探\n\n内容：\n\n智能文案生成 Agent 功能扩展 (可选)：\n\n文生图功能集成： Stable Diffusion WebUI 介绍、API 调用、Langchain 工具集成、实现文生图功能。\n多模态 AI 初探： 多模态 AI 概念、应用场景 (图文生成、跨模态检索)、多模态 LLM 简介 (例如：LLaVA, VisualGPT)。\nAgent 功能扩展思路： 更多 Agent 功能扩展方向 (例如：文案编辑、多语言支持、内容发布)。\n\nStable Diffusion WebUI 实践：安装和配置 Stable Diffusion WebUI, 体验文生图功能，了解常用模型和参数。\n\n实践：\n\n为智能文案生成 Agent 添加文生图功能 (可选)，实现图文结合的内容创作。\n体验 Stable Diffusion WebUI 的文生图功能，尝试不同的模型和 Prompt。\n使用 AI 工具辅助完成文生图功能集成代码。\n\n小组项目四：智能文案生成 Agent 功能扩展 (文生图 - 可选) 与 Agent 报告撰写\n\n鼓励学生为智能文案生成 Agent 添加文生图功能，探索多模态 AI 应用，并开始撰写小组项目四实验报告。\n小组项目四：智能文案生成 Agent 功能扩展代码 提交 (第14周课前)\n\n\n第二次课: 小组项目四：智能文案生成 Agent 功能扩展代码实践与报告撰写指导\n\n内容：\n\n学生分组进行小组项目四：智能文案生成 Agent 文生图功能探索 (可选) 代码实现\n教师巡回指导，解答学生在文生图功能集成和代码实现中遇到的问题。\n小组项目四报告撰写指导： 报告结构、内容要点、撰写技巧。\n\n实践：\n\n学生以小组为单位，使用 Python 和 AI 工具，完成智能文案生成 Agent 的文生图功能集成代码 (可选)。\n开始撰写小组项目四实验报告。\n\n\n\n\n\n\n\n第四阶段 (14-16周): 小组综合项目实战与商业应用拓展\n\n第14-15周 (共4次课): 小组综合项目开发与指导\n\n\n内容：\n\n学生以小组为单位，进行小组综合项目的设计、开发和实验。\n\n指导：\n\n教师在课堂上提供项目指导和技术支持，解答学生在项目开发过程中遇到的问题。\n\n协作：\n\n鼓励学生充分利用 AI 辅助编程工具，提高开发效率，加强团队协作。\n\n检查：\n\n第 15 周进行小组综合项目中期检查，了解项目进展，及时发现和解决问题。\n\n\n\n\n\n第16周 (共1次课): 小组综合项目展示与课程总结\n\n\n第一次课: 小组综合项目展示与答辩\n\n展示：\n\n各小组进行项目成果展示，讲解项目背景、问题定义、技术方案、实验结果、商业价值和创新点。\n\n答辩：\n\n接受教师和同学的提问和点评。\n\n小组综合项目：开放式商业问题 提交 (本周课前)\n\n第二次课: 课程总结与未来展望\n\n总结：\n\n回顾课程内容，总结机器学习和生成式AI的核心概念、算法、技术和应用。\n\n展望：\n\n展望机器学习和生成式AI 在商业领域的未来发展前景，鼓励学生继续深入学习和探索。\n再次强调生成式AI 技术的商业伦理和社会责任。\n\n评估：\n\n进行课程评估和反馈，收集学生对课程的意见和建议，为课程改进提供参考。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>课程大纲</span>"
    ]
  },
  {
    "objectID": "syllabus.html#项目设置-贯穿课程",
    "href": "syllabus.html#项目设置-贯穿课程",
    "title": "课程大纲",
    "section": "项目设置 (贯穿课程)",
    "text": "项目设置 (贯穿课程)\n\n小组项目 (共4个 + 综合项目)\n\n小组项目一 (电商用户行为数据分析与分类)\n\n学习目标：掌握数据预处理、特征工程、分类算法 (逻辑回归, SVM, 决策树, 随机森林) 的应用，理解分类模型评估指标。\n周期：2 周 (数据预处理 + 模型构建与优化)\n提交时间：\n\n数据预处理结果：第 2 周课前\n分类模型 (逻辑回归/SVM)：第 3 周课前\n分类模型 (随机森林) 及报告：第 5 周课前\n\n数据集: 学生小组自主选择数据集，建议选择电商或零售行业的用户行为数据集。数据集应包含用户行为数据，数据量适中，特征维度不低于10个，且具有一定的分类难度。 鼓励各小组选择不同的数据集，以提高项目实践的多样性。\n\n小组项目二 (房价预测模型构建与优化)\n\n学习目标：掌握回归算法 (线性回归, 多项式回归, XGBoost) 的应用，理解正则化和模型调优方法，掌握回归模型评估指标。\n周期：2 周 (模型构建 + 模型优化与调优)\n提交时间：\n\n回归模型 (线性回归/多项式回归)：第 5 周课前\n回归模型 (XGBoost) 及报告：第 7 周课前\n\n数据集: 学生小组自主选择数据集，建议选择包含城市或地区房价信息的公开数据集。数据集应包含房价相关特征，数据量不低于500条，特征维度不低于8个，且特征之间应具有一定的相关性。 鼓励各小组选择不同的数据集，以提高项目实践的多样性。\n\n小组项目三 (用户分群模型构建与分析)\n\n学习目标：掌握聚类算法 (K-Means, DBSCAN) 的应用，理解聚类算法选择和聚类结果评估方法，掌握聚类结果可视化和分析技巧。\n周期：2 周 (模型构建 + 模型优化与结果分析)\n提交时间：\n\n聚类模型 (K-Means)：第 7 周课前\n聚类模型 (DBSCAN 或 K-Means 优化) 及报告：第 9 周课前\n\n数据集: 学生小组自主选择数据集，建议选择包含用户画像或用户行为特征的数据集。数据集应适用于用户分群任务，数据量不低于300条，特征维度不低于5个，且数据集中应包含可用于区分用户群体的特征。 鼓励各小组选择不同的数据集，以提高项目实践的多样性。\n\n小组项目四 (智能文案生成 Agent 开发)\n\n学习目标：掌握 Prompt 工程、Langchain 框架和 Agent 开发技术，理解 LLM 的应用和局限性，初步接触多模态 AI (可选)。\n周期：4 周 (Prompt 设计 + Agent 框架 + 功能完善 + 扩展探索)\n提交时间：\n\nPrompt 设计方案和 Agent 代码框架：第 10 周课前\nAgent 核心功能 (文案生成)：第 12 周课前\nAgent 功能扩展 (可选) 及报告：第 14 周课前\n\n数据集: 本项目不涉及传统数据集，学生需自行准备或生成 Agent 应用所需的示例数据或知识库 (例如：产品信息、营销知识等)。鼓励各小组根据 Agent 应用场景，构建不同的知识库或示例数据，以体现 Agent 的个性化和创新性。\n\n小组综合项目 (开放式商业问题)\n\n学习目标：综合应用课程所学机器学习和生成式AI知识，解决开放式商业问题，提升创新思维、团队协作和项目管理能力。\n周期：3 周 (项目选题与方案设计 + 项目开发与实验 + 项目展示与报告)\n提交时间：\n\n小组综合项目报告和展示材料：第 16 周课前\n\n数据集: 学生小组根据所选的开放式商业问题自主选择或构建数据集，数据集应与项目主题紧密相关，数据量和难度应与项目目标相匹配。 鼓励各小组选择不同的数据集，以提高项目实践的多样性。\n\n小组人数：建议每组 2-3 人。\n项目形式: 小组合作完成，包括项目方案设计、数据准备 (或环境搭建)、算法/模型实现、实验验证、结果分析、项目展示和项目报告。 每次小组项目提交后，选取 3 个优秀小组在课堂上进行项目讲解和展示，分享经验和成果。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>课程大纲</span>"
    ]
  },
  {
    "objectID": "syllabus.html#ai-辅助编程工具",
    "href": "syllabus.html#ai-辅助编程工具",
    "title": "课程大纲",
    "section": "AI 辅助编程工具",
    "text": "AI 辅助编程工具\n\n代码编辑器: VS Code, Cursor\n\n特点：轻量级、高效、插件丰富，支持 Jupyter Notebook，适合 Python 开发和机器学习。\n\n代码辅助插件: GitHub Copilot, 通义灵码, Cline 等\n\n功能：代码自动补全、代码片段生成、代码错误检查、自然语言代码解释等，提高编程效率，降低编程难度，辅助 Prompt 设计和 Agent 开发。\n\n机器学习/深度学习库: scikit-learn, TensorFlow, PyTorch\n\n功能：提供丰富的机器学习和深度学习算法、模型和工具，方便学生进行模型构建、训练和评估。\n\n生成式AI 框架: Langchain\n\n功能：简化 LLM 应用和 Agent 开发流程，提供丰富的组件和工具，方便学生快速构建生成式AI应用。\n\n文生图工具: Stable Diffusion WebUI\n\n功能：强大的文生图工具，支持多种模型和参数调整，方便学生进行图像生成和创意设计。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>课程大纲</span>"
    ]
  },
  {
    "objectID": "syllabus.html#考核方式",
    "href": "syllabus.html#考核方式",
    "title": "课程大纲",
    "section": "考核方式",
    "text": "考核方式\n\n小组项目一 ~ 四 (40%): 根据项目完成质量、代码质量、实验结果、创新性和项目报告进行综合评分，每个项目 10%。\n小组综合项目 (20%): 根据项目选题意义、创新性、实用性、完成度、展示效果和项目报告进行评分 (小组内成员评分可以考虑组内互评和贡献度)。\n课堂参与 (10%): 根据课堂讨论、提问、出勤、作业完成情况等进行评分，鼓励学生积极参与课堂互动。\n期末考试 (30%): 期末考试采用闭卷形式，重点考察学生对机器学习和生成式AI 基础理论、核心概念和算法的掌握程度。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>课程大纲</span>"
    ]
  },
  {
    "objectID": "week1.html",
    "href": "week1.html",
    "title": "第一周：机器学习导论与Python环境搭建",
    "section": "",
    "text": "第一次课：机器学习导论与Python环境搭建",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>第一周：机器学习导论与Python环境搭建</span>"
    ]
  },
  {
    "objectID": "week1.html#第一次课机器学习导论与python环境搭建",
    "href": "week1.html#第一次课机器学习导论与python环境搭建",
    "title": "第一周：机器学习导论与Python环境搭建",
    "section": "",
    "text": "1. 机器学习导论\n\n\n\n\n\n\n什么是机器学习\n\n\n\n无需显式编程，让计算机从数据中学习规律和知识。\n与传统编程的区别： 传统编程 (显式规则) vs. 机器学习 (从数据中学习规则)。\n\n\n\n\n\n\n\n\n机器学习的核心概念\n\n\n\n\n监督学习 (Supervised Learning)\n从带有标签的数据中学习，用于分类和回归任务。\n\n示例： 垃圾邮件识别 (分类)、房价预测 (回归)\n\n\n\n无监督学习 (Unsupervised Learning)\n从无标签数据中学习数据结构和模式，用于聚类、降维、关联规则挖掘等任务。\n\n示例： 用户分群 (聚类)、商品推荐 (关联规则)\n\n\n\n强化学习 (Reinforcement Learning)\n通过与环境交互学习最优策略，以获得最大奖励。\n\n示例： 游戏 AI、自动驾驶\n\n\n\n\n\n\n\n\n\n\n机器学习与相关学科\n\n\n\n\n机器学习 vs. 传统统计分析\n\n侧重点不同： 统计分析 (解释性、推断) vs. 机器学习 (预测性、性能)\n方法论差异： 统计分析 (模型假设、参数估计) vs. 机器学习 (算法迭代、模型优化)\n\n\n\n机器学习 vs. 数据挖掘\n\n目标相似： 从数据中发现知识和模式\n侧重面不同： 数据挖掘 (更侧重数据预处理、模式发现) vs. 机器学习 (更侧重模型构建、预测和决策)\n\n\n\n\n\n\n\n\n\n\n机器学习和生成式AI 的商业应用案例\n\n\n\n\n精准营销： 用户画像、个性化推荐、广告投放优化\n风险管理： 信用评分、欺诈检测、风险预警\n智能客服： 聊天机器人、智能问答、工单自动化\n内容创作： 文案生成、图像生成、音乐创作、代码生成 (生成式AI)\n智能产品与服务： 智能家居、自动驾驶、智能医疗\n\n\n\n\n\n2. Python 机器学习环境搭建\n\n\n\n\n\n\nPython 解释器安装\n\n\n\n\n从 Python 官网下载并安装 Python 解释器： https://www.python.org/downloads/ (建议安装 3.9 及以上版本)\n安装过程中，务必勾选 “Add Python to PATH”\n验证安装： 打开命令行，输入 python --version 和 pip --version\n\n\n\n\n\n\n\n\n\nVS Code 编辑器安装与配置\n\n\n\n\n安装 VS Code： https://code.visualstudio.com/download\n安装 Python 插件：\n\n打开 VS Code，点击左侧 “Extensions” 图标\n搜索并安装 “Python” 插件 (由 Microsoft 提供)\n功能： 代码智能提示、格式化、调试、Jupyter Notebook 支持等\n\n选择 Python 解释器：\n\n点击窗口右下角的 Python 版本号\n或使用 Ctrl+Shift+P / Cmd+Shift+P 选择解释器\n\n\n\n\n\n\n\n\n\n\n虚拟环境管理\n\n\n\n什么是虚拟环境： 隔离的 Python 环境，可以为不同项目安装不同的 Python 包，避免包版本冲突。\n\n创建虚拟环境： python -m venv .venv\n激活虚拟环境：\n\nWindows (cmd): .venv\\Scripts\\activate.bat\nWindows (PowerShell): .venv\\Scripts\\Activate.ps1\nmacOS/Linux: source .venv/bin/activate\n\n在 VS Code 中使用： VS Code 会自动检测项目中的虚拟环境\n\n\n\n\n\n\n\n\n\nPython 常用库安装\n\n\n\n在虚拟环境中安装以下库：\npip install scikit-learn pandas numpy matplotlib seaborn\n配置 pip 国内镜像源 (推荐)：\n\n创建配置文件：\n\nWindows: %USERPROFILE%\\pip\\pip.ini\nmacOS/Linux: ~/.pip/pip.conf\n\n添加配置内容：\n\n[global]\nindex-url = https://pypi.tuna.tsinghua.edu.cn/simple\n[install]\ntrusted-host = pypi.tuna.tsinghua.edu.cn\n\n\n\n\n\n\n\n\nAI 辅助编程工具体验\n\n\n\n\nGitHub Copilot: github.com/features/copilot\n通义灵码: tongyi.aliyun.com/lingma\nCursor: cursor.sh",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>第一周：机器学习导论与Python环境搭建</span>"
    ]
  },
  {
    "objectID": "week1.html#第二次课python-基础语法与数据操作",
    "href": "week1.html#第二次课python-基础语法与数据操作",
    "title": "第一周：机器学习导论与Python环境搭建",
    "section": "第二次课：Python 基础语法与数据操作",
    "text": "第二次课：Python 基础语法与数据操作\n\n\n\n\n\n\n课程目标\n\n\n\n\n回顾 Python 基础语法\n掌握 Numpy 库的基本操作\n掌握 Pandas 库的基本操作\n能够使用 AI 辅助编程工具完成编程练习\n\n\n\n\n1. Python 基础语法回顾\n\n\n\n\n\n\n基础语法要点\n\n\n\n\n变量与数据类型\n\n变量命名规则、动态类型\n常用数据类型：int, float, str, bool, list, tuple, dict, set\n\n\n\n运算符\n\n算术运算符\n比较运算符\n逻辑运算符\n赋值运算符\n成员运算符\n身份运算符\n\n\n\n控制流\n\n条件语句：if, elif, else\n循环语句：for, while, break, continue\n\n\n\n函数\n\n函数定义\n函数参数\n函数返回值\n匿名函数 (lambda)\n\n\n\n模块与包\n\n模块导入：import, from ... import ...\n常用内置模块：math, random, os, sys, datetime\n\n\n\n\n\n\n2. Numpy 基础\n\n\n\n\n\n\nNumpy 数组操作\n\n\n\n\n数组创建\n\nnp.array(), np.zeros(), np.ones()\nnp.arange(), np.linspace()\nnp.random.rand()\n\n\n\n数组属性\n\nshape, dtype, ndim, size\n数组索引和切片\n维度变换：reshape(), flatten(), transpose()\n\n\n\n\n\n\n\n\n\n\n数组运算\n\n\n\n\n元素级运算： 加减乘除、幂运算、比较运算\n矩阵运算： 矩阵乘法 (np.dot(), @ 运算符)\n广播机制\n\n\n常用函数\n\n数学函数： np.sin(), np.cos(), np.exp(), np.log(), np.sqrt()\n统计函数： np.mean(), np.median(), np.std(), np.max(), np.min(), np.sum()\n排序和查找： np.sort(), np.argsort(), np.unique(), np.where()\n\n\n\n\n\n\n3. Pandas 基础\n\n\n\n\n\n\nPandas 数据结构\n\n\n\n\nSeries\n\n带标签的一维数组\n创建、索引、切片、属性和方法\n\n\n\nDataFrame\n\n带标签的二维数据表\n创建、列选择、行选择、索引、切片\n常用属性和方法\n\n\n\n\n\n\n\n\n\n\n数据操作\n\n\n\n\n数据读取\n\npd.read_csv(): 读取 CSV 文件\npd.read_excel(): 读取 Excel 文件\npd.read_json(): 读取 JSON 文件\n常用参数：filepath_or_buffer, sep, header, index_col, encoding\n\n\n\n数据选择\n\n列选择：df['列名'], df[['列名1', '列名2']]\n行选择：df.loc[], df.iloc[], 条件索引\n索引方式的区别和应用场景\n\n\n\n数据清洗\n\n重复值处理：df.duplicated(), df.drop_duplicates()\n缺失值处理：df.isnull(), df.fillna(), df.dropna()\n\n\n\n\n\n\n\n\n\n\n实践环节\n\n\n\n\nPython 基础语法练习\nNumpy 数组操作练习\nPandas 数据操作练习\n使用 AI 工具辅助完成代码\n\n\n\n\n\n\n\n\n\n课后作业\n\n\n\n\n完成 Python 基础语法和数据操作的编程练习\n将代码上传到 GitHub 仓库\n预习下周课程内容：数据预处理与特征工程基础\n\n\n\n\n\n\n\n\n\n相关资源\n\n\n\n\nPython 官方文档\nNumpy 官方文档\nPandas 官方文档\nW3School Python 教程\n廖雪峰 Python 教程",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>第一周：机器学习导论与Python环境搭建</span>"
    ]
  },
  {
    "objectID": "week2.html",
    "href": "week2.html",
    "title": "第二周：数据预处理与特征工程基础",
    "section": "",
    "text": "第一次课：数据预处理与特征工程基础",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>第二周：数据预处理与特征工程基础</span>"
    ]
  },
  {
    "objectID": "week2.html#第一次课数据预处理与特征工程基础",
    "href": "week2.html#第一次课数据预处理与特征工程基础",
    "title": "第二周：数据预处理与特征工程基础",
    "section": "",
    "text": "1. 数据加载与探索\n\n\n\n\n\n\n数据加载基础知识\n\n\n\n1.1 使用 Pandas 读取数据\n常用函数: pd.read_csv(), pd.read_excel() 等\n常用参数:\n\nfilepath_or_buffer: 文件路径 (必需)\nsep 或 delimiter: 分隔符 (默认为 ,)\nheader: 列名行 (默认为 0)\nindex_col: 索引列\nencoding: 文件编码 (例如 utf-8, gbk)\n\n注意: 如果读取 CSV 文件出现乱码，可以尝试更换 encoding 参数。\n\n\n\n\n\n\n\n\n数据查看常用方法\n\n\n\n1.2 数据查看\n\ndf.head(n): 查看前 n 行\ndf.tail(n): 查看后 n 行\ndf.info(): 查看数据摘要信息 (重要)\ndf.describe(): 查看数值列统计信息\ndf.shape, df.columns, df.index, df.dtypes: 查看数据结构信息\n\n\n\n\n\n2. 数据清洗\n\n2.1 缺失值处理\n\n\n\n\n\n\n缺失值检测与处理\n\n\n\n\n2.1.1 检测缺失值\n\ndf.isnull()\ndf.sum()\ndf.isnull().sum()\nmissingno 库 (可视化)\n\n\n\n\n缺失值分布矩阵图\n\n\n\n\n\n缺失值相关性热力图\n\n\n\n\n2.1.2 缺失值处理策略\n\n删除法: df.dropna() (慎用)\n填充法: df.fillna(value)\n\n固定值填充\n均值/中位数/众数填充 (使用 SimpleImputer)\n\n\n\n\n\nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\n\n# 创建包含缺失值的 DataFrame\ndata = {'数值特征': [1, 2, np.nan, 4, 5, np.nan],\n        '类别特征': ['A', 'B', 'C', np.nan, 'A', 'B']}\ndf = pd.DataFrame(data)\nprint(\"原始数据:\\n\", df)\n\n# 均值填充数值特征\nmean_imputer = SimpleImputer(strategy='mean')\ndf['数值特征_mean_填充'] = mean_imputer.fit_transform(df[['数值特征']])\nprint(\"\\n均值填充后:\\n\", df)\n\n# 中位数填充数值特征\nmedian_imputer = SimpleImputer(strategy='median')\ndf['数值特征_median_填充'] = median_imputer.fit_transform(df[['数值特征']])\nprint(\"\\n中位数填充后:\\n\", df)\n\n# 众数填充类别特征 (或数值特征)\nmode_imputer = SimpleImputer(strategy='most_frequent')\ndf['类别特征_mode_填充'] = mode_imputer.fit_transform(df[['类别特征']])\nprint(\"\\n众数填充后:\\n\", df)\n\n\n\n\n\n\n其他缺失值处理方法\n\n\n\n\n插值法: df.interpolate()\n高级方法: 模型预测 (本课程不深入)\n\n选择合适的缺失值处理策略需要根据具体情况和业务理解。\n\n\n\n\n2.2 异常值处理\n\n\n\n\n\n\n异常值检测与处理方法\n\n\n\n\n2.2.1 异常值检测方法\n\n箱线图 (Boxplot)\nZ-score\nIQR (四分位距)\n其他方法 (聚类, LOF 等)\n\n\n\n\n箱线图异常值检测示例\n\n\n\n\n2.2.2 异常值处理策略\n\n删除法 (慎用)\n替换法\n视为缺失值\n不处理\n\n注意: 异常值处理同样需要根据具体情况和业务理解。\n\n\n\n\n\n\n3. 特征工程基础\n\n3.1 数值特征处理\n\n\n\n\n\n\n特征缩放\n\n\n\n特征缩放 (Feature Scaling): 将数值特征缩放到一定范围，避免特征量纲不一致和数值过大/过小带来的问题。\n\n标准化 (Standardization):\n\n将特征缩放到均值为 0，标准差为 1 的分布\n公式：x_scaled = (x - mean) / std\nsklearn.preprocessing.StandardScaler\n\n\n\n\n\n特征缩放效果对比\n\n\n\n\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\n# 创建示例数据\ndata_scaling = {'数值特征': np.array([[10], [20], [30], [40], [50]])}\ndf_scaling = pd.DataFrame(data_scaling)\nprint(\"原始数据:\\n\", df_scaling)\n\n# 使用 StandardScaler 进行标准化\nscaler = StandardScaler()\ndf_scaling['数值特征_标准化'] = scaler.fit_transform(df_scaling[['数值特征']])\nprint(\"\\nStandardScaler 标准化后:\\n\", df_scaling)\n\n\n\n\n\n\n标准化使用场景\n\n\n\n标准化适用于大多数机器学习算法，特别是： - 基于距离度量的算法 (例如 KNN, 聚类) - 梯度下降算法 (例如 线性回归, 逻辑回归, 神经网络)\n标准化后，数据分布的形态可能会发生改变，但不会改变数据的相对大小关系。\n\n\n\n\n\n\n\n\n归一化\n\n\n\n归一化 (Normalization): - 将特征缩放到 0 和 1 之间 (也称 Min-Max 缩放) - 公式：x_scaled = (x - min) / (max - min) - sklearn.preprocessing.MinMaxScaler\n\n\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\n\n# 继续使用上面的示例数据\ndf_scaling_minmax = pd.DataFrame(data_scaling)\nprint(\"原始数据:\\n\", df_scaling_minmax)\n\n# 使用 MinMaxScaler 进行归一化\nminmax_scaler = MinMaxScaler()\ndf_scaling_minmax['数值特征_归一化'] = minmax_scaler.fit_transform(df_scaling_minmax[['数值特征']])\nprint(\"\\nMinMaxScaler 归一化后:\\n\", df_scaling_minmax)\n\n\n\n\n\n\n归一化使用场景\n\n\n\n\n归一化通常用于对数值范围敏感的算法 (例如 神经网络)\n归一化后，数据会被压缩到 [0, 1] 区间\n数据分布的形态会发生改变\n如果数据中存在 outliers，MinMaxScaler 对 outliers 比较敏感\n\n\n\n\n\n\n\n\n\n其他缩放方法\n\n\n\n\nRobustScaler: 使用 IQR (四分位距) 进行缩放，对 outliers 鲁棒\nMaxAbsScaler: 将特征缩放到 [-1, 1] 区间，适用于稀疏数据\n\n选择合适的缩放方法需要根据数据分布和模型特点。\n\n\n\n\n\n\n\n\n特征转换\n\n\n\n特征转换 (Feature Transformation): 对数值特征进行函数变换，使其更符合模型假设或业务需求。\n\n多项式特征:\n\n通过多项式扩展，增加特征的非线性\nsklearn.preprocessing.PolynomialFeatures\n\n\n\n\n\n多项式特征转换示例\n\n\n\n\nimport pandas as pd\nfrom sklearn.preprocessing import PolynomialFeatures\nimport numpy as np\n\n# 创建示例数据\ndata_poly = {'数值特征': np.array([[1], [2], [3], [4]])}\ndf_poly = pd.DataFrame(data_poly)\nprint(\"原始数据:\\n\", df_poly)\n\n# 使用 PolynomialFeatures 创建 2 次多项式特征\npoly = PolynomialFeatures(degree=2, include_bias=False)\npoly_features = poly.fit_transform(df_poly[['数值特征']])\npoly_feature_names = poly.get_feature_names_out(['数值特征'])\ndf_poly_encoded = pd.DataFrame(poly_features, columns=poly_feature_names)\nprint(\"\\nPolynomialFeatures 2 次多项式特征:\\n\", df_poly_encoded)\n\n\n\n\n\n\n多项式特征注意事项\n\n\n\n\n多项式特征可以捕捉特征之间的非线性关系\n适用于线性模型处理非线性数据\n但多项式特征会增加特征维度\n高次多项式容易过拟合\n\n\n\n\n\n\n\n\n\n对数变换\n\n\n\n对数变换: 对特征取对数，压缩数据范围，平滑数据分布，处理长尾分布。\n\nnumpy.log, numpy.log1p (推荐使用 log1p，避免 log(0) 错误)\n\n\n\n\n对数变换效果对比\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\n# 创建示例数据 (模拟长尾分布)\nnp.random.seed(42)\ndata_log = {'数值特征': np.exp(np.random.normal(0, 1, 100))}\ndf_log = pd.DataFrame(data_log)\n\n# 绘制原始数据直方图\ndf_log['数值特征'].hist(bins=50)\nplt.title('原始数据直方图 (长尾分布)')\nplt.show()\n\n# 进行对数变换\ndf_log['数值特征_对数变换'] = np.log1p(df_log['数值特征'])\n# 绘制对数变换后数据直方图\ndf_log['数值特征_对数变换'].hist(bins=50)\nplt.title('对数变换后数据直方图')\nplt.show()\n\n\n\n\n\n\n对数变换使用场景\n\n\n\n\n适用于处理长尾分布数据，例如用户行为数据、商品销量数据等\n对数变换可以减小数据的偏度，使其更接近正态分布\n有利于一些模型的训练\n\n\n\n\n\n\n\n\n\n幂变换\n\n\n\n幂变换: 对特征进行幂运算，调整数据分布形态。\n\nBox-Cox 变换, Yeo-Johnson 变换: 更通用的幂变换，可以处理不同分布的数据\nsklearn.preprocessing.PowerTransformer\n\n选择合适的特征转换方法需要根据业务理解和数据探索选择。\n\n\n\n\n3.2 类别特征处理\n\n\n\n\n\n\n独热编码\n\n\n\n独热编码 (One-Hot Encoding):\n\n将类别特征转换为多个二元特征\n每个类别对应一列，存在该类别则为 1，否则为 0\n适用于类别之间没有顺序关系的情况 (例如：颜色、城市)\npandas.get_dummies, sklearn.preprocessing.OneHotEncoder\n\n\n\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\n\n# 创建类别特征数据\ndata_onehot = {'颜色': ['红', '绿', '蓝', '红', '绿'],\n               '城市': ['北京', '上海', '广州', '北京', '深圳']}\ndf_onehot = pd.DataFrame(data_onehot)\nprint(\"原始数据:\\n\", df_onehot)\n\n# 使用 pandas.get_dummies 进行独热编码\ndf_onehot_dummies = pd.get_dummies(df_onehot, columns=['颜色', '城市'])\nprint(\"\\npandas.get_dummies 独热编码后:\\n\", df_onehot_dummies)\n\n# 使用 OneHotEncoder 进行独热编码\nencoder = OneHotEncoder(sparse_output=False)\nonehot_features = encoder.fit_transform(df_onehot[['颜色', '城市']])\nfeature_names = encoder.get_feature_names_out(['颜色', '城市'])\ndf_onehot_encoded = pd.DataFrame(onehot_features, columns=feature_names)\nprint(\"\\nOneHotEncoder 独热编码后:\\n\", df_onehot_encoded)\n\n\n\n\n\n\n独热编码注意事项\n\n\n\n处理高基数类别特征 (类别数量过多) 可能导致维度灾难。\n\n\n\n\n\n\n\n\n标签编码\n\n\n\n标签编码 (Label Encoding):\n\n将类别特征转换为数值标签，每个类别对应一个整数\n适用于类别之间有顺序关系的情况 (例如：学历、等级)\nsklearn.preprocessing.LabelEncoder\n\n\n\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\n# 创建类别特征数据\ndata_label = {'学历': ['本科', '硕士', '博士', '本科', '硕士']}\ndf_label = pd.DataFrame(data_label)\nprint(\"原始数据:\\n\", df_label)\n\n# 使用 LabelEncoder 进行标签编码\nlabel_encoder = LabelEncoder()\ndf_label['学历_标签编码'] = label_encoder.fit_transform(df_label['学历'])\nprint(\"\\nLabelEncoder 标签编码后:\\n\", df_label)\n\n# 查看类别标签映射关系\nlabel_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\nprint(\"\\n类别标签映射关系:\\n\", label_mapping)\n\n\n\n\n\n\n标签编码注意事项\n\n\n\n标签编码可能会引入类别之间的顺序关系，如果类别没有实际顺序意义，可能不适用。\n\n\n\n\n\n\n\n\n其他类别特征编码方法\n\n\n\n\n顺序编码 (Ordinal Encoding): 类似于标签编码，但可以自定义类别顺序\n\n适用于类别之间有明确顺序关系的情况\nsklearn.preprocessing.OrdinalEncoder\n\n二进制编码 (Binary Encoding)\n效应编码 (Effect Encoding)\n目标编码 (Target Encoding)\n\n\n\n\n\n\n4. Scikit-learn 数据预处理和特征工程常用模块\n\n\n\n\n\n\n常用模块概览\n\n\n\n\nsklearn.preprocessing: 提供了各种数据预处理和特征工程的函数和类\n\nStandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler (特征缩放)\nPolynomialFeatures, PowerTransformer (特征转换)\nOneHotEncoder, LabelEncoder, OrdinalEncoder (类别特征编码)\nSimpleImputer (缺失值填充)\n\nsklearn.impute: 提供了缺失值填充的类\n\nSimpleImputer, KNNImputer (KNN 填充)\nIterativeImputer (迭代填充)\n\nsklearn.feature_selection: 提供了特征选择的类 (下周讲解)\n\n\n\n\n\n5. 小组项目一：电商用户行为数据探索与预处理\n\n\n\n\n\n\n项目概述\n\n\n\n项目背景: 电商平台积累了大量的用户行为数据，例如用户的点击、浏览、购买、加购、收藏等行为。通过分析这些数据，可以了解用户兴趣偏好、购买行为模式，为个性化推荐、精准营销、用户增长等提供数据支持。\n\n\n\n\n\n\n\n\n数据集说明\n\n\n\n\n鼓励学生小组自主选择数据集，建议选择电商或零售行业的用户行为数据集\n数据集应包含用户行为数据，数据量适中，特征维度不低于10个\n鼓励各小组选择不同的数据集，以提高项目实践的多样性\n\n示例数据集：阿里巴巴天池 - 淘宝用户行为数据\n\n包含 2000 万条淘宝用户的行为记录\n字段：用户ID、商品ID、商品类目ID、行为类型、时间戳\n数据已简化处理，便于教学使用\n\n\n\n\n\n\n\n\n\n项目任务\n\n\n\n\n数据探索:\n\n加载数据集，查看数据基本信息\n分析用户行为类型分布、时间分布等\n可视化用户行为数据\n\n数据清洗:\n\n处理缺失值\n检测和处理异常值\n根据数据探索结果进行必要的清洗操作\n\n特征工程:\n\n构建用户特征和商品特征\n尝试构建交叉特征或衍生特征\n对特征进行必要的预处理\n\n\n\n\n\n\n\n\n\n\n提交要求\n\n\n\n\n预处理后的数据集 (CSV 格式)\nPython 代码 (Jupyter Notebook 或 Python 脚本)\n\n包含数据探索、清洗和特征工程的完整代码\n代码需要有清晰的注释\n\n小组分工说明 (可选)\n\n评分标准: 数据预处理的完整性、代码的规范性、数据探索的深入程度、特征工程的合理性",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>第二周：数据预处理与特征工程基础</span>"
    ]
  },
  {
    "objectID": "week2.html#第二次课小组项目一实践",
    "href": "week2.html#第二次课小组项目一实践",
    "title": "第二周：数据预处理与特征工程基础",
    "section": "第二次课：小组项目一实践",
    "text": "第二次课：小组项目一实践\n\n\n\n\n\n\n课程安排\n\n\n\n\n学生分组进行小组项目一\n教师巡回指导，解答问题\n完成小组项目一代码初稿\n\n\n\n\n\n\n\n\n\n课后作业\n\n\n\n\n继续完善小组项目一的数据预处理代码\n课前提交预处理结果\n预习下周课程：特征选择与降维\n\n\n\n\n\n\n\n\n\n相关资源\n\n\n\n\nPandas 官方文档\nScikit-learn 官方文档\nScikit-learn Preprocessing 模块文档\nScikit-learn Impute 模块文档\nPython Data Science Handbook\nHands-On Machine Learning with Scikit-Learn, Keras & TensorFlow\n阿里巴巴天池 - 淘宝用户行为数据",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>第二周：数据预处理与特征工程基础</span>"
    ]
  },
  {
    "objectID": "week3.html",
    "href": "week3.html",
    "title": "第三周：分类算法基础 (一) - 逻辑回归与支持向量机",
    "section": "",
    "text": "第一次课：分类算法 (一) - 逻辑回归与支持向量机 (SVM)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>第三周：分类算法基础 (一) - 逻辑回归与支持向量机</span>"
    ]
  },
  {
    "objectID": "week3.html#第一次课分类算法-一---逻辑回归与支持向量机-svm",
    "href": "week3.html#第一次课分类算法-一---逻辑回归与支持向量机-svm",
    "title": "第三周：分类算法基础 (一) - 逻辑回归与支持向量机",
    "section": "",
    "text": "1. 分类问题概述\n\n\n\n\n\n\n分类问题的定义与应用\n\n\n\n分类问题的定义： 预测样本属于哪个类别 (离散值)。\n分类问题的应用场景：\n\n客户分类： 根据客户特征划分客户群体\n风险预测： 预测用户是否会违约、交易是否为欺诈\n垃圾邮件识别： 判断邮件是否为垃圾邮件\n图像识别： 识别图像中的物体类别\n文本分类： 将文本分为不同的类别\n\n\n\n\n\n\n\n\n\n分类问题的类型\n\n\n\n\n二分类： 类别只有两种 (是/否, 正/负, 0/1)\n多分类： 类别多于两种 (例如：新闻分类、图像识别)\n\n\n\n\n\n2. 逻辑回归 (Logistic Regression)\n\n\n\n\n\n\n逻辑回归的原理\n\n\n\n\n线性模型 + Sigmoid 函数： 逻辑回归本质上是一个线性模型，使用 Sigmoid 函数将输出映射到 (0, 1) 区间\nSigmoid 函数： \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\)\n\n\n\n\nSigmoid函数示意图\n\n\n\n决策边界： 通过设定阈值 (通常为 0.5) 将概率值转换为类别标签\n\n\n\n\n逻辑回归决策边界示例\n\n\n\n\n\n\n\n\n\n\n逻辑回归的公式\n\n\n\n\n线性模型： \\(z = w^T x + b\\)\n概率预测： \\(p = \\sigma(z) = \\frac{1}{1 + e^{-(w^T x + b)}}\\)\n损失函数： 使用交叉熵损失函数进行模型训练\n\n\n\n\n\n\n\n\n\nScikit-learn 实现逻辑回归\n\n\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# 加载电商用户行为数据集 (假设已预处理完成)\ndata = pd.read_csv('ecommerce_user_behavior_preprocessed.csv')\n\n# 假设 'label' 列为分类目标变量，其他列为特征变量\nX = data.drop('label', axis=1)\ny = data['label']\n\n# 划分训练集和测试集\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# 创建 LogisticRegression 模型\nlogreg_model = LogisticRegression(max_iter=1000)\n\n# 训练模型\nlogreg_model.fit(X_train, y_train)\n\n# 预测测试集\ny_pred = logreg_model.predict(X_test)\n\n# 评估模型\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"逻辑回归模型准确率: {accuracy:.4f}\")\nprint(\"\\n分类报告:\\n\", classification_report(y_test, y_pred))\n\n\n\n\n\n\n\n\n逻辑回归的优缺点\n\n\n\n优点：\n\n模型简单，易于理解和实现\n训练速度快\n可解释性强，可以输出特征的权重系数\n\n缺点：\n\n只能处理线性可分或近似线性可分的数据\n对特征之间的多重共线性比较敏感\n容易欠拟合\n\n\n\n\n\n3. 支持向量机 (Support Vector Machine, SVM)\n\n\n\n\n\n\nSVM 的原理\n\n\n\n\n核心思想： 努力找到一条线 (或者超平面)，尽可能完美地把两类数据分隔开来。\n最大化间隔 (Margin Maximization)： SVM 不仅要分隔数据，还要让分隔线距离最近的数据点越远越好。这个”距离”就是间隔 (Margin)。\n支持向量 (Support Vectors)： 在分隔线边缘，对分隔线位置起决定性作用的”关键”数据点。SVM 的名字就来源于它们。\n\n\n\n\nSVM最大间隔和支持向量示意图\n\n\n简单来说，SVM 就像是在两种不同类型的物品之间划出一条尽可能宽的隔离带。隔离带越宽，模型就越稳健。\n\n\n\n\n\n\n\n\n核函数 (Kernel Functions) - “核技巧” 的精髓\n\n\n\n\n为什么需要核函数？ 当数据不是线性可分的时候，线性 SVM 就无法很好地工作。核函数 是一种强大的工具，它可以将数据隐式地映射到更高维的空间，使得原本线性不可分的数据在新空间中变得线性可分。这个过程被称为 “核技巧 (Kernel Trick)”。\n\n\n\n\n不同核函数的效果对比\n\n\n\n“核技巧” 的核心思想： 与其显式地计算高维空间中的坐标，不如直接计算高维空间中两个向量的点积。而核函数就是用来高效计算这个点积的。\n常用核函数： (以下核函数都实现了 “核技巧”，可以用于非线性 SVM)\n\n线性核 (Linear Kernel)： \\(K(x_i, x_j) = x_i^T x_j\\)。 虽然名字叫 “核函数”，但线性核并没有升维，它等价于线性 SVM，直接在原空间计算点积。适用于线性可分的数据。\n多项式核 (Polynomial Kernel)： \\(K(x_i, x_j) = (\\gamma (x_i^T x_j) + r)^d\\)。 通过多项式的方式升维，可以处理一些非线性数据。\nRBF 核 (Radial Basis Function Kernel)： \\(K(x_i, x_j) = \\exp(-\\gamma ||x_i - x_j||^2)\\)。 也叫高斯核，是最常用的核函数之一。它可以将数据映射到无限维空间，非常灵活，能处理各种复杂形状的数据分布。\nSigmoid 核 (Sigmoid Kernel)： \\(K(x_i, x_j) = \\tanh(\\gamma (x_i^T x_j) + r)\\)。 类似于神经网络中的 Sigmoid 激活函数，也常用于非线性分类。\n\n\n选择核函数的简单原则：\n\n对于线性可分的数据，线性核是首选，因为它最简单高效。\n对于非线性可分的数据，RBF 核 通常是默认的 “万金油”，因为它适用性广，效果通常不错。\n可以尝试多项式核和 Sigmoid 核，但通常 RBF 核更常用且效果更好。\n实际应用中，可以通过交叉验证等方法来选择最佳的核函数和参数。\n\n总结： 核函数是 SVM 处理非线性数据的关键。通过 “核技巧”，我们可以在不显式进行高维映射的情况下，利用核函数高效地完成非线性分类。当你使用非线性核函数 (如 RBF 核) 时，SVM 实际上是在高维空间中寻找最优超平面，但所有的计算都可以在原始特征空间中通过核函数完成。\n\n\n\n\n\n\n\n\nScikit-learn 实现 SVM\n\n\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# 加载电商用户行为数据集 (假设已预处理完成)\ndata = pd.read_csv('ecommerce_user_behavior_preprocessed.csv')\n\n# 假设 'label' 列为分类目标变量，其他列为特征变量\nX = data.drop('label', axis=1)\ny = data['label']\n\n# 划分训练集和测试集\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# 创建 SVM 模型 (使用 RBF 核)\nsvm_model = SVC(kernel='rbf', C=1.0, gamma='scale')\n\n# 训练模型\nsvm_model.fit(X_train, y_train)\n\n# 预测测试集\ny_pred = svm_model.predict(X_test)\n\n# 评估模型\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"SVM 模型准确率: {accuracy:.4f}\")\nprint(\"\\n分类报告:\\n\", classification_report(y_test, y_pred))\n\n\n\n\n\n\n\n\nSVM 的优缺点\n\n\n\n优点：\n\n在高维空间和复杂数据集上表现优秀\n泛化能力强，不容易过拟合\n可以使用核函数处理非线性数据\n\n缺点：\n\n训练速度较慢，特别是对于大规模数据集\n参数调优比较复杂\n模型可解释性较差\n对缺失值和特征缩放敏感\n\n\n\n\n\n4. 模型评估指标\n\n\n\n\n\n\n评估指标概述\n\n\n\n\n混淆矩阵： 展示分类模型预测结果的矩阵 (TP, TN, FP, FN)\n准确率： (TP + TN) / (TP + TN + FP + FN)\n精确率： TP / (TP + FP)\n召回率： TP / (TP + FN)\nF1-score： 2 * (Precision * Recall) / (Precision + Recall)\n\n\n\n\n\n\n\n\n\nAUC-ROC\n\n\n\n\nROC 曲线： 以假正例率为横轴，真正例率为纵轴的曲线\nAUC 值： ROC 曲线下的面积，值越大表示模型性能越好\n\n\n\n\nROC曲线示例\n\n\n\n\n\n\n5. 模型选择与调优\n\n\n\n\n\n\n常用方法\n\n\n\n\n交叉验证： K 折交叉验证，用于更可靠地评估模型性能\n网格搜索： 遍历参数组合，选择性能最佳的参数",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>第三周：分类算法基础 (一) - 逻辑回归与支持向量机</span>"
    ]
  },
  {
    "objectID": "week3.html#小组项目一电商用户行为分类模型构建",
    "href": "week3.html#小组项目一电商用户行为分类模型构建",
    "title": "第三周：分类算法基础 (一) - 逻辑回归与支持向量机",
    "section": "小组项目一：电商用户行为分类模型构建",
    "text": "小组项目一：电商用户行为分类模型构建\n\n\n\n\n\n\n项目概述\n\n\n\n项目目标： 基于预处理后的电商用户行为数据，构建用户分类模型。\n项目任务：\n\n数据准备： 使用预处理后的数据集\n模型选择： 选择逻辑回归或 SVM 算法\n模型训练： 使用训练集训练模型\n模型评估： 计算并分析评估指标\n模型调优： 使用交叉验证和网格搜索\n撰写报告： 记录实验过程和结果\n\n\n\n\n\n\n\n\n\n提交要求\n\n\n\n\nPython 代码 (含完整注释)\n实验结果报告 (包括项目背景、步骤、分析等)\n\n评分标准： 模型选择合理性、代码规范性、评估完整性、分析深入程度",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>第三周：分类算法基础 (一) - 逻辑回归与支持向量机</span>"
    ]
  },
  {
    "objectID": "week3.html#第二次课小组项目一实践",
    "href": "week3.html#第二次课小组项目一实践",
    "title": "第三周：分类算法基础 (一) - 逻辑回归与支持向量机",
    "section": "第二次课：小组项目一实践",
    "text": "第二次课：小组项目一实践\n\n\n\n\n\n\n课程安排\n\n\n\n\n学生分组进行项目实践\n教师巡回指导，解答问题\n完成项目代码和报告初稿\n\n\n\n\n\n\n\n\n\n课后作业\n\n\n\n\n完善项目代码和实验报告\n课前提交完整成果\n预习下周课程：决策树与集成学习\n\n\n\n\n\n\n\n\n\n相关资源\n\n\n\n\nScikit-learn 官方文档 - 逻辑回归\nScikit-learn 官方文档 - SVM\nScikit-learn 官方文档 - 模型评估\nPython Data Science Handbook\nHands-On Machine Learning with Scikit-Learn",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>第三周：分类算法基础 (一) - 逻辑回归与支持向量机</span>"
    ]
  },
  {
    "objectID": "week4.html",
    "href": "week4.html",
    "title": "第四周：分类算法基础 (二) - 决策树与集成学习 (随机森林)",
    "section": "",
    "text": "第一次课：分类算法 (二) - 决策树与集成学习 (随机森林)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>第四周：分类算法基础 (二) - 决策树与集成学习 (随机森林)</span>"
    ]
  },
  {
    "objectID": "week4.html#第一次课分类算法-二---决策树与集成学习-随机森林",
    "href": "week4.html#第一次课分类算法-二---决策树与集成学习-随机森林",
    "title": "第四周：分类算法基础 (二) - 决策树与集成学习 (随机森林)",
    "section": "",
    "text": "本周学习目标\n\n\n\n\n掌握决策树算法的原理、信息增益/基尼系数的计算方法和 Scikit-learn 实现。\n理解决策树的优缺点，并能够进行可视化展示。\n掌握集成学习 Bagging 方法和随机森林算法的原理和 Scikit-learn 实现。\n理解随机森林的优缺点和特征重要性的概念。\n回顾分类模型评估指标，并深入理解交叉验证和网格搜索的模型选择与调优方法。\n能够根据不同的应用场景选择合适的分类模型评估指标。\n使用 Scikit-learn 构建、评估和调优决策树和随机森林分类模型。\n比较不同分类模型在电商用户行为数据集上的性能。\n使用 AI 辅助编程工具完成模型训练、评估和调优代码。\n使用随机森林算法优化小组项目一的电商用户行为分类模型。\n\n\n\n\n内容概要\n\n决策树 (Decision Tree)\n\n\n\n\n\n\n\n决策树的原理\n\n\n\n\n树形结构: 决策树是一种树形结构的分类模型，每个内部节点表示一个特征的测试，每个分支代表一个测试输出，每个叶节点代表一个类别。\n\n\n\n\n决策树示例\n\n\n\n决策过程: 从根节点开始，根据样本在每个节点上的特征取值，递归地将样本分到不同的分支，直到到达叶节点，叶节点对应的类别即为预测结果。\n非线性模型: 决策树可以处理非线性数据，能够进行复杂的分类。\n\n\n\n\n\n\n\n\n\n特征选择 (分裂准则)\n\n\n\n\n特征选择 (分裂准则): 决策树算法的关键在于如何选择最优的特征进行节点分裂。 “最优”特征是指能够最大程度提高数据纯度的特征。 常用的分裂准则包括：\n\n信息增益 (Information Gain): 基于信息熵 (Entropy) 的分裂准则。 信息熵 衡量了数据集的混乱程度，熵越高，数据越”乱”，纯度越低。 信息增益 表示使用某个特征进行分裂后，数据集信息熵减少的程度。 选择信息增益最大的特征进行分裂，意味着使用该特征分裂后，数据变得更有序，纯度更高。\n\n信息熵 (Entropy) 公式: \\(Entropy(S) = - \\sum_{i=1}^{c} p_i \\log_2(p_i)\\)，其中 \\(S\\) 是数据集， \\(p_i\\) 是类别 \\(i\\) 在数据集 \\(S\\) 中所占的比例，\\(c\\) 是类别数量。 熵值越大，数据集纯度越低。\n\n\n\n\n信息熵与基尼系数对比\n\n\n\n信息增益 (Information Gain) 公式: \\(Gain(S, A) = Entropy(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} Entropy(S_v)\\)，其中 \\(A\\) 是特征， \\(Values(A)\\) 是特征 \\(A\\) 的取值集合， \\(S_v\\) 是特征 \\(A\\) 取值为 \\(v\\) 的子数据集。 信息增益越大，使用特征 \\(A\\) 分裂数据集 \\(S\\) 带来的纯度提升越大。\n\n\n\n\n\n\n\nNote\n\n\n\n信息增益直观理解: 信息增益就像是”提纯”数据的能力。 选择信息增益大的特征，就像是用更有效的”筛子”来筛选数据，使得相同类别的样本更集中在一起，不同类别的样本更容易区分开。\n\n\n基尼系数 (Gini Impurity): 基于基尼指数 (Gini Index) 的分裂准则。 基尼指数 衡量了数据集的不纯度，基尼指数越小，数据纯度越高。 选择使得基尼系数下降最快的特征进行分裂，意味着使用该特征分裂后，数据的不纯度降低得最多，纯度提升最大。\n\n基尼指数 (Gini Index) 公式: \\(Gini(S) = 1 - \\sum_{i=1}^{c} p_i^2\\)，其中 \\(S\\) 是数据集， \\(p_i\\) 是类别 \\(i\\) 在数据集 \\(S\\) 中所占的比例，\\(c\\) 是类别数量。 基尼指数越大，数据集纯度越低。\n基尼系数增益 (Gini Gain): 类似于信息增益，基尼系数增益表示使用某个特征分裂后，基尼系数下降的程度。 决策树算法会选择基尼系数增益最大的特征进行分裂。\n\n\n\n\n\n\n\nNote\n\n\n\n基尼系数直观理解: 基尼系数可以理解为衡量数据”杂乱”程度的指标。 选择基尼系数下降最快的特征，就像是用最有效的”梳子”来梳理数据，使得数据更有条理，相同类别的样本更”抱团”，不同类别的样本更”分离”。\n\n\n\n\n\n\n\n\n\n\n\n\nScikit-learn 实现决策树\n\n\n\n\nScikit-learn 实现决策树: 使用 sklearn.tree.DecisionTreeClassifier 类。\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.tree import export_graphviz\nimport graphviz\n\n# 加载电商用户行为数据集 (假设已预处理完成)\ndata = pd.read_csv('ecommerce_user_behavior_preprocessed.csv') # 请替换为您的数据集路径\n\n# 假设 'label' 列为分类目标变量，其他列为特征变量\nX = data.drop('label', axis=1)\ny = data['label']\n\n# 划分训练集和测试集\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# 创建 DecisionTreeClassifier 模型\ndt_model = DecisionTreeClassifier(criterion='gini', max_depth=5) # 可以调整 criterion, max_depth 等参数\n\n# 训练模型\ndt_model.fit(X_train, y_train)\n\n# 预测测试集\ny_pred = dt_model.predict(X_test)\n\n# 评估模型\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"决策树模型准确率: {accuracy:.4f}\")\nprint(\"\\n分类报告:\\n\", classification_report(y_test, y_pred))\n\n# 可视化决策树 (需要安装 graphviz)\ndot_data = export_graphviz(dt_model, out_file=None,\n                           feature_names=X_train.columns,\n                           class_names=[str(c) for c in dt_model.classes_],\n                           filled=True, rounded=True,\n                           special_characters=True)\ngraph = graphviz.Source(dot_data)\ngraph.render(\"decision_tree\") # 保存为 decision_tree.pdf\ngraph # 在 notebook 中显示\n\n\n\n\n\n\n\n\n决策树的优缺点\n\n\n\n\n决策树的优点:\n\n易于理解和解释，树形结构可视化直观。\n可以处理类别型和数值型数据。\n对数据预处理要求不高，不需要特征缩放。\n能够处理非线性关系。\n\n决策树的缺点:\n\n容易过拟合，特别是当树的深度过大时。\n对数据中的噪声和异常值比较敏感。\n决策树模型不稳定，数据的小幅变动可能导致树结构发生很大变化。\n\n\n\n\n\n集成学习 - Bagging 与随机森林 (Random Forest)\n\n\n\n\n\n\n\n集成学习基础\n\n\n\n\n集成学习 (Ensemble Learning): 将多个”弱学习器” (weak learner, 例如简单的决策树) 集成起来，构建一个”强学习器” (strong learner)，提高模型的泛化能力和鲁棒性。 “众人拾柴火焰高”，集成学习的核心思想就是 “集思广益”，通过多个模型的共同决策来提高整体性能。\n\n\n\n\n\n\n\n\n\nBagging 方法\n\n\n\n\nBagging (Bootstrap Aggregating): 一种常用的集成学习方法，名字 “Bagging” 来自于 “Bootstrap Aggregating” (自助抽样聚合)。 Bagging 的核心思想是 降低模型的方差 (Variance)，提高模型的稳定性。\n\n自助采样 (Bootstrap Sampling): 从原始数据集中 有放回地随机抽取 多个子数据集 (bootstrap sample)。 “有放回” 意味着每次抽取的样本，下次抽取时仍然可能被抽到。 这样，每个子数据集都和原始数据集大小相近，但样本组成略有不同。\n基学习器训练: 基于每个子数据集，训练一个 独立的基学习器 (例如决策树)。\n集成 (Aggregating): 将所有基学习器的预测结果进行 集成。 对于分类问题，通常使用 投票法 (Voting)，即选择得票最多的类别作为最终预测结果；对于回归问题，通常使用 平均法 (Averaging)，即对所有基学习器的预测值取平均。\n\n\n\n\nBagging方法示意图\n\n\n\n\n\n\n\n\nNote\n\n\n\nBagging 原理的形象理解: Bagging 就像是 “三个臭皮匠顶个诸葛亮”。 每个基学习器就像是一个 “臭皮匠”，可能模型能力有限，容易犯错 (高方差)。 但通过 Bagging 将多个 “臭皮匠” 的预测结果 “投票” 起来，就相当于进行了 “集体决策”，可以降低犯错的概率，得到更可靠、更稳定的预测结果，最终 “顶” 上一个 “诸葛亮” (低方差、高性能的强学习器)。\nBagging 降低方差的直观解释: 想象一下，你要预测明天的天气。 如果只问一个气象专家，预测结果可能比较依赖于该专家的个人经验，有一定的偶然性 (高方差)。 但如果同时问 10 个气象专家，并将他们的预测结果综合起来 (例如取平均或投票)，那么最终的预测结果就会更加稳定可靠，不容易受到个别专家预测失误的影响 (低方差)。 Bagging 的自助采样和集成过程，就类似于 “多咨询几个专家，综合决策” 的过程，可以有效降低模型的方差，提高模型的稳定性。\n\n\n\n\n\n\n\n\n\n\n\n随机森林算法\n\n\n\n\n随机森林 (Random Forest): Bagging 的一种变体，在 Bagging 的基础上，进一步引入了特征的随机选择。 随机森林以决策树为基学习器，并在决策树的训练过程中引入了 随机特征选择。 随机森林的 “随机性” 体现在两个方面：\n\n样本随机性 (Bagging 引入): 使用自助采样随机抽取子数据集。\n特征随机性 (随机森林特有): 在每个节点分裂时，随机选择一部分特征 (而不是考虑所有特征) 进行分裂特征的选择。 例如，如果共有 \\(M\\) 个特征，随机森林在每个节点分裂时，会随机选择 \\(m\\) 个特征 (\\(m &lt; M\\))，然后从这 \\(m\\) 个特征中选择最优的分裂特征。 通常 \\(m\\) 的取值建议为 \\(\\sqrt{M}\\)。\n\n\n\n\n随机森林特征随机选择示意图\n\n\n\n\n\n\n\n\nNote\n\n\n\n随机森林特征随机选择的重要性: 特征随机选择使得随机森林中的决策树更加 “多样化”，进一步降低了模型之间的相关性，使得随机森林的泛化能力更强，更不容易过拟合。\n特征随机选择的直观理解: 继续用 “气象专家预测天气” 的例子。 随机森林不仅咨询多个气象专家 (Bagging)，而且还要求每个气象专家在预测天气时，只允许参考一部分气象指标 (例如，专家 A 只能参考温度和湿度，专家 B 只能参考风速和气压，等等)。 这样，每个专家都只能 “片面” 地看问题，但多个 “片面” 的预测结果综合起来，反而可能得到更全面、更准确的预测。 特征随机选择的目的就是 限制单个决策树的能力，避免模型过度依赖于某些强特征，从而提高模型的整体泛化能力。\n\n\n\n\n\n\n\n\n随机森林的优缺点\n\n\n\n\n随机森林的优点:\n\n精度高，泛化能力强，不容易过拟合。 随机森林通过集成多个决策树，并引入样本随机性和特征随机性，有效降低了过拟合的风险。\n能够处理高维数据，不需要进行特征选择。 随机森林在特征选择时引入了随机性，降低了特征维度过高带来的影响。\n可以评估特征的重要性。 随机森林可以输出每个特征在模型训练过程中的重要性评分，用于特征选择和特征理解。\n对缺失值和异常值有一定的鲁棒性。\n易于并行化，训练速度快。\n\n随机森林的缺点:\n\n模型可解释性较差，相对于决策树，随机森林的模型结构更复杂，难以解释。\n当随机森林中的决策树数量非常大时，模型训练和预测的计算开销会比较大。\n\n\n\n\n\n\n\n\n\n\n\n\n\nScikit-learn 实现随机森林\n\n\n\n\nScikit-learn 实现随机森林: 使用 sklearn.ensemble.RandomForestClassifier 类。\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 加载电商用户行为数据集 (假设已预处理完成)\ndata = pd.read_csv('ecommerce_user_behavior_preprocessed.csv') # 请替换为您的数据集路径\n\n# 假设 'label' 列为分类目标变量，其他列为特征变量\nX = data.drop('label', axis=1)\ny = data['label']\n\n# 划分训练集和测试集\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# 创建 RandomForestClassifier 模型\nrf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42) # 可以调整 n_estimators, max_depth 等参数\n\n# 训练模型\nrf_model.fit(X_train, y_train)\n\n# 预测测试集\ny_pred = rf_model.predict(X_test)\n\n# 评估模型\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"随机森林模型准确率: {accuracy:.4f}\")\nprint(\"\\n分类报告:\\n\", classification_report(y_test, y_pred))\n\n# 特征重要性\nfeature_importances = pd.Series(rf_model.feature_importances_, index=X_train.columns).sort_values(ascending=False)\nplt.figure(figsize=(10, 6))\nsns.barplot(x=feature_importances, y=feature_importances.index)\nplt.title(\"随机森林特征重要性\")\nplt.show()\n\n![随机森林特征重要性示例](img/week4/feature_importance.png)\n\n\n\n模型评估与选择 (回顾与实践)\n\n\n\n\n\n\n\n模型评估指标\n\n\n\n\n模型评估指标回顾: 准确率、精确率、召回率、F1-score、AUC-ROC 等 (回顾第三周内容)。\n交叉验证 (Cross-Validation): K 折交叉验证的详细讲解和实践，使用 sklearn.model_selection.cross_val_score 和 sklearn.model_selection.KFold。\n\n\n\n\n5折交叉验证示意图\n\n\n\n网格搜索 (GridSearchCV): GridSearchCV 的详细讲解和实践，使用 sklearn.model_selection.GridSearchCV 进行模型参数调优。\n\n// ... existing code ...\n\n分类模型评估指标的选择和应用场景: 选择合适的评估指标，需要根据具体的业务目标和问题类型来决定。 不同的评估指标关注模型的不同方面，适用于不同的应用场景。\n\n准确率 (Accuracy): 最常用的评估指标之一， 表示模型预测正确的样本比例。 适用于类别分布均衡的分类问题。 例如，在手写数字识别、图像分类等问题中，如果每个类别的样本数量相差不大，可以使用准确率作为主要评估指标。 但当类别分布不均衡时，准确率可能会产生误导。 例如，如果在一个疾病预测问题中，99% 的样本都是健康人，模型如果将所有人都预测为健康，也能达到 99% 的准确率，但这显然不是一个好的模型。\n精确率 (Precision) 和 召回率 (Recall): 适用于类别分布不均衡的分类问题。 精确率关注 “预测为正例的样本中，有多少是真正的正例”， 召回率关注 “真正的正例样本中，有多少被模型预测为正例”。\n\n精确率的应用场景: 当更关注 “预测为正例的准确性” 时，例如，在垃圾邮件识别中，我们更关注 “被模型判断为垃圾邮件的邮件，有多少是真正的垃圾邮件”， 因为如果将正常邮件误判为垃圾邮件，可能会造成用户的重要信息丢失，误判的代价较高。 此时，我们希望提高精确率，降低误判率 (FP)。\n召回率的应用场景: 当更关注 “对正例的识别能力” 时，例如，在疾病诊断中，我们更关注 “真正的病人中，有多少被模型诊断出来”， 因为如果漏诊病人，可能会延误治疗，造成更严重的后果，漏诊的代价较高。 此时，我们希望提高召回率，降低漏诊率 (FN)。\n\nF1-score: 精确率和召回率的调和平均值，综合考虑了精确率和召回率。 适用于类别分布不均衡，且希望平衡精确率和召回率的场景。 例如，在欺诈交易检测、用户流失预测等问题中，我们既希望尽可能准确地识别出欺诈交易或潜在流失用户 (提高精确率)，也希望尽可能全面地覆盖所有欺诈交易或潜在流失用户 (提高召回率)，此时可以使用 F1-score 作为综合评估指标。\nAUC-ROC (Area Under the ROC Curve): 适用于二分类问题，特别是当需要权衡不同阈值下的模型性能时。 ROC 曲线描述了在不同阈值下，模型的真正例率 (TPR, 召回率) 和假正例率 (FPR) 之间的关系。 AUC 值是 ROC 曲线下的面积，AUC 值越大，模型性能越好。 AUC-ROC 关注的是模型对正负样本的排序能力，对类别分布不均衡的情况不敏感， 因此在类别不均衡问题中也经常使用。\n\n\n\n\n\n\nNote\n\n\n\n模型评估指标选择总结: 没有 “万能” 的评估指标，选择合适的评估指标需要根据具体的业务场景和问题目标来决定。 理解各种评估指标的含义和适用场景，才能更好地评估模型性能，并根据评估结果优化模型。\n\n\n\n\n\n\n\n\n实践环节\n\n\n\n\n\n\n实践任务\n\n\n\n\n使用 Scikit-learn 构建和评估决策树和随机森林分类模型。\n使用网格搜索和交叉验证进行模型调优。\n比较逻辑回归、SVM、决策树和随机森林在电商用户行为数据集上的性能。\n使用 AI 工具辅助完成模型训练、评估和调优代码。\n\n\n\n\n\n小组项目一：电商用户行为数据分类模型优化 (随机森林)\n\n\n\n\n\n\n项目要求\n\n\n\n\n项目目标: 使用随机森林算法，优化小组项目一的电商用户行为分类模型，并与之前使用的逻辑回归或 SVM 模型进行性能比较。\n项目任务 (小组完成):\n\n数据准备: 继续使用预处理后的电商用户行为数据集。\n模型选择: 选择随机森林算法构建分类模型。\n模型训练与调优: 使用训练集训练随机森林模型，并使用网格搜索和交叉验证进行模型参数调优，选择最佳模型参数。\n模型评估与比较: 使用测试集评估优化后的随机森林模型性能，计算并分析模型评估指标，并将随机森林模型的性能与之前小组项目一中使用的逻辑回归或 SVM 模型的性能进行比较。\n特征重要性分析: 分析随机森林模型的特征重要性，尝试解释模型预测结果。\n撰写实验报告: 在之前的实验报告基础上，补充决策树和随机森林模型的实验过程、模型参数、评估指标、结果分析和结论，重点突出随机森林模型的优化过程和性能提升。\n提交内容:\n\nPython 代码 (Jupyter Notebook 或 Python 脚本)，包含决策树和随机森林模型的训练、评估和调优代码，以及模型比较和特征重要性分析代码，代码需要有清晰的注释。\n完善的实验结果报告 (Markdown 或 PDF 格式)，包括项目背景、分类目标、数据集描述、模型选择 (包括逻辑回归、SVM、决策树和随机森林)、实验步骤、模型评估指标、结果分析、结论、特征重要性分析和小组分工说明。\n小组项目一：电商用户行为数据探索与预处理 提交 (本周课前，如果之前未提交)。\n\n\n评分标准: 模型选择和优化的合理性、代码的规范性、模型评估的完整性、结果分析的深入程度、实验报告的规范性、模型性能的提升程度。",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>第四周：分类算法基础 (二) - 决策树与集成学习 (随机森林)</span>"
    ]
  },
  {
    "objectID": "week4.html#第二次课小组项目一电商用户行为数据分类优化-模型优化与报告撰写",
    "href": "week4.html#第二次课小组项目一电商用户行为数据分类优化-模型优化与报告撰写",
    "title": "第四周：分类算法基础 (二) - 决策树与集成学习 (随机森林)",
    "section": "第二次课：小组项目一：电商用户行为数据分类优化 (模型优化与报告撰写)",
    "text": "第二次课：小组项目一：电商用户行为数据分类优化 (模型优化与报告撰写)\n\n内容概要\n\n\n\n\n\n\n课程安排\n\n\n\n\n学生分组进行小组项目一：电商用户行为数据分类优化 (随机森林)\n教师巡回指导，解答学生在模型优化和代码实现中遇到的问题。\n\n\n\n\n\n实践环节\n\n\n\n\n\n\n实践任务\n\n\n\n\n学生以小组为单位，使用 Python 和 AI 工具，优化随机森林分类模型。\n撰写小组项目一实验报告，包括数据预处理、模型构建、模型评估、结果分析和结论。\n\n\n\n\n\n课后作业\n\n\n\n\n\n\n作业要求\n\n\n\n\n继续完善小组项目一：电商用户行为数据分类优化 的模型代码和实验报告。\n课前提交小组项目一的完整代码和实验报告。\n预习下周课程内容：聚类算法 - K-Means 聚类。\n\n\n\n\n\n相关资源\n\n\n\n\n\n\n参考资料\n\n\n\n\nScikit-learn 官方文档 - 决策树: https://scikit-learn.org/stable/modules/tree.html\nScikit-learn 官方文档 - 随机森林: https://scikit-learn.org/stable/modules/ensemble.html#forests\nScikit-learn 官方文档 - 交叉验证: https://scikit-learn.org/stable/modules/cross_validation.html\nScikit-learn 官方文档 - 网格搜索: https://scikit-learn.org/stable/modules/grid_search.html\nGraphviz 官方网站: https://graphviz.org/ (用于决策树可视化)\n《Python Data Science Handbook》: jakevdp.github.io/PythonDataScienceHandbook/ (Chapter 5 - Machine Learning)\n《Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow》: www.oreilly.com/library/view/hands-on-machine-learning/9781098125973/ (Chapter 6 - Decision Trees, Chapter 7 - Ensemble Learning and Random Forests)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>第四周：分类算法基础 (二) - 决策树与集成学习 (随机森林)</span>"
    ]
  },
  {
    "objectID": "week5.html",
    "href": "week5.html",
    "title": "第五周：项目汇报与回归算法基础",
    "section": "",
    "text": "第一次课：项目一小组汇报",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>第五周：项目汇报与回归算法基础</span>"
    ]
  },
  {
    "objectID": "week5.html#第一次课项目一小组汇报",
    "href": "week5.html#第一次课项目一小组汇报",
    "title": "第五周：项目汇报与回归算法基础",
    "section": "",
    "text": "本周学习目标\n\n\n\n\n通过小组汇报，分享和学习电商用户行为数据分析与分类的项目经验\n掌握回归算法的基本原理和应用场景\n理解线性回归和多项式回归的区别与联系\n掌握回归模型的评估指标和方法\n学习正则化技术在回归中的应用\n能够使用scikit-learn实现和评估回归模型\n将学到的回归算法应用于实际的房价预测问题\n\n\n\n\n小组项目汇报\n\n\n\n\n\n\n项目一：电商用户行为数据分析与分类\n\n\n\n\n项目目标：对电商用户行为数据进行探索性分析和预处理，构建分类模型预测用户行为（如购买意愿、用户价值等）\n使用的算法：逻辑回归、SVM、决策树、随机森林\n评估指标：准确率、精确率、召回率、F1值、AUC-ROC等\n\n\n\n\n汇报流程\n\n小组汇报 (每组5-10分钟)\n\n项目背景与问题定义\n数据集介绍与预处理方法\n使用的分类算法与原因\n实验结果与模型比较\n结论与实际应用价值\n\n问答与讨论 (每组5分钟)\n\n教师点评与建议\n同学提问与交流",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>第五周：项目汇报与回归算法基础</span>"
    ]
  },
  {
    "objectID": "week5.html#第二次课回归算法基础---线性回归与多项式回归",
    "href": "week5.html#第二次课回归算法基础---线性回归与多项式回归",
    "title": "第五周：项目汇报与回归算法基础",
    "section": "第二次课：回归算法基础 - 线性回归与多项式回归",
    "text": "第二次课：回归算法基础 - 线性回归与多项式回归\n\n内容概要\n\n回归问题概述\n\n\n\n\n\n\n\n什么是回归问题？\n\n\n\n\n定义: 回归分析是一种预测分析，研究自变量（特征）与因变量（目标）之间的关系，目标是预测连续型数值。\n与分类的区别: 分类预测的是离散类别，回归预测的是连续数值。\n应用场景:\n\n房价预测\n销量预测\n股票价格预测\n温度预测\n能源消耗估计\n\n\n\n\n\n线性回归\n\n\n\n\n\n\n\n线性回归原理\n\n\n\n\n基本模型: \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n + \\epsilon\\)\n\n\\(y\\) 是目标变量\n\\(x_1, x_2, ..., x_n\\) 是特征变量\n\\(\\beta_0, \\beta_1, ..., \\beta_n\\) 是模型参数（系数）\n\\(\\epsilon\\) 是误差项\n\n参数估计: 最小二乘法（最小化残差平方和）\n\n残差平方和：\\(\\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2\\)\n封闭解：\\(\\boldsymbol{\\beta} = (X^TX)^{-1}X^Ty\\)\n\n假设条件:\n\n线性关系\n误差项独立同分布\n误差项服从均值为0的正态分布\n无多重共线性\n\n\n\n\n\n多项式回归\n\n\n\n\n\n\n\n多项式回归\n\n\n\n\n基本模型: \\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + ... + \\beta_n x^n + \\epsilon\\)\n本质: 线性回归的扩展，通过引入高阶特征捕捉非线性关系\n特征转换: 将原始特征转换为多项式特征，然后应用线性回归\n过拟合风险: 高阶多项式容易过拟合，需要正则化\n\n\n\n\n回归模型评估指标\n\n\n\n\n\n\n\n评估指标\n\n\n\n\n均方误差(MSE): \\(\\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2\\)\n均方根误差(RMSE): \\(\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2}\\)\n平均绝对误差(MAE): \\(\\frac{1}{n}\\sum_{i=1}^{n}|y_i-\\hat{y}_i|\\)\n决定系数(R²): \\(1 - \\frac{\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2}{\\sum_{i=1}^{n}(y_i-\\bar{y})^2}\\)\n\n取值范围：(-∞, 1]，1表示完美拟合\n可解释为模型解释的目标变量方差比例\n\n\n\n\n\n正则化方法\n\n\n\n\n\n\n\n正则化简介\n\n\n\n\nL1正则化(LASSO): 向目标函数添加L1范数惩罚项 \\(\\lambda\\sum_{j=1}^{n}|\\beta_j|\\)\n\n特点：可产生稀疏解，进行特征选择\n\nL2正则化(Ridge): 向目标函数添加L2范数惩罚项 \\(\\lambda\\sum_{j=1}^{n}\\beta_j^2\\)\n\n特点：减小模型复杂度，但不产生稀疏解\n\n弹性网(Elastic Net): 结合L1和L2正则化\n\n特点：结合两者优点，适用于多重共线性情况\n\n\n\n\n\n\n实践: scikit-learn实现\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split\n\n# 生成示例数据\nnp.random.seed(42)\nX = 2 * np.random.rand(100, 1)\ny = 4 + 3 * X + np.random.randn(100, 1)\n\n# 划分训练集和测试集\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# 线性回归\nlin_reg = LinearRegression()\nlin_reg.fit(X_train, y_train)\ny_pred = lin_reg.predict(X_test)\n\nprint(\"线性回归系数:\", lin_reg.coef_)\nprint(\"线性回归截距:\", lin_reg.intercept_)\nprint(\"MSE:\", mean_squared_error(y_test, y_pred))\nprint(\"R²:\", r2_score(y_test, y_pred))\n\n# 多项式回归\npoly_reg = Pipeline([\n    (\"poly_features\", PolynomialFeatures(degree=2, include_bias=False)),\n    (\"lin_reg\", LinearRegression())\n])\npoly_reg.fit(X_train, y_train)\ny_poly_pred = poly_reg.predict(X_test)\n\nprint(\"\\n多项式回归MSE:\", mean_squared_error(y_test, y_poly_pred))\nprint(\"多项式回归R²:\", r2_score(y_test, y_poly_pred))\n\n# Ridge回归（L2正则化）\nridge_reg = Ridge(alpha=1.0)\nridge_reg.fit(X_train, y_train)\ny_ridge_pred = ridge_reg.predict(X_test)\n\nprint(\"\\nRidge回归MSE:\", mean_squared_error(y_test, y_ridge_pred))\nprint(\"Ridge回归R²:\", r2_score(y_test, y_ridge_pred))\n\n# Lasso回归（L1正则化）\nlasso_reg = Lasso(alpha=0.1)\nlasso_reg.fit(X_train, y_train)\ny_lasso_pred = lasso_reg.predict(X_test)\n\nprint(\"\\nLasso回归MSE:\", mean_squared_error(y_test, y_lasso_pred))\nprint(\"Lasso回归R²:\", r2_score(y_test, y_lasso_pred))\n\n# 可视化结果\nplt.figure(figsize=(10, 6))\nplt.scatter(X_test, y_test, color='black', label='实际值')\nplt.plot(X_test, y_pred, color='blue', linewidth=3, label='线性回归')\nplt.plot(X_test, y_poly_pred, color='red', linewidth=3, label='多项式回归')\nplt.plot(X_test, y_ridge_pred, color='green', linewidth=3, label='Ridge回归')\nplt.plot(X_test, y_lasso_pred, color='orange', linewidth=3, label='Lasso回归')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('回归模型比较')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n小组项目二：房价预测模型构建\n\n\n\n\n\n\n项目介绍\n\n\n\n\n目标: 构建房价预测模型，根据房屋特征预测房价\n数据: 各小组自主选择房价数据集，应包含房价相关特征，数据量不低于500条\n算法: 线性回归或多项式回归\n提交内容: 模型代码、实验结果报告（包括模型评估指标）\n\n\n\n\n\n\n\n\n\n项目要求\n\n\n\n\n数据探索与预处理\n\n处理缺失值和异常值\n进行特征工程（特征转换、特征选择等）\n进行数据可视化，理解特征分布与相关性\n\n模型构建\n\n实现线性回归和/或多项式回归模型\n考虑应用正则化方法（Ridge、Lasso）减少过拟合\n\n模型评估\n\n使用多种评估指标（MSE、RMSE、MAE、R²）\n进行交叉验证，确保模型稳定性\n\n结果分析\n\n解释模型系数的含义\n分析特征重要性\n讨论模型局限性和可能的改进方向\n\n\n\n\n\n\n下次课预告\n下一次课我们将继续深入探讨回归算法，重点介绍集成学习（XGBoost）在回归问题中的应用，并进行小组项目二的代码实践。",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>第五周：项目汇报与回归算法基础</span>"
    ]
  },
  {
    "objectID": "week6.html",
    "href": "week6.html",
    "title": "第六周：集成学习与回归模型优化",
    "section": "",
    "text": "第一次课：回归算法(二) - 集成学习(XGBoost)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>第六周：集成学习与回归模型优化</span>"
    ]
  },
  {
    "objectID": "week6.html#第一次课回归算法二---集成学习xgboost",
    "href": "week6.html#第一次课回归算法二---集成学习xgboost",
    "title": "第六周：集成学习与回归模型优化",
    "section": "",
    "text": "本周学习目标\n\n\n\n\n理解集成学习的基本原理和分类\n掌握梯度提升决策树(GBDT)的工作原理\n深入学习XGBoost算法的特点和优势\n掌握XGBoost模型的参数调优方法\n能够使用xgboost库实现和评估回归模型\n学习回归模型评估指标的选择和应用场景\n将XGBoost应用于实际的房价预测问题\n\n\n\n\n内容概要\n\n集成学习概述\n\n\n\n\n\n\n\n什么是集成学习？\n\n\n\n\n定义: 集成学习通过组合多个基学习器来提高学习效果的一种机器学习方法\n主要策略:\n\nBagging: 并行训练多个基学习器(如随机森林)\nBoosting: 串行训练基学习器，每个学习器关注前一个学习器的错误(如AdaBoost, GBDT, XGBoost)\nStacking: 将多个不同类型的模型组合起来\n\n优势:\n\n降低方差，减少过拟合\n提高模型稳定性和预测准确性\n处理复杂非线性关系的能力强\n\n\n\n\n\n梯度提升决策树(GBDT)基础\n\n\n\n\n\n\n\nGBDT原理\n\n\n\n\n基本思想: 通过不断拟合前一个模型的残差来提高整体模型性能\n算法步骤:\n\n初始化模型为一个常数值(如平均值)\n计算当前模型的负梯度(残差)\n拟合一个新的决策树来预测这些残差\n将新树添加到模型中(带学习率)\n更新残差并重复步骤2-4直到满足停止条件\n\n特点:\n\n具有天然的特征重要性评估能力\n可以处理缺失值和混合类型特征\n对数据尺度不敏感\n\n\n\n\n\nXGBoost算法详解\n\n\n\n\n\n\n\nXGBoost的优势\n\n\n\n\nXGBoost = eXtreme Gradient Boost**ing\n相比传统GBDT的改进:\n\n使用了更为正则化的目标函数，减少过拟合\n支持列抽样(类似随机森林)，进一步降低过拟合风险\n优化算法，支持并行计算\n自动处理缺失值\n内置交叉验证和提前停止功能\n\n损失函数:\n\n目标函数 = 训练损失 + 正则化项\n\\(L = \\sum_{i=1}^n l(y_i, \\hat{y}_i) + \\sum_{k=1}^K \\Omega(f_k)\\)\n\n\n\n\n\nXGBoost参数调优\n\n\n\n\n\n\n\n重要参数解析\n\n\n\n\n训练参数:\n\nlearning_rate: 学习率，控制每棵树的贡献权重\nn_estimators: 树的数量\nmax_depth: 树的最大深度\nmin_child_weight: 叶子节点最小样本权重和\n\n正则化参数:\n\ngamma: 节点分裂所需的最小损失减少值\nreg_alpha: L1正则化参数\nreg_lambda: L2正则化参数\n\n随机化参数:\n\nsubsample: 行抽样比例\ncolsample_bytree: 列抽样比例\n\n调参策略:\n\n先调整树的复杂度参数(max_depth, min_child_weight)\n再调整随机化参数(subsample, colsample_bytree)\n最后调整正则化参数(gamma, reg_alpha, reg_lambda)\n降低learning_rate并增加n_estimators\n\n\n\n\n\n回归模型评估指标选择\n\n\n\n\n\n\n\n评估指标与应用场景\n\n\n\n\n均方误差(MSE): 对离群值敏感，适用于预测值不能有大偏差的场景\n均方根误差(RMSE): 与MSE类似，但单位与预测值相同，更直观\n平均绝对误差(MAE): 对离群值不敏感，适用于存在异常值的场景\n平均绝对百分比误差(MAPE): 衡量相对误差，适用于不同量级预测的比较\n决定系数(R²): 衡量模型解释方差的比例，适用于模型解释能力的评估\n选择原则:\n\n根据业务需求选择最合适的指标\n通常使用多个指标结合评估\n考虑预测偏差的严重程度与业务影响\n\n\n\n\n\n\n实践: XGBoost实现与调优\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport xgboost as xgb\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\n\n# 加载数据示例 (可替换为实际的房价数据集)\n# 这里使用生成的示例数据\nnp.random.seed(42)\nn_samples = 1000\nX = np.random.rand(n_samples, 5) \n# 创建非线性关系\ny = 5 + 3*X[:, 0]**2 + 2*X[:, 1] + np.sin(3*X[:, 2]) + np.exp(X[:, 3]) - 2*X[:, 4] + np.random.normal(0, 1, n_samples)\n\n# 划分训练集和测试集\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# 标准化特征\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# 线性回归作为基线模型\nlr = LinearRegression()\nlr.fit(X_train_scaled, y_train)\nlr_pred = lr.predict(X_test_scaled)\nlr_mse = mean_squared_error(y_test, lr_pred)\nlr_rmse = np.sqrt(lr_mse)\nlr_mae = mean_absolute_error(y_test, lr_pred)\nlr_r2 = r2_score(y_test, lr_pred)\n\nprint(\"线性回归基线模型:\")\nprint(f\"MSE: {lr_mse:.4f}\")\nprint(f\"RMSE: {lr_rmse:.4f}\")\nprint(f\"MAE: {lr_mae:.4f}\")\nprint(f\"R²: {lr_r2:.4f}\")\n\n# 基本XGBoost模型\nparams = {\n    'objective': 'reg:squarederror',\n    'learning_rate': 0.1,\n    'max_depth': 5,\n    'min_child_weight': 1,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'gamma': 0,\n    'reg_alpha': 0,\n    'reg_lambda': 1,\n    'random_state': 42\n}\n\nxgb_model = xgb.XGBRegressor(**params)\nxgb_model.fit(X_train_scaled, y_train)\nxgb_pred = xgb_model.predict(X_test_scaled)\nxgb_mse = mean_squared_error(y_test, xgb_pred)\nxgb_rmse = np.sqrt(xgb_mse)\nxgb_mae = mean_absolute_error(y_test, xgb_pred)\nxgb_r2 = r2_score(y_test, xgb_pred)\n\nprint(\"\\n基本XGBoost模型:\")\nprint(f\"MSE: {xgb_mse:.4f}\")\nprint(f\"RMSE: {xgb_rmse:.4f}\")\nprint(f\"MAE: {xgb_mae:.4f}\")\nprint(f\"R²: {xgb_r2:.4f}\")\n\n# 简单的参数调优示例\nparam_grid = {\n    'max_depth': [3, 5, 7],\n    'learning_rate': [0.01, 0.1, 0.2],\n    'n_estimators': [50, 100, 200],\n    'min_child_weight': [1, 3, 5]\n}\n\ngrid_search = GridSearchCV(\n    estimator=xgb.XGBRegressor(objective='reg:squarederror', \n                               subsample=0.8, \n                               colsample_bytree=0.8, \n                               random_state=42),\n    param_grid=param_grid,\n    cv=5,\n    scoring='neg_mean_squared_error',\n    verbose=1,\n    n_jobs=-1\n)\n\n# 注意：实际运行时这一步可能需要较长时间\n# 可以根据实际情况减少参数搜索空间\ngrid_search.fit(X_train_scaled, y_train)\n\nprint(\"\\n最优参数:\")\nprint(grid_search.best_params_)\n\n# 使用最优参数的模型\nbest_xgb_model = grid_search.best_estimator_\nbest_xgb_pred = best_xgb_model.predict(X_test_scaled)\nbest_xgb_mse = mean_squared_error(y_test, best_xgb_pred)\nbest_xgb_rmse = np.sqrt(best_xgb_mse)\nbest_xgb_mae = mean_absolute_error(y_test, best_xgb_pred)\nbest_xgb_r2 = r2_score(y_test, best_xgb_pred)\n\nprint(\"\\n调优后的XGBoost模型:\")\nprint(f\"MSE: {best_xgb_mse:.4f}\")\nprint(f\"RMSE: {best_xgb_rmse:.4f}\")\nprint(f\"MAE: {best_xgb_mae:.4f}\")\nprint(f\"R²: {best_xgb_r2:.4f}\")\n\n# 特征重要性可视化\nplt.figure(figsize=(10, 6))\nxgb.plot_importance(best_xgb_model, importance_type='weight')\nplt.title('XGBoost 特征重要性')\nplt.tight_layout()\nplt.show()\n\n# 预测vs实际值对比\nplt.figure(figsize=(10, 6))\nplt.scatter(y_test, best_xgb_pred, alpha=0.5)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\nplt.xlabel('实际值')\nplt.ylabel('预测值')\nplt.title('XGBoost模型预测vs实际值')\nplt.tight_layout()\nplt.show()\n\n# 对比不同模型性能\nmodels = ['线性回归', '基本XGBoost', '调优XGBoost']\nmse_values = [lr_mse, xgb_mse, best_xgb_mse]\nr2_values = [lr_r2, xgb_r2, best_xgb_r2]\n\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.bar(models, mse_values)\nplt.title('均方误差比较')\nplt.ylabel('MSE')\n\nplt.subplot(1, 2, 2)\nplt.bar(models, r2_values)\nplt.title('R²比较')\nplt.ylabel('R²')\n\nplt.tight_layout()\nplt.show()\n\n\n小组项目二：房价预测模型优化 (XGBoost)\n\n\n\n\n\n\n项目要求\n\n\n\n\n在上周线性/多项式回归模型的基础上，使用XGBoost算法优化房价预测模型\n进行模型参数调优，提高预测性能\n比较XGBoost与线性回归、多项式回归的性能差异\n分析特征重要性，解释模型预测结果\n\n\n\n\n\n\n\n\n\n提交内容\n\n\n\n\n代码实现\n\nXGBoost模型的实现代码\n参数调优策略与实现\n性能对比代码\n\n实验报告\n\n数据集描述\n预处理步骤\n不同模型的性能指标比较\n参数调优过程与结果\n特征重要性分析\n结论与讨论\n\n提交时间\n\n第7周课前提交\n\n\n\n\n\n\n下次课预告\n下节课我们将进行小组项目二的代码实践，指导大家使用XGBoost优化房价预测模型，解决实际问题中遇到的困难，并帮助大家撰写高质量的项目报告。",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>第六周：集成学习与回归模型优化</span>"
    ]
  },
  {
    "objectID": "week6.html#第二次课小组项目二房价预测模型优化-xgboost",
    "href": "week6.html#第二次课小组项目二房价预测模型优化-xgboost",
    "title": "第六周：集成学习与回归模型优化",
    "section": "第二次课：小组项目二：房价预测模型优化 (XGBoost)",
    "text": "第二次课：小组项目二：房价预测模型优化 (XGBoost)\n\n课堂实践安排\n\n\n\n\n\n\n课堂活动\n\n\n\n\n小组开发环境准备与代码规划\n在老师指导下，各小组使用XGBoost算法优化房价预测模型\n实施参数调优，提高模型性能\n比较不同模型性能，分析优劣\n撰写项目报告\n\n\n\n\n\n指导重点\n\n参数调优实践\n\n使用网格搜索、随机搜索或贝叶斯优化进行参数调优\n使用交叉验证评估模型性能\n解读参数调优结果，理解参数对模型性能的影响\n\n特征工程探索\n\n尝试不同的特征变换方法\n利用XGBoost的特征重要性进行特征选择\n处理类别特征的多种方法\n\n模型评估与解释\n\n使用多种评估指标综合评估模型性能\n解释XGBoost模型的预测结果\n通过特征重要性分析理解房价影响因素\n\n报告撰写指导\n\n实验报告结构与内容组织\n数据可视化的有效方法\n结果分析与讨论的深度与广度\n\n\n\n\n开发流程建议\n\n数据准备与特征工程\n# 数据加载与预处理\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n# 加载数据\ndf = pd.read_csv('房价数据集.csv')\n\n# 分离特征和目标\nX = df.drop('price', axis=1)\ny = df['price']\n\n# 识别数值和类别特征\nnumerical_features = X.select_dtypes(include=['int64', 'float64']).columns\ncategorical_features = X.select_dtypes(include=['object']).columns\n\n# 创建预处理管道\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), numerical_features),\n        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n    ])\n\n# 划分训练集和测试集\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nXGBoost模型训练与调优\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\n\n# 创建模型管道\nxgb_pipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('xgb', xgb.XGBRegressor(objective='reg:squarederror', random_state=42))\n])\n\n# 参数网格\nparam_grid = {\n    'xgb__max_depth': [3, 5, 7],\n    'xgb__learning_rate': [0.01, 0.1, 0.2],\n    'xgb__n_estimators': [100, 200],\n    'xgb__min_child_weight': [1, 3, 5],\n    'xgb__subsample': [0.7, 0.8, 0.9],\n    'xgb__colsample_bytree': [0.7, 0.8, 0.9]\n}\n\n# 网格搜索\ngrid_search = GridSearchCV(\n    xgb_pipeline,\n    param_grid,\n    cv=5,\n    scoring='neg_mean_squared_error',\n    verbose=1,\n    n_jobs=-1\n)\n\ngrid_search.fit(X_train, y_train)\n\n# 最优模型\nbest_model = grid_search.best_estimator_\n模型评估与比较\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 获取预测结果\ny_pred = best_model.predict(X_test)\n\n# 计算各种评估指标\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"MSE: {mse:.2f}\")\nprint(f\"RMSE: {rmse:.2f}\")\nprint(f\"MAE: {mae:.2f}\")\nprint(f\"R²: {r2:.2f}\")\n\n# 与线性回归和多项式回归比较\n# (假设你已有这些模型的结果)\nmodels = ['线性回归', '多项式回归', 'XGBoost']\nrmse_scores = [linear_rmse, poly_rmse, rmse]\nr2_scores = [linear_r2, poly_r2, r2]\n\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.bar(models, rmse_scores)\nplt.title('RMSE比较')\nplt.ylabel('RMSE (越低越好)')\n\nplt.subplot(1, 2, 2)\nplt.bar(models, r2_scores)\nplt.title('R²比较')\nplt.ylabel('R² (越高越好)')\n\nplt.tight_layout()\nplt.show()\n特征重要性分析\n# 获取特征重要性\nxgb_model = best_model.named_steps['xgb']\npreprocessor = best_model.named_steps['preprocessor']\n\n# 获取处理后的特征名称\nfeature_names = (\n    numerical_features.tolist() +\n    preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features).tolist()\n)\n\n# 可视化特征重要性\nimportance = xgb_model.feature_importances_\nindices = np.argsort(importance)[::-1]\n\nplt.figure(figsize=(12, 8))\nplt.title('XGBoost特征重要性')\nplt.bar(range(len(importance)), importance[indices])\nplt.xticks(range(len(importance)), [feature_names[i] for i in indices], rotation=90)\nplt.tight_layout()\nplt.show()\n\n\n\n提醒事项\n\n\n\n\n\n\n重要提示\n\n\n\n\n注意模型过拟合问题，使用交叉验证评估模型性能\n参数调优时注意计算资源消耗，合理设置参数搜索空间\n结合业务背景解释特征重要性和模型预测结果\n同时关注模型的预测准确性和可解释性\n报告中需对比不同模型，分析XGBoost的优势与局限性\n\n\n\n\n\n下周预告\n下周我们将开始学习聚类算法，首先介绍K-Means聚类算法，并将其应用于用户分群问题，开启我们在无监督学习领域的探索。同时，请各小组在下周课前提交房价预测模型优化(XGBoost)及报告。",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>第六周：集成学习与回归模型优化</span>"
    ]
  },
  {
    "objectID": "week7.html",
    "href": "week7.html",
    "title": "第 7 周：聚类基础与层次结构",
    "section": "",
    "text": "学习目标",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>第 7 周：聚类基础与层次结构</span>"
    ]
  },
  {
    "objectID": "week7.html#学习目标",
    "href": "week7.html#学习目标",
    "title": "第 7 周：聚类基础与层次结构",
    "section": "",
    "text": "回顾 K-Means 算法的原理、优缺点和局限性。\n理解层次聚类的基本原理、不同连接标准和树状图 (Dendrogram) 的解读。\n掌握使用 scikit-learn 实现 K-Means 和层次聚类的方法。\n了解层次聚类在市场细分等场景的应用。\n启动小组项目三，明确项目要求和可选算法。",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>第 7 周：聚类基础与层次结构</span>"
    ]
  },
  {
    "objectID": "week7.html#第一次课k-means-回顾与层次聚类入门",
    "href": "week7.html#第一次课k-means-回顾与层次聚类入门",
    "title": "第 7 周：聚类基础与层次结构",
    "section": "第一次课：K-Means 回顾与层次聚类入门",
    "text": "第一次课：K-Means 回顾与层次聚类入门\n\n1. K-Means 算法回顾与局限性\n我们在之前的课程中已经学习了 K-Means 算法，它是一种非常常用且高效的划分式聚类算法。\n\n核心思想: 将数据点划分为预先设定的 K 个簇，使得每个点都属于离其最近的簇中心（质心），并且簇内数据点的平方和最小。\n优点:\n\n算法简单，易于理解和实现。\n对于球状簇且簇之间分离明显的数据集，效果较好。\n计算效率相对较高，适合处理大规模数据集。\n\n缺点与局限性:\n\n需要预先指定 K 值: K 值的选择对结果影响很大，选择不当会导致次优的聚类效果。肘部法则和轮廓系数可以辅助选择，但并非总能找到最优 K。\n对初始质心敏感: 不同的初始质心可能导致不同的聚类结果。scikit-learn 中的 n_init 参数可以通过多次初始化来缓解这个问题。\n对非球状簇效果不佳: K-Means 假设簇是凸形的、各向同性的（类似圆形或球形），难以发现非规则形状的簇。\n对异常值敏感: 异常值会对质心的计算产生较大影响，可能导致簇的划分发生偏移。\n\n\n正是因为 K-Means 的这些局限性，我们需要学习其他聚类算法来应对更复杂的数据和场景。\n\n\n2. 层次聚类 (Hierarchical Clustering)\n层次聚类是一种不需要预先指定簇数量的聚类方法，它会构建一个嵌套的簇的层次结构。\n\n两种主要方式:\n\n凝聚式 (Agglomerative): 自底向上，开始时每个数据点自成一簇，然后逐步合并最相似的簇，直到所有点合并为一个簇。这是最常用的方式。\n分裂式 (Divisive): 自顶向下，开始时所有数据点在一个簇，然后逐步分裂最不相似的簇，直到每个点自成一簇。计算复杂度较高，不太常用。\n\n凝聚式层次聚类步骤:\n\n将每个数据点视为一个单独的簇。\n计算所有簇之间的距离（或相似度）。\n合并距离最近（最相似）的两个簇。\n重新计算新合并簇与其他簇之间的距离。\n重复步骤 3 和 4，直到所有数据点合并为一个簇。\n\n关键概念:\n\n距离度量 (Distance Metric): 用于计算数据点或簇之间的距离，常用欧氏距离 (Euclidean distance)。\n连接标准 (Linkage Criteria): 定义如何计算簇之间的距离。常用标准包括：\n\nWard: 最小化簇内方差的增量。通常效果较好，倾向于产生大小相似的簇。(linkage='ward')\nAverage Linkage: 计算两个簇中所有点对之间距离的平均值。(linkage='average')\nComplete Linkage (Maximum Linkage): 计算两个簇中所有点对之间距离的最大值。(linkage='complete')\nSingle Linkage (Minimum Linkage): 计算两个簇中所有点对之间距离的最小值。(linkage='single') 对异常值敏感，可能产生链状效果。\n\n\n树状图 (Dendrogram):\n\n层次聚类的结果通常用树状图可视化。\n纵轴表示簇合并时的距离（或不相似度），横轴表示数据点（或样本索引）。\n通过在某个距离阈值水平切割树状图，可以得到指定数量的簇。切割线穿过的垂直线数量即为簇的数量。\n树状图的高度差可以反映簇之间的分离程度。\n\n优点:\n\n无需预先指定 K 值: 可以根据树状图决定合适的簇数量。\n可以揭示数据的层次结构: 树状图本身提供了丰富的结构信息。\n对于某些连接标准（如 Single Linkage），可以发现非凸形状的簇。\n\n缺点:\n\n计算复杂度较高: 通常为 O(n^3) 或 O(n^2 log n)，不适合非常大的数据集。\n合并决策不可撤销: 一旦两个簇被合并，后续步骤无法撤销。\n对距离度量和连接标准的选择比较敏感。\n\n\n\n\n3. 实践：使用 scikit-learn 实现层次聚类\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import AgglomerativeClustering\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\n# 1. 生成示例数据\nX, y = make_blobs(n_samples=50, centers=3, cluster_std=1.0, random_state=42)\n\n# 2. 计算连接矩阵 (用于绘制树状图)\n# 使用 'ward' 连接标准\nlinked_ward = linkage(X, method='ward')\n\n# 3. 绘制树状图 (Dendrogram)\nplt.figure(figsize=(10, 7))\ndendrogram(linked_ward,\n            orientation='top',\n            labels=np.arange(1, X.shape[0] + 1), # 可以替换为样本标签\n            distance_sort='descending',\n            show_leaf_counts=True)\nplt.title('Hierarchical Clustering Dendrogram (Ward Linkage)')\nplt.xlabel('Sample Index')\nplt.ylabel('Distance (Ward)')\n# 添加一条水平线来建议切割点 (例如，选择3个簇)\nplt.axhline(y=15, color='r', linestyle='--')\nplt.show()\n\n# 4. 使用 AgglomerativeClustering 进行聚类\n# 假设我们根据树状图决定分为 3 个簇\nn_clusters = 3\nagg_clustering = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')\nagg_labels = agg_clustering.fit_predict(X)\n\n# 5. 可视化聚类结果\nplt.figure(figsize=(8, 6))\nplt.scatter(X[:, 0], X[:, 1], c=agg_labels, cmap='viridis', s=50)\nplt.title(f'Agglomerative Clustering (k={n_clusters}, Ward)')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.grid(True)\nplt.show()\n\nprint(\"层次聚类分配的标签:\", agg_labels)\nAI 辅助编程提示:\n\n尝试让 AI 解释 scipy.cluster.hierarchy.linkage 和 dendrogram 函数的参数。\n让 AI 生成不同连接标准 (average, complete, single) 的树状图并比较差异。\n询问 AI 如何根据树状图自动选择最佳 K 值（虽然没有完美方法，但可以了解一些启发式策略）。\n\n\n\n4. 小组项目三启动：用户分群模型构建与分析 (更新)\n\n目标: 掌握不同聚类算法的应用，理解聚类结果评估，并能对聚类结果进行业务解读。\n核心要求:\n\n算法选择: 至少尝试两种聚类算法（从 K-Means, 层次聚类, GMM, DBSCAN 中选择）。\n对比分析: 对比不同算法在你的数据集上的表现（例如，使用轮廓系数、Davies-Bouldin 指数评估，并结合可视化结果），解释选择最终模型的原因。\n结果解读: 对最终的聚类结果进行业务层面的分析和解读（例如，不同用户群体的特征是什么？）。\n\n数据集: 学生小组自主选择，建议选择包含用户画像或用户行为特征的数据集，鼓励寻找能体现不同算法优势的数据。\n可选挑战: 鼓励有能力的小组尝试基于文本 Embeddings 的聚类（我们将在下周介绍概念）。例如，对电商评论数据进行聚类。\n提交时间:\n\n初步模型与算法选择理由 (例如 K-Means vs 层次聚类): 第 9 周课前\n最终模型、对比分析与报告: 第 10 或 11 周课前 (待定)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>第 7 周：聚类基础与层次结构</span>"
    ]
  },
  {
    "objectID": "week7.html#第二次课层次聚类应用与项目讨论",
    "href": "week7.html#第二次课层次聚类应用与项目讨论",
    "title": "第 7 周：聚类基础与层次结构",
    "section": "第二次课：层次聚类应用与项目讨论",
    "text": "第二次课：层次聚类应用与项目讨论\n\n1. 层次聚类应用场景\n层次聚类由于其能够展示数据的层级结构，在许多领域都有应用：\n\n市场细分 (Market Segmentation):\n\n通过对用户的购买行为、人口统计学特征、兴趣偏好等数据进行层次聚类，可以发现不同层级的用户群体。\n树状图可以直观地展示细分市场之间的关系和距离，帮助营销人员理解市场的结构，制定更精准的营销策略。例如，先区分高价值和低价值客户，再在高价值客户中细分出不同偏好的群体。\n\n生物信息学 (Bioinformatics):\n\n基因表达谱分析：对不同样本或不同条件下的基因表达数据进行聚类，可以发现功能相似或共同调控的基因群，或者对样本进行分类。\n物种分类：构建物种的系统发育树。\n\n社交网络分析 (Social Network Analysis):\n\n社群发现：识别社交网络中的紧密连接的子群组或社区。\n\n图像分割 (Image Segmentation):\n\n将图像中的像素根据颜色、纹理等特征进行层次聚类，实现图像区域的划分。\n\n\n\n\n2. 小组项目三：数据探索与算法选择讨论\n\n课堂活动:\n\n各小组展示初步选择的数据集。\n讨论数据集的特点（维度、样本量、特征类型、是否存在明显异常值、预期的簇形状等）。\n基于数据特点和项目目标，初步讨论选择哪两种或更多聚类算法进行尝试。\n\n如果数据量很大，层次聚类可能计算较慢。\n如果预期簇是球状的，K-Means 是个好的起点。\n如果想探索数据的内在层次结构，层次聚类更合适。\n如果数据中有明显异常值，K-Means 可能受影响较大。\n\n教师巡回指导，提供建议。\n\n\n\n\n3. 思考与练习\n\nK-Means 的主要局限性有哪些？在什么情况下它可能不是最佳选择？\n解释凝聚式层次聚类和分裂式层次聚类的区别。为什么凝聚式更常用？\n什么是连接标准 (Linkage Criteria)？ ‘Ward’ 连接和 ‘Single’ 连接的主要区别和适用场景是什么？\n如何解读树状图 (Dendrogram)？如何利用它来帮助选择簇的数量？\n假设你要对一批客户根据他们的消费金额和购买频率进行聚类，你会优先考虑 K-Means 还是层次聚类？为什么？如果数据量非常大（百万级别），你的选择会改变吗？",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>第 7 周：聚类基础与层次结构</span>"
    ]
  },
  {
    "objectID": "week8.html",
    "href": "week8.html",
    "title": "第 8 周：概率聚类、密度聚类与现代应用",
    "section": "",
    "text": "学习目标",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>第 8 周：概率聚类、密度聚类与现代应用</span>"
    ]
  },
  {
    "objectID": "week8.html#学习目标",
    "href": "week8.html#学习目标",
    "title": "第 8 周：概率聚类、密度聚类与现代应用",
    "section": "",
    "text": "理解高斯混合模型 (GMM) 的原理、概率聚类思想和 EM 算法的基本概念。\n掌握使用 scikit-learn 实现 GMM 的方法。\n回顾 DBSCAN 算法的原理，并理解其在发现任意形状簇和异常点检测方面的优势。\n能够对比 K-Means、层次聚类、GMM 和 DBSCAN 的特点、适用场景和优缺点。\n理解基于 Embeddings 的聚类基本概念及其在处理文本、图像等非结构化数据中的应用。\n继续推进小组项目三，实现所选算法并分析聚类结果。",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>第 8 周：概率聚类、密度聚类与现代应用</span>"
    ]
  },
  {
    "objectID": "week8.html#第一次课高斯混合模型-gmm",
    "href": "week8.html#第一次课高斯混合模型-gmm",
    "title": "第 8 周：概率聚类、密度聚类与现代应用",
    "section": "第一次课：高斯混合模型 (GMM)",
    "text": "第一次课：高斯混合模型 (GMM)\n我们在 K-Means 中假设簇是球状的，并且每个点只能硬性地属于一个簇。高斯混合模型 (GMM) 提供了一种更灵活的概率聚类方法。\n\n1. 高斯混合模型 (Gaussian Mixture Models, GMM)\n\n核心思想: GMM 假设数据是由 K 个不同的高斯分布（正态分布）混合生成的。每个高斯分布代表一个簇，具有自己的均值 (mean)、协方差 (covariance) 和权重 (weight)。\n概率聚类 (Soft Clustering): 与 K-Means 将每个点分配给唯一一个簇（硬分配）不同，GMM 计算的是每个数据点属于每个高斯分布（簇）的概率。一个点可以同时属于多个簇，只是概率不同。这对于描述簇之间存在重叠或边界模糊的数据很有用。\n参数:\n\n均值 (μ): 每个高斯分布的中心。\n协方差 (Σ): 描述每个高斯分布的形状和方向。协方差矩阵决定了簇是圆形的、椭圆形的，以及椭圆的方向。这是 GMM 能适应更复杂簇形状的关键。\n权重 (π): 每个高斯分布在整个混合模型中所占的比例或先验概率。所有权重的和为 1。\n\n期望最大化 (Expectation-Maximization, EM) 算法: GMM 通常使用 EM 算法来估计模型的参数（均值、协方差、权重）。EM 算法是一个迭代过程：\n\nE 步 (Expectation): 基于当前的参数，估计每个数据点属于每个高斯分布的后验概率（责任 Rresponsibility）。\nM 步 (Maximization): 基于 E 步计算出的概率，重新估计模型的参数（均值、协方差、权重），使得模型的似然函数最大化。\n重复 E 步和 M 步，直到参数收敛或达到最大迭代次数。\n\n\n注：我们不需要深入 EM 的数学推导，理解其迭代优化思想即可。\n\n优点:\n\n灵活性高: 可以拟合非球状的簇（椭圆形），因为每个高斯分量有自己的协方差矩阵。\n提供概率信息: 软聚类提供了更丰富的信息，表示数据点属于每个簇的不确定性。\n模型有扎实的概率基础。\n\n缺点:\n\n对初始化敏感: EM 算法可能收敛到局部最优解，不同的初始参数可能导致不同的结果。scikit-learn 中的 n_init 参数可以运行多次来缓解。\n可能需要较多数据: 估计协方差矩阵需要足够的数据支持。\n计算可能较慢: 特别是对于高维数据或大量簇。\n需要预先指定 K 值 (组件数量): 与 K-Means 类似，需要确定高斯分布的数量。可以使用 AIC (Akaike Information Criterion) 或 BIC (Bayesian Information Criterion) 等模型选择准则来辅助选择。\n\n\n\n\n2. 实践：使用 scikit-learn 实现 GMM\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom sklearn.mixture import GaussianMixture\nfrom matplotlib.patches import Ellipse # 用于绘制椭圆\n\n# 1. 生成一些可能重叠或非球状的数据\nX, y_true = make_blobs(n_samples=300, centers=3, cluster_std=[1.0, 2.5, 0.5], random_state=170)\n# 稍微变换数据使其更像椭圆\ntransformation = [[0.6, -0.6], [-0.4, 0.8]]\nX = np.dot(X, transformation)\n\n# 2. 使用 GMM 进行聚类\nn_components = 3 # 假设我们知道有 3 个簇\ngmm = GaussianMixture(n_components=n_components, random_state=42, n_init=10)\ngmm.fit(X)\ngmm_labels = gmm.predict(X)\ngmm_probs = gmm.predict_proba(X) # 获取每个点属于每个簇的概率\n\n# 3. 可视化 GMM 聚类结果和概率椭圆\nplt.figure(figsize=(10, 8))\nax = plt.gca()\n\n# 绘制数据点，颜色根据 GMM 预测的标签\ncolors = plt.cm.viridis(gmm_labels / (n_components - 1)) if n_components &gt; 1 else ['blue'] * len(X)\nplt.scatter(X[:, 0], X[:, 1], c=colors, s=10, alpha=0.7)\n\n# 绘制表示每个高斯分量的椭圆\nfor i in range(n_components):\n    mean = gmm.means_[i]\n    covar = gmm.covariances_[i]\n\n    # 计算椭圆参数\n    v, w = np.linalg.eigh(covar)\n    v = 2. * np.sqrt(2.) * np.sqrt(v) # 放大椭圆以包含约 95% 的点\n    u = w[0] / np.linalg.norm(w[0])\n    angle = np.arctan2(u[1], u[0])\n    angle = 180. * angle / np.pi # 转换为度\n\n    # 绘制椭圆\n    ellipse = Ellipse(mean, v[0], v[1], angle=angle, fill=False, edgecolor='red', linewidth=2)\n    ax.add_patch(ellipse)\n\nplt.title(f'Gaussian Mixture Model Clustering (k={n_components})')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.grid(True)\nplt.axis('equal') # 保持横纵轴比例一致，以便正确显示椭圆形状\nplt.show()\n\n# 打印一些样本的概率\nprint(\"前 5 个样本属于每个簇的概率:\\n\", gmm_probs[:5].round(3))\nAI 辅助编程提示:\n\n询问 AI 如何使用 AIC 或 BIC 来帮助选择 GMM 的最佳组件数量 (n_components)。\n让 AI 解释 GaussianMixture 类中 covariance_type 参数的不同选项 ('full', 'tied', 'diag', 'spherical') 及其含义。\n尝试让 AI 生成一个对比 K-Means 和 GMM 在同一份非球状数据集上聚类效果的示例代码和可视化。",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>第 8 周：概率聚类、密度聚类与现代应用</span>"
    ]
  },
  {
    "objectID": "week8.html#第二次课dbscan-回顾算法对比与现代应用",
    "href": "week8.html#第二次课dbscan-回顾算法对比与现代应用",
    "title": "第 8 周：概率聚类、密度聚类与现代应用",
    "section": "第二次课：DBSCAN 回顾、算法对比与现代应用",
    "text": "第二次课：DBSCAN 回顾、算法对比与现代应用\n\n1. DBSCAN 算法回顾\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise) 是一种基于密度的聚类算法。\n\n核心思想: 寻找被低密度区域分隔的高密度区域作为簇。它不需要预先指定簇的数量。\n关键概念:\n\n核心点 (Core Point): 在半径 eps 内至少包含 min_samples 个点的点。\n边界点 (Border Point): 不是核心点，但在某个核心点的半径 eps 内。\n噪声点 (Noise Point): 既不是核心点也不是边界点。\n密度可达 (Directly Density-Reachable): 点 p 在核心点 q 的 eps 邻域内。\n密度相连 (Density-Connected): 存在一个核心点 r，使得 p 和 q 都从 r 密度可达。\n\n算法流程:\n\n随机选择一个未访问的点 p。\n检查 p 是否为核心点。\n如果是核心点，则创建一个新簇，并将 p 加入该簇。然后找到所有从 p 密度可达的点，将它们也加入该簇。递归地查找这些新加入点的密度可达点。\n如果 p 不是核心点，则暂时标记为噪声点（后续可能被发现是边界点而加入某个簇）。\n重复步骤 1-4，直到所有点都被访问。\n\n优点:\n\n可以发现任意形状的簇: 不局限于球状或凸形。\n对噪声点不敏感: 可以识别并标记噪声点。\n无需预先指定 K 值: 簇的数量由算法根据数据密度自动确定。\n\n缺点:\n\n对参数 eps 和 min_samples 敏感: 参数选择对结果影响很大，需要调试。\n对于密度变化较大的数据集效果不佳: 难以用一组全局参数处理密度差异很大的簇。\n对于高维数据效果可能下降: “维度灾难” 会导致点之间的距离趋于一致，密度定义变得困难（k-近邻图等方法可以辅助选择 eps）。\n\n\n\n\n2. 聚类算法选择与对比\n\n\n\n\n\n\n\n\n\n\n特性\nK-Means\n层次聚类 (凝聚式)\nGMM (高斯混合模型)\nDBSCAN\n\n\n\n\n簇形状\n球状 (各向同性)\n任意 (取决于连接标准)\n椭圆状 (灵活)\n任意形状\n\n\n簇数量 (K)\n需预先指定\n无需预先指定 (看树状图)\n需预先指定 (可用AIC/BIC)\n无需预先指定\n\n\n聚类类型\n硬聚类\n硬聚类\n软聚类 (概率)\n硬聚类 (含噪声点)\n\n\n对异常值\n敏感\n较敏感 (取决于连接标准)\n相对鲁棒\n不敏感 (识别为噪声)\n\n\n主要优点\n简单、高效、适合球状簇\n可视化层次结构、无需定 K\n概率模型、适应椭圆簇\n发现任意形状、处理噪声\n\n\n主要缺点\n对 K 和初始值敏感、限球状\n计算复杂度高、合并不可逆\n对初始化敏感、计算可能较慢\n对参数敏感、难处理密度变化\n\n\n适用场景\n簇较规则、数据量大\n需要探索层次结构、簇数量未知\n簇可能重叠或呈椭圆状\n簇形状不规则、含噪声数据\n\n\n主要参数\nn_clusters\nn_clusters (或切割高度), linkage\nn_components, covariance_type\neps, min_samples\n\n\n\n选择建议:\n\n没有绝对最好的算法，选择取决于数据特性和分析目标。\n先探索数据: 可视化数据分布、了解特征含义。\n尝试多种算法: 对比不同算法的结果。\n评估结果: 使用轮廓系数、DB 指数等内部指标，并结合业务知识进行外部评估。\n考虑计算成本: 对于大数据集，K-Means 和 MiniBatchKMeans 通常更快。\n\n\n\n3. 新概念：基于 Embeddings 的聚类\n传统的聚类算法主要处理数值型数据。但现实中我们经常遇到非结构化数据，如文本、图像、音频等。如何对这些数据进行聚类？\n核心思想: 将高维、稀疏、非结构化的数据，通过 Embedding 技术 转换为低维、稠密的向量表示 (Embeddings)，这些向量能够捕捉原始数据的语义信息。然后，在这些 Embedding 向量上应用标准的聚类算法（如 K-Means, DBSCAN 等）。\n\n什么是 Embedding?\n\n可以理解为一种 “编码” 或 “映射”，将复杂的对象（如一个词、一篇文档、一张图片）表示为一个固定长度的实数向量。\n好的 Embedding 应该使得语义上相似的对象在向量空间中距离也相近。\n\n常见的 Embedding 技术:\n\n文本:\n\nWord Embeddings: Word2Vec, GloVe, FastText (将词映射为向量)。\nSentence/Document Embeddings: Sentence-BERT (SBERT), Universal Sentence Encoder (USE), 或通过预训练语言模型 (如 BERT, GPT) 获取 [CLS] token 或对词向量进行池化 (Pooling)。\n\n图像: 通过预训练的卷积神经网络 (CNN) 如 ResNet, VGG, EfficientNet 等，提取其中间层或最后一层的特征向量。\n其他: 图神经网络 (GNN) 用于图数据，Autoencoders 用于通用降维和特征提取。\n\n流程:\n\n选择/训练 Embedding 模型: 根据数据类型选择合适的预训练模型或自行训练。\n数据转换: 将原始数据（文本、图像等）输入 Embedding 模型，得到对应的向量表示。\n聚类: 在得到的 Embedding 向量上应用 K-Means, DBSCAN, GMM 或层次聚类等算法。\n结果分析: 分析聚类结果，解读每个簇的含义（例如，查看文本簇中的高频词，或图像簇中的代表性图片）。\n\n应用场景:\n\n用户评论/反馈聚类: 发现用户关注的主要问题或意见类别。\n新闻/文档主题挖掘: 对大量文章进行聚类，自动发现潜在主题。\n商品推荐: 对商品描述或图片进行 Embedding 和聚类，找到相似商品。\n图像检索/分类: 对图像进行聚类，用于相似图像查找或无监督分类。\n\n\nAI 辅助编程提示:\n\n询问 AI 如何使用 Sentence-Transformers 库将句子列表转换为 Embeddings。\n让 AI 提供一个简单的示例：获取文本 Embeddings 后，使用 K-Means 进行聚类。\n询问 AI 在图像聚类中，如何使用预训练的 ResNet (例如通过 PyTorch 或 TensorFlow/Keras) 来提取图像特征向量。\n\n\n\n4. 小组项目三：模型实现与结果分析\n\n课堂活动:\n\n学生分组继续实现小组项目三。\n重点关注：\n\n实现至少两种选择的聚类算法。\n使用轮廓系数、DB 指数等指标评估不同算法和不同参数下的结果。\n可视化聚类结果（二维散点图，如果维度较高可先降维）。\n开始尝试对聚类结果进行业务解读。\n\n教师巡回指导，解答代码实现、算法选择、结果评估和解读中遇到的问题。\n对于选择挑战“基于 Embeddings 的聚类”的小组，提供额外的指导和资源。\n\n\n\n\n5. 思考与练习\n\nGMM 与 K-Means 的主要区别是什么？GMM 的“软聚类”体现在哪里？\nDBSCAN 相比于 K-Means 的主要优势是什么？它在什么场景下特别有用？\n假设你要对一个包含不同密度区域和一些噪声点的数据集进行聚类，你会优先考虑哪种算法？为什么？\n解释“基于 Embeddings 的聚类”的基本流程。为什么需要 Embedding 这一步？\n如果你要对大量用户评论进行聚类以发现主要抱怨点，你会如何设计技术方案？（提示：考虑 Embedding 和聚类算法的选择）",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>第 8 周：概率聚类、密度聚类与现代应用</span>"
    ]
  }
]