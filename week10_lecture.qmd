---
title: "第十周：降维与特征选择 - 为模型减负增效"
subtitle: "学习 PCA 降维和多种特征选择技术"
format:
  html:
    toc: true
    toc-location: left
    code-fold: show
    theme: cosmo
    number-sections: true
---

# 第十周：降维与特征选择

在处理现实世界的数据时，我们经常会遇到特征维度非常高的情况（即有很多列）。高维度数据不仅会增加模型的计算复杂度、延长训练时间，还可能引入噪声、导致**维度灾难 (Curse of Dimensionality)**，使得模型难以学习有效的模式，甚至降低性能。本周，我们将学习两大类处理高维数据的关键技术：**降维 (Dimensionality Reduction)**，以**主成分分析 (PCA)** 为代表；以及**特征选择 (Feature Selection)**，包括过滤法、包裹法和嵌入法。这些技术能帮助我们简化数据、去除冗余、提高模型效率和性能。

## 1. 为什么需要降维和特征选择？

*   **降低计算复杂度:** 特征越少，模型训练和预测所需的时间和内存就越少。
*   **缓解维度灾难:** 在高维空间中，数据点变得稀疏，距离度量失去意义，模型更难找到有效的模式。
*   **去除冗余和噪声:** 并非所有特征都是有用的，有些特征可能高度相关（冗余），有些可能是噪声。去除它们有助于提高模型性能。
*   **提高模型可解释性:** 使用更少的关键特征更容易理解模型的决策过程。
*   **数据可视化:** 将高维数据降到 2 维或 3 维，方便我们进行可视化探索。

## 2. 主成分分析 (PCA - Principal Component Analysis) 深入

PCA 是一种非常流行的**无监督线性降维**技术，属于**特征提取 (Feature Extraction)** 的范畴。它不是简单地选择一部分原始特征，而是将原始特征**线性组合**成一组新的、不相关的**主成分 (Principal Components)**，这些主成分能最大程度地保留原始数据的**方差 (Variance)**。

### 2.1 原理回顾与深入

*   **目标:** 找到一组新的正交（相互垂直）坐标轴（即主成分），使得数据在这些轴上的投影方差最大化。
*   **第一主成分:** 数据投影后方差最大的那个方向。
*   **第二主成分:** 与第一主成分正交，并且是剩余方差最大的方向。
*   **以此类推:** 第 k 个主成分与前 k-1 个主成分都正交，并且是剩余方差最大的方向。
*   **降维:** 选择方差最大的前 k 个主成分来代表原始数据，从而达到降维的目的。这 k 个主成分是原始特征的线性组合。

### 2.2 方差解释率 (Explained Variance Ratio)

PCA 的一个重要输岀是每个主成分能够解释原始数据**方差的比例** (`explained_variance_ratio_`)。

*   第一个主成分解释的方差比例最高，第二个次之，以此类推。
*   所有主成分解释的方差比例之和为 1 (或 100%)。
*   通过计算**累积方差解释率**，我们可以决定需要保留多少个主成分才能保留足够的信息（例如，保留能够解释 95% 或 99% 方差的主成分）。

### 2.3 如何选择主成分数量 (`n_components`)

*   **根据累积方差解释率:** 绘制累积方差解释率随主成分数量变化的曲线，选择能够达到目标方差解释率（如 95%）的最小主成分数量。
*   **根据业务需求或可视化需求:** 如果是为了可视化，通常选择 2 或 3 个主成分。
*   **作为超参数:** 在某些情况下，可以将 `n_components` 视为一个超参数，通过交叉验证来选择最佳值（例如，看哪个数量的主成分能让后续模型的性能最好）。

### 2.4 使用 Scikit-learn 实现

::: {.callout-warning title="特征缩放"}
PCA 对特征的尺度非常敏感。在应用 PCA 之前，**必须对数据进行特征缩放** (通常使用 `StandardScaler`)。
:::

```python
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_digits # 手写数字数据集示例
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# --- 加载数据示例 (手写数字) ---
digits = load_digits()
X_digits = digits.data # (1797, 64) - 1797个样本，每个是8x8图像展平后的64个特征
y_digits = digits.target
print("原始数据形状:", X_digits.shape)

# --- 特征缩放 ---
scaler = StandardScaler()
X_digits_scaled = scaler.fit_transform(X_digits)

# --- 应用 PCA ---
# 1. 先不指定 n_components，计算所有主成分的方差解释率
pca_full = PCA(random_state=42)
pca_full.fit(X_digits_scaled)

# 计算累积方差解释率
explained_variance_ratio_cumsum = np.cumsum(pca_full.explained_variance_ratio_)

# 绘制累积方差解释率曲线
plt.figure(figsize=(8, 5))
plt.plot(range(1, len(explained_variance_ratio_cumsum) + 1), explained_variance_ratio_cumsum, marker='.', linestyle='--')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance Ratio')
plt.title('Explained Variance by Number of Components')
plt.grid(True)
# 添加阈值线 (例如 95%)
plt.axhline(y=0.95, color='r', linestyle='-', label='95% Explained Variance')
plt.legend(loc='best')
# plt.show()

# 2. 根据目标方差解释率选择 n_components
# 例如，我们希望保留 95% 的方差
pca_95 = PCA(n_components=0.95, random_state=42) # 直接传入比例
X_digits_pca_95 = pca_95.fit_transform(X_digits_scaled)
print(f"\n保留 95% 方差所需的主成分数量: {pca_95.n_components_}")
print("降维后数据形状 (95%方差):", X_digits_pca_95.shape)

# 3. 或者直接指定主成分数量 (例如用于可视化)
pca_2d = PCA(n_components=2, random_state=42)
X_digits_pca_2d = pca_2d.fit_transform(X_digits_scaled)
print("\n降维到 2D 后的数据形状:", X_digits_pca_2d.shape)

# --- 可视化降维结果 (2D) ---
plt.figure(figsize=(10, 7))
plt.scatter(X_digits_pca_2d[:, 0], X_digits_pca_2d[:, 1], c=y_digits, cmap='viridis', edgecolor='none', alpha=0.7, s=40)
plt.colorbar(label='Digit Label')
plt.title('PCA of Digits Dataset (2 Components)')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
# plt.show()
```

### 2.5 PCA 应用与局限性

*   **应用:**
    *   **数据压缩:** 用更少的维度存储数据，减少存储空间和计算时间。
    *   **噪声去除:** 保留方差较大的主成分通常能过滤掉部分噪声。
    *   **可视化:** 将高维数据降到 2D 或 3D 进行可视化。
    *   **作为预处理步骤:** 将降维后的数据输入到其他机器学习模型中（有时能提高性能，有时会损失信息导致性能下降，需要尝试）。
*   **局限性:**
    *   **线性假设:** PCA 假设数据的主要结构是线性的，对于高度非线性的数据效果可能不佳（可以考虑 Kernel PCA 等非线性方法）。
    *   **可解释性差:** 主成分是原始特征的线性组合，其物理意义不如原始特征直观。
    *   **对特征缩放敏感:** 必须进行特征缩放。

## 3. 特征选择 (Feature Selection)

特征选择与特征提取（如 PCA）不同，它**直接从原始特征集合中选出最优的子集**，而不改变原始特征本身。

**目标:** 选择与目标变量最相关、冗余度最低的特征子集。

### 3.1 方法分类

主要分为三类：过滤法、包裹法和嵌入法。

#### 3.1.1 过滤法 (Filter Methods)

*   **原理:** 基于特征本身的**统计特性**（如方差、相关性、与目标变量的统计相关度）来评估特征的重要性，独立于后续使用的机器学习模型。
*   **优点:** 计算速度快，不易过拟合。
*   **缺点:** 没有考虑特征之间的组合效应，可能选出冗余的特征，选择的特征子集不一定是特定模型的最佳选择。
*   **常用方法:**
    *   **方差阈值 (Variance Threshold):** 移除方差低于某个阈值的特征（认为方差小的特征包含信息少）。`sklearn.feature_selection.VarianceThreshold`。
    *   **相关系数 (Correlation Coefficient):** 计算特征与目标变量之间的相关系数（如 Pearson 相关系数），选择相关系数绝对值高的特征。也可以用来移除特征之间相关性过高的冗余特征。`pandas.DataFrame.corr()`。
    *   **单变量选择 (Univariate Selection):** 使用单变量统计检验（如卡方检验 `chi2` 用于分类特征，F 检验 `f_classif`/`f_regression` 用于数值特征）来评估每个特征与目标变量之间的关系强度，选择得分最高的 K 个特征或一定比例的特征。`sklearn.feature_selection.SelectKBest`, `SelectPercentile`。

```python
from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif, chi2
# 假设已有 X_train, y_train (分类), X_test
# 假设 X_train 是 DataFrame 或 Numpy Array

# --- 方差阈值 ---
# threshold=0.1 表示移除方差小于 0.1 的特征
selector_var = VarianceThreshold(threshold=0.1)
# 假设 X_train 是 Numpy Array 或已缩放
# X_train_high_variance = selector_var.fit_transform(X_train)
# X_test_high_variance = selector_var.transform(X_test) # 在测试集上只 transform
# print("原始特征数:", X_train.shape[1])
# print("方差阈值筛选后特征数:", X_train_high_variance.shape[1])
# print("被移除的特征索引:", ~selector_var.get_support())

# --- 相关系数 (使用 Pandas) ---
# 假设 df_train 是包含特征和目标变量 'target_column' 的 DataFrame
# correlations = df_train.corr()['target_column'].abs().sort_values(ascending=False)
# print("\n特征与目标的相关系数:\n", correlations)
# # 可以选择相关系数大于某个阈值的特征
# threshold_corr = 0.1
# high_corr_features_idx = correlations[correlations > threshold_corr].index
# high_corr_features = high_corr_features_idx.drop('target_column', errors='ignore').tolist() # 移除目标列本身
# print(f"相关系数 > {threshold_corr} 的特征:", high_corr_features)

# --- 单变量选择 (SelectKBest) ---
# 假设是分类问题，y_train 是类别标签
# k=10 表示选择最重要的 10 个特征
# score_func=f_classif 用于数值特征和分类目标
# 确保 X_train 是数值类型
selector_kbest = SelectKBest(score_func=f_classif, k=10)
# X_train_kbest = selector_kbest.fit_transform(X_train, y_train)
# X_test_kbest = selector_kbest.transform(X_test)
# print("\nSelectKBest 筛选后特征数:", X_train_kbest.shape[1])
# print("被选中的特征索引:", selector_kbest.get_support(indices=True))
# print("各特征得分:", selector_kbest.scores_)
# 对于非负特征和分类目标，可以使用 chi2
# 假设 X_train_nonneg 是非负特征
# selector_chi2 = SelectKBest(score_func=chi2, k=10)
# X_train_chi2 = selector_chi2.fit_transform(X_train_nonneg, y_train)
```

#### 3.1.2 包裹法 (Wrapper Methods)

*   **原理:** 将特征选择过程看作是一个搜索问题。它使用一个**特定的机器学习模型**来评估不同**特征子集**的性能，选择能使该模型性能达到最优的那个特征子集。
*   **优点:** 考虑了特征之间的组合效应以及特定模型的偏好，通常能找到性能更好的特征子集。
*   **缺点:** 计算成本非常高，因为需要为每个尝试的特征子集训练和评估模型；容易在特征子集的选择上过拟合。
*   **常用方法:**
    *   **递归特征消除 (Recursive Feature Elimination, RFE):**
        *   **过程:**
            1.  使用所有特征训练一个模型。
            2.  计算每个特征的重要性（例如，线性模型的系数，树模型的特征重要性）。
            3.  移除最不重要的特征。
            4.  重复步骤 1-3，直到达到所需的特征数量。
        *   `sklearn.feature_selection.RFE` (需要指定一个带 `coef_` 或 `feature_importances_` 属性的模型)。
    *   **前向选择 (Forward Selection):** 从空集开始，每次添加一个能最大程度提升模型性能的特征，直到性能不再提升。
    *   **后向消除 (Backward Elimination):** 从全集开始，每次移除一个对模型性能影响最小（或负面影响最大）的特征，直到性能开始下降。

```python
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression # RFE 需要一个评估器

# --- 递归特征消除 (RFE) ---
# 假设是分类问题，使用逻辑回归作为评估器
estimator_rfe = LogisticRegression(solver='liblinear', random_state=42)
# n_features_to_select: 指定最终要选择的特征数量
selector_rfe = RFE(estimator=estimator_rfe, n_features_to_select=10, step=1) # step=1 表示每次移除1个特征

# 假设 X_train, y_train 已定义
# X_train_rfe = selector_rfe.fit_transform(X_train, y_train)
# X_test_rfe = selector_rfe.transform(X_test)
# print("\nRFE 筛选后特征数:", X_train_rfe.shape[1])
# print("被选中的特征索引:", selector_rfe.support_)
# print("特征排名 (1 表示最好):", selector_rfe.ranking_)
```

#### 3.1.3 嵌入法 (Embedded Methods)

*   **原理:** 特征选择过程**嵌入**在模型训练过程中自动完成。模型在训练时会自动学习哪些特征是重要的。
*   **优点:** 计算效率比包裹法高，考虑了特征与模型的交互，通常效果较好。
*   **缺点:** 特征选择结果依赖于所使用的模型。
*   **常用方法:**
    *   **L1 正则化 (Lasso):** 如上周所述，Lasso 倾向于将不重要特征的系数压缩为 0，从而实现特征选择。`sklearn.linear_model.Lasso` (回归) 或带 L1 惩罚的 `LogisticRegression`/`LinearSVC` (分类)。
    *   **基于树模型的特征重要性:** 决策树、随机森林、梯度提升树（如 XGBoost, LightGBM）在训练后可以提供每个特征的重要性评分 (`feature_importances_`)。可以选择重要性高于某个阈值的特征。

```python
from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import Lasso # 用于回归示例

# --- 使用 Lasso 进行特征选择 (回归任务) ---
# 假设有回归数据 X_train_reg, y_train_reg
# lasso_selector = SelectFromModel(Lasso(alpha=0.01, random_state=42)) # alpha 需要调优
# lasso_selector.fit(X_train_reg, y_train_reg)
# X_train_lasso = lasso_selector.transform(X_train_reg)
# print("\nLasso 选择后特征数:", X_train_lasso.shape[1])
# print("Lasso 选择的特征索引:", lasso_selector.get_support())

# --- 使用随机森林进行特征选择 (分类任务) ---
# threshold='median' 表示选择重要性高于中位数的特征，也可以是数值阈值或 '1.25*mean' 等
rf_for_select = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
rf_selector = SelectFromModel(rf_for_select, threshold='median')
# 假设 X_train, y_train 已定义
# rf_selector.fit(X_train, y_train)
# X_train_rf_select = rf_selector.transform(X_train)
# X_test_rf_select = rf_selector.transform(X_test)
# print("\n随机森林选择后特征数:", X_train_rf_select.shape[1])
# print("随机森林选择的特征索引:", rf_selector.get_support())
```

### 3.2 如何选择特征选择方法？

*   **没有万能的方法。**
*   **初步探索:** 可以先用**过滤法**（如方差阈值、相关系数、单变量选择）快速去除明显无用或冗余的特征。
*   **模型相关:** 如果计算资源允许，**包裹法 (RFE)** 或**嵌入法 (Lasso, Tree Importance)** 通常能获得更好的性能，因为它们考虑了模型的需求。
*   **嵌入法**通常是速度和效果之间较好的平衡。
*   **最终选择:** 往往需要尝试多种方法，并通过交叉验证来评估哪种方法和参数组合能带来最佳的模型性能。

::: {.callout-tip title="AI 辅助特征选择"}
*   "解释过滤法、包裹法和嵌入法在特征选择中的主要区别。"
*   "方差阈值法适用于哪些类型的特征？它有什么局限性？"
*   "RFE 是如何工作的？它需要指定哪些关键参数？"
*   "如何使用随机森林的 `feature_importances_` 来进行特征选择？"
*   "在进行特征选择时，应该在训练集上 fit 还是在整个数据集上 fit？为什么？" (答案：只在训练集上 fit)
:::

## 4. 实践与讨论

*   **应用到项目:** 选择你之前的一个项目（分类或回归），尝试应用本周学习的 PCA 或特征选择技术。
    *   **PCA:** 对特征进行缩放后应用 PCA，尝试保留不同比例的方差（如 90%, 95%, 99%），或者直接降到 2 维进行可视化。观察降维后的效果。
    *   **特征选择:** 尝试至少一种过滤法（如 SelectKBest）、一种包裹法（如 RFE）或一种嵌入法（如 SelectFromModel with Lasso/RandomForest）。比较不同方法选择出的特征子集，以及使用这些子集训练模型后的性能变化。
*   **讨论:**
    *   PCA 和特征选择的主要区别是什么？它们各自的优缺点和适用场景？
    *   在你的项目中，降维或特征选择是否提升了模型性能或效率？
    *   你认为哪种特征选择方法（过滤/包裹/嵌入）在你的项目中更合适？为什么？

## 5. 本周总结

本周我们学习了处理高维数据的两种重要策略：降维和特征选择。我们深入探讨了 PCA 的原理、实现、参数选择和应用，并学习了过滤法、包裹法和嵌入法这三大类特征选择技术及其常用方法。掌握这些技术有助于我们构建更简洁、高效、性能更优的机器学习模型。

**下周我们将开启时间序列分析之旅，学习处理和分析带时间戳的数据！**